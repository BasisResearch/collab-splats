{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a mesh\n",
    "\n",
    "We can also use the Splatter wrapper class to take an existing nerfstudio model and create a mesh!\n",
    "1. **mesh:** creates a mesh via TSDF fusion\n",
    "\n",
    "2. **query_mesh:** uses the trained model to query the mesh and returns a similarity map\n",
    "\n",
    "3. **plot_mesh:** enables plotting of mesh features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from collab_splats.wrapper import Splatter, SplatterConfig\n",
    "# import pyvista as pv\n",
    "\n",
    "# pv.start_xvfb()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths to the file for running splats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforms.json already exists at /workspace/fieldwork-data/rats/2024-07-11/environment/C0119/preproc/transforms.json\n",
      "To rerun preprocessing, set overwrite=True\n",
      "Output already exists for rade-features\n",
      "To rerun feature extraction, set overwrite=True\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('/workspace/fieldwork-data/')\n",
    "session_dir = base_dir / \"rats/2024-07-11/SplatsSD\"\n",
    "\n",
    "# Make the configuration \n",
    "splatter_config = SplatterConfig(\n",
    "    file_path=session_dir / \"C0119.MP4\",\n",
    "    method='rade-features',\n",
    "    frame_proportion=0.25, # Use 25% of the frames within the video (or default to minimum 300 frames)\n",
    ")\n",
    "\n",
    "# Initialize the Splatter class\n",
    "splatter = Splatter(splatter_config)\n",
    "\n",
    "# Call these to populate the splatter with paths (probably a better way to do this --> maybe save out config)\n",
    "splatter.preprocess()\n",
    "splatter.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mesh\n",
    "\n",
    "We can create a mesh by calling the ```mesh()``` method. Under the hood, this runs TSDF fusion creating an integrated volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available runs:\n",
      "[0] 2025-07-11_171420\n"
     ]
    }
   ],
   "source": [
    "splatter.mesh(\n",
    "    depth_name=\"median_depth\",\n",
    "    depth_trunc=1.0,\n",
    "    # overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mesh!\n",
    "\n",
    "We can use the splatter function ```plot_mesh``` to visualize given attributes of the mesh. The inherent attributes are RGB and Normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 116178\n",
      "Number of cells: 215841\n",
      "Bounds: BoundsTuple(x_min=-0.5249999761581421, x_max=0.7450000047683716, y_min=-0.125, y_max=1.4850000143051147, z_min=-1.0149999856948853, z_max=1.0214753150939941)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84a2cfe326040b396a58d5bf18f78ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34453/index.html?ui=P_0x76d4a170e890_14&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splatter.plot_mesh(attribute=\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using semantic queries \n",
    "\n",
    "The mesh contains semantic features which we can query via positive and negative prompts. The goal of this is to find points that are more similar to the positive prompts compared to the negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml\n",
      "[Taichi] version 1.7.3, llvm 15.0.4, commit 5ec301be, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/23/25 20:43:35.153 203920] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "ename": "ConstructorError",
     "evalue": "while constructing a Python object\ncannot find module 'ns_extension.utils.trainer_config' (No module named 'ns_extension')\n  in \"<unicode string>\", line 1, column 1:\n    !!python/object:ns_extension.uti ... \n    ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:551\u001b[0m, in \u001b[0;36mFullConstructor.find_python_name\u001b[0;34m(self, name, mark, unsafe)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ns_extension'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43msplatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_mesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaves\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/collab-splats/collab_splats/wrapper/splatter.py:331\u001b[0m, in \u001b[0;36mSplatter.query_mesh\u001b[0;34m(self, positive_queries, negative_queries, method)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnerfstudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eval_setup\n\u001b[0;32m--> 331\u001b[0m     _, pipeline, _,  _ \u001b[38;5;241m=\u001b[39m \u001b[43meval_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_config_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmesh_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/utils/eval_utils.py:90\u001b[0m, in \u001b[0;36meval_setup\u001b[0;34m(config_path, eval_num_rays_per_chunk, test_mode, update_config_callback)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shared setup for loading a saved pipeline for evaluation.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Loaded config, pipeline module, corresponding checkpoint, and step\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# load save config\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, TrainerConfig)\n\u001b[1;32m     93\u001b[0m config\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mdatamanager\u001b[38;5;241m.\u001b[39m_target \u001b[38;5;241m=\u001b[39m all_methods[config\u001b[38;5;241m.\u001b[39mmethod_name]\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mdatamanager\u001b[38;5;241m.\u001b[39m_target\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:51\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_single_node()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:55\u001b[0m, in \u001b[0;36mBaseConstructor.construct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstruct_document\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[0;32m---> 55\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generators:\n\u001b[1;32m     57\u001b[0m         state_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generators\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:105\u001b[0m, in \u001b[0;36mBaseConstructor.construct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, types\u001b[38;5;241m.\u001b[39mGeneratorType):\n\u001b[1;32m    104\u001b[0m     generator \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 105\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_construct:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dummy \u001b[38;5;129;01min\u001b[39;00m generator:\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:617\u001b[0m, in \u001b[0;36mFullConstructor.construct_python_object\u001b[0;34m(self, suffix, node)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstruct_python_object\u001b[39m(\u001b[38;5;28mself\u001b[39m, suffix, node):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# Format:\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;66;03m#   !!python/object:module.name { ... state ... }\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_python_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m instance\n\u001b[1;32m    619\u001b[0m     deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:722\u001b[0m, in \u001b[0;36mUnsafeConstructor.make_python_instance\u001b[0;34m(self, suffix, node, args, kwds, newobj)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_python_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, suffix, node, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, kwds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mUnsafeConstructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_python_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:585\u001b[0m, in \u001b[0;36mFullConstructor.make_python_instance\u001b[0;34m(self, suffix, node, args, kwds, newobj, unsafe)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwds:\n\u001b[1;32m    584\u001b[0m     kwds \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 585\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_python_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (unsafe \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m)):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a Python instance\u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mstart_mark,\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a class, but found \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mcls\u001b[39m),\n\u001b[1;32m    589\u001b[0m             node\u001b[38;5;241m.\u001b[39mstart_mark)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:719\u001b[0m, in \u001b[0;36mUnsafeConstructor.find_python_name\u001b[0;34m(self, name, mark)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_python_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mark):\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mUnsafeConstructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_python_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/yaml/constructor.py:553\u001b[0m, in \u001b[0;36mFullConstructor.find_python_name\u001b[0;34m(self, name, mark, unsafe)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28m__import__\u001b[39m(module_name)\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m, mark,\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot find module \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (module_name, exc), mark)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m, mark,\n\u001b[1;32m    557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m is not imported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_name, mark)\n",
      "\u001b[0;31mConstructorError\u001b[0m: while constructing a Python object\ncannot find module 'ns_extension.utils.trainer_config' (No module named 'ns_extension')\n  in \"<unicode string>\", line 1, column 1:\n    !!python/object:ns_extension.uti ... \n    ^"
     ]
    }
   ],
   "source": [
    "similarity = splatter.query_mesh(\n",
    "    positive_queries=[\"tree\"],\n",
    "    negative_queries=[\"ground\", \"leaves\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot similarity maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 116178\n",
      "Number of cells: 215841\n",
      "Bounds: BoundsTuple(x_min=-0.5249999761581421, x_max=0.7450000047683716, y_min=-0.125, y_max=1.4850000143051147, z_min=-1.0149999856948853, z_max=1.0214753150939941)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186f6c4c3754c0e92a0ab4e8f26ee99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34453/index.html?ui=P_0x76d499a8f5e0_17&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splatter.plot_mesh(attribute=similarity, rgb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets clean up the mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix mesh multistep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing meshlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating average edge length: 100%|██████████| 268439/268439 [00:04<00:00, 59913.95it/s]\n",
      "Filling holes (822):   2%|▏         | 13/822 [00:00<00:16, 49.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hole Id_EdgeTag(27) of perimeter 45.35377921021609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling holes (822): 100%|██████████| 822/822 [00:11<00:00, 69.15it/s] \n"
     ]
    }
   ],
   "source": [
    "import meshlib.mrmeshpy as mm\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def collapse_spikes_on_boundary(mesh, threshold=2.0):\n",
    "    collapsed = 0\n",
    "\n",
    "    # Find boundary edge loops\n",
    "    boundary_loops = mm.findLeftBoundary(mesh.topology)\n",
    "\n",
    "    # Flatten list of edge loops\n",
    "    flat_edges = [e for loop in boundary_loops for e in loop]\n",
    "\n",
    "    # Define callback function required by collapseEdge\n",
    "    def on_edge_del(e1, e2):\n",
    "        # This function is called when edges are deleted after collapse\n",
    "        # You can add logging here if needed\n",
    "        pass\n",
    "\n",
    "    for edge in tqdm(flat_edges, desc=\"Collapsing long boundary edges\"):\n",
    "        edge = mm.EdgeId(edge)  # ensure type\n",
    "\n",
    "        org = mesh.topology.org(edge)\n",
    "        dest = mesh.topology.dest(edge)\n",
    "        p0 = mesh.points.vec[org.get()]\n",
    "        p1 = mesh.points.vec[dest.get()]\n",
    "        length = (p0 - p1).length()\n",
    "\n",
    "        if length > threshold:\n",
    "            result = mesh.topology.collapseEdge(edge, on_edge_del)\n",
    "            if result:\n",
    "                collapsed += 1\n",
    "            # Optionally print failures:\n",
    "            else:\n",
    "                print(f\"Collapse failed for edge {edge}, length={length:.4f}\")\n",
    "\n",
    "    print(f\"Collapsed {collapsed} long boundary edges\")\n",
    "\n",
    "def clean_repair_mesh(mesh_path: str, max_hole_size: float = 1.0, max_edge_splits: int = 1000):\n",
    "    \n",
    "    # Load mesh\n",
    "    mesh = mm.loadMesh(mesh_path)\n",
    "    \n",
    "    # Identify all connected components\n",
    "    components = mm.getAllComponents(mesh)\n",
    "\n",
    "    # Determine component sizes\n",
    "    sizes = [mask.count() for mask in components]  # count: faces or vertices\n",
    "    largest_idx = max(range(len(sizes)), key=lambda i: sizes[i])\n",
    "\n",
    "    # Create a new mesh for the largest component\n",
    "    largest_mask = components[largest_idx]\n",
    "    part = mm.Mesh()\n",
    "    part.addPartByMask(mesh, largest_mask)\n",
    "\n",
    "    # Set the mesh to the largest component\n",
    "    mesh = part\n",
    "\n",
    "    avg_edge_length = 0.0\n",
    "    num_edges = 0\n",
    "    for i in trange(mesh.topology.undirectedEdgeSize(), desc=\"Calculating average edge length\"):\n",
    "        dir_edge = mm.EdgeId(i*2)\n",
    "        org = mesh.topology.org(dir_edge)\n",
    "        dest = mesh.topology.dest(dir_edge)\n",
    "        avg_edge_length += (mesh.points.vec[dest.get()] - mesh.points.vec[org.get()]).length()\n",
    "        num_edges = num_edges + 1\n",
    "    avg_edge_length = avg_edge_length/num_edges\n",
    "\n",
    "    # Find all holes\n",
    "    hole_ids = mesh.topology.findHoleRepresentiveEdges()\n",
    "    fill_params = mm.FillHoleParams()\n",
    "    # fill_params.metric = mm.getUniversalMetric(mesh)\n",
    "\n",
    "    # Fill all holes\n",
    "    num_holes = len(hole_ids)\n",
    "    for he in tqdm(hole_ids, desc=f\"Filling holes ({num_holes})\", total=num_holes):\n",
    "        if mesh.holePerimiter( he ) < max_hole_size:\n",
    "            new_faces = mm.FaceBitSet()\n",
    "            fill_params.outNewFaces = new_faces\n",
    "            mm.fillHole( mesh, he, fill_params )\n",
    "\n",
    "            new_verts = mm.VertBitSet()\n",
    "            subdiv_settings = mm.SubdivideSettings()\n",
    "            subdiv_settings.maxEdgeLen = avg_edge_length\n",
    "            subdiv_settings.maxEdgeSplits = max_edge_splits\n",
    "            subdiv_settings.region = new_faces\n",
    "            subdiv_settings.newVerts = new_verts\n",
    "            mm.subdivideMesh(mesh,subdiv_settings)\n",
    "            mm.positionVertsSmoothly(mesh,new_verts)\n",
    "        else:\n",
    "            print(f\"Skipping hole {he} of perimeter {mesh.holePerimiter( he )}\")\n",
    "            \n",
    "    return mesh\n",
    "\n",
    "mesh = clean_repair_mesh(splatter.config['mesh_info']['mesh'], max_hole_size=3.0, max_edge_splits=10000)\n",
    "mm.saveMesh(mesh, \"test.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_mesh = pv.read(\"test.ply\")\n",
    "original_mesh = pv.read(splatter.config['mesh_info']['mesh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_splats.utils.mesh import normals2vertex, features2vertex\n",
    "\n",
    "normals = normals2vertex(cleaned_mesh.points, original_mesh.points, original_mesh.point_data['Normals'])\n",
    "rgb = features2vertex(cleaned_mesh.points, original_mesh.points, original_mesh.point_data['RGB'])\n",
    "tree_features = features2vertex(cleaned_mesh.points, original_mesh.points, similarity)\n",
    "\n",
    "cleaned_mesh.point_data['Normals'] = normals\n",
    "cleaned_mesh.point_data['RGB'] = np.asarray(rgb).astype(np.uint8)\n",
    "cleaned_mesh.point_data['tree'] = tree_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding normals\n",
      "\u001b[1;33m[Open3D WARNING] Write geometry::TriangleMesh failed: unknown file extension.\u001b[0;m\n",
      "Saved: example_mesh\n",
      "Saved: example_mesh_tree.ply\n"
     ]
    }
   ],
   "source": [
    "save_pyvista_to_open3d_ply(cleaned_mesh, \"example_mesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_pyvista_to_open3d_ply(pv_mesh, filename_base):\n",
    "    \"\"\"\n",
    "    Save a PyVista mesh to Open3D-compatible PLY format.\n",
    "    \n",
    "    Parameters:\n",
    "        pv_mesh (pyvista.PolyData): Input mesh with point data.\n",
    "        filename_base (str): Base filename without extension.\n",
    "    \"\"\"\n",
    "    # Extract basic geometry\n",
    "    points = np.asarray(pv_mesh.points)\n",
    "    faces = pv_mesh.faces.reshape((-1, 4))[:, 1:]  # triangle faces assumed\n",
    "\n",
    "    # Prepare base Open3D mesh\n",
    "    base_mesh = o3d.geometry.TriangleMesh()\n",
    "    base_mesh.vertices = o3d.utility.Vector3dVector(points)\n",
    "    base_mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "\n",
    "    # Handle RGB\n",
    "    if \"RGB\" in pv_mesh.point_data:\n",
    "        rgb = np.asarray(pv_mesh.point_data[\"RGB\"])\n",
    "        if rgb.max() > 1.0:\n",
    "            rgb = rgb / 255.0\n",
    "        base_mesh.vertex_colors = o3d.utility.Vector3dVector(rgb)\n",
    "    \n",
    "    # Handle normals\n",
    "    if \"Normals\" in pv_mesh.point_data:\n",
    "        print (f\"Adding normals\")\n",
    "        normals = np.asarray(pv_mesh.point_data[\"Normals\"])\n",
    "        base_mesh.vertex_normals = o3d.utility.Vector3dVector(normals)\n",
    "\n",
    "    # Save main mesh\n",
    "    main_path = f\"{filename_base}\"\n",
    "    o3d.io.write_triangle_mesh(main_path, base_mesh, write_ascii=True)\n",
    "    print(f\"Saved: {main_path}\")\n",
    "\n",
    "    # Save other point data fields in separate files\n",
    "    for key in pv_mesh.point_data.keys():\n",
    "        if key in [\"RGB\", \"Normals\"]:\n",
    "            continue\n",
    "        \n",
    "        attr = np.asarray(pv_mesh.point_data[key])\n",
    "        if attr.ndim == 1:\n",
    "            attr = attr[:, np.newaxis]\n",
    "        if attr.shape[1] > 3:\n",
    "            print(f\"Skipping {key}: more than 3 channels (shape={attr.shape})\")\n",
    "            continue\n",
    "\n",
    "        # Normalize and pad to RGB\n",
    "        color_data = np.zeros((len(attr), 3))\n",
    "        norm_attr = attr.astype(np.float64)\n",
    "        if np.max(norm_attr) > 0:\n",
    "            norm_attr /= np.max(norm_attr)\n",
    "        color_data[:, :attr.shape[1]] = norm_attr\n",
    "\n",
    "        # Build new mesh\n",
    "        custom_mesh = o3d.geometry.TriangleMesh()\n",
    "        custom_mesh.vertices = o3d.utility.Vector3dVector(points)\n",
    "        custom_mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "        custom_mesh.vertex_colors = o3d.utility.Vector3dVector(color_data)\n",
    "\n",
    "        # Save it\n",
    "        out_path = f\"{filename_base}_{key}.ply\"\n",
    "        o3d.io.write_triangle_mesh(out_path, custom_mesh, write_ascii=True)\n",
    "        print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(\"example_mesh.ply\")\n",
    "\n",
    "# o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh.has_vertex_normals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee4342b52fa469eaf3bc230805ed06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:35473/index.html?ui=P_0x7c0d33d9c130_4&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "vis_mesh = pv.read(\"example_mesh_tree.ply\")\n",
    "\n",
    "vis_mesh.plot(scalars='RGB', rgb=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
