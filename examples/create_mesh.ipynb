{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a mesh\n",
    "\n",
    "We can also use the Splatter wrapper class to take an existing nerfstudio model and create a mesh!\n",
    "1. **mesh:** creates a mesh via TSDF fusion\n",
    "\n",
    "2. **query_mesh:** uses the trained model to query the mesh and returns a similarity map\n",
    "\n",
    "3. **plot_mesh:** enables plotting of mesh features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/pyvista/plotting/utilities/xvfb.py:48: PyVistaDeprecationWarning: This function is deprecated and will be removed in future version of PyVista. Use vtk-osmesa instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from ns_extension.wrapper import Splatter, SplatterConfig\n",
    "\n",
    "import pyvista as pv\n",
    "pv.start_xvfb()\n",
    "\n",
    "# import vtkmodules.all as vtk\n",
    "# render_window = vtk.vtkRenderWindow()\n",
    "# render_window.SetOffScreenRendering(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths to the file for running splats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforms.json already exists at /workspace/fieldwork-data/rats/2024-07-11/environment/C0119/preproc/transforms.json\n",
      "To rerun preprocessing, set overwrite=True\n",
      "Output already exists for rade-features\n",
      "To rerun feature extraction, set overwrite=True\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('/workspace/fieldwork-data/')\n",
    "session_dir = base_dir / \"rats/2024-07-11/SplatsSD\"\n",
    "\n",
    "# Make the configuration \n",
    "splatter_config = SplatterConfig(\n",
    "    file_path=session_dir / \"C0119.MP4\",\n",
    "    method='rade-features',\n",
    "    frame_proportion=0.25, # Use 25% of the frames within the video (or default to minimum 300 frames)\n",
    ")\n",
    "\n",
    "# Initialize the Splatter class\n",
    "splatter = Splatter(splatter_config)\n",
    "\n",
    "# Call these to populate the splatter with paths (probably a better way to do this --> maybe save out config)\n",
    "splatter.preprocess()\n",
    "splatter.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mesh\n",
    "\n",
    "We can create a mesh by calling the ```mesh()``` method. Under the hood, this runs TSDF fusion creating an integrated volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available runs:\n",
      "[0] 2025-07-11_171420\n"
     ]
    }
   ],
   "source": [
    "splatter.mesh(\n",
    "    depth_name=\"median_depth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mesh!\n",
    "\n",
    "We can use the splatter function ```plot_mesh``` to visualize given attributes of the mesh. The inherent attributes are RGB and Normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 277426\n",
      "Number of cells: 494857\n",
      "Bounds: BoundsTuple(x_min=-1.4198578596115112, x_max=0.7450000047683716, y_min=-0.125, y_max=3.0850000381469727, z_min=-1.459478735923767, z_max=1.81288480758667)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23646689d11149679961c955febf19d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:32829/index.html?ui=P_0x700634e8a770_3&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splatter.plot_mesh(attribute=\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using semantic queries \n",
    "\n",
    "The mesh contains semantic features which we can query via positive and negative prompts. The goal of this is to find points that are more similar to the positive prompts compared to the negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml\n",
      "[Taichi] version 1.7.3, llvm 15.0.4, commit 5ec301be, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/21/25 16:04:48.915 34474] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:04:52] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:04:52]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=321733;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53397;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43msplatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_mesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaves\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/wrapper/splatter.py:327\u001b[0m, in \u001b[0;36mSplatter.query_mesh\u001b[0;34m(self, positive_queries, negative_queries, method)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_config_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnerfstudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eval_setup\n\u001b[0;32m--> 327\u001b[0m     _, pipeline, _,  _ \u001b[38;5;241m=\u001b[39m \u001b[43meval_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_config_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmesh_info\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/utils/eval_utils.py:106\u001b[0m, in \u001b[0;36meval_setup\u001b[0;34m(config_path, eval_num_rays_per_chunk, test_mode, update_config_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# setup pipeline (which includes the DataManager)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pipeline, Pipeline)\n\u001b[1;32m    108\u001b[0m pipeline\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/configs/base_config.py:53\u001b[0m, in \u001b[0;36mInstantiateConfig.setup\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the instantiated object using the config.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/pipelines/base_pipeline.py:254\u001b[0m, in \u001b[0;36mVanillaPipeline.__init__\u001b[0;34m(self, config, device, test_mode, world_size, local_rank, grad_scaler)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_mode \u001b[38;5;241m=\u001b[39m test_mode\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamanager: DataManager \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# TODO make cleaner\u001b[39;00m\n\u001b[1;32m    258\u001b[0m seed_pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/configs/base_config.py:53\u001b[0m, in \u001b[0;36mInstantiateConfig.setup\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the instantiated object using the config.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/datamanagers/features_datamanager.py:72\u001b[0m, in \u001b[0;36mFeatureSplattingDataManager.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Extract or load cached features for all images\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dict)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Split features into train and eval sets\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/datamanagers/features_datamanager.py:98\u001b[0m, in \u001b[0;36mFeatureSplattingDataManager.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Try loading from cache if enabled\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_cache \u001b[38;5;129;01mand\u001b[39;00m cache_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 98\u001b[0m     cache_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_filenames\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m image_filenames:\n\u001b[1;32m    101\u001b[0m         CONSOLE\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage filenames have changed, cache invalidated...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarity = splatter.query_mesh(\n",
    "    positive_queries=[\"tree\"],\n",
    "    negative_queries=[\"ground\", \"leaves\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot similarity maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 277426\n",
      "Number of cells: 494857\n",
      "Bounds: BoundsTuple(x_min=-1.4198578596115112, x_max=0.7450000047683716, y_min=-0.125, y_max=3.0850000381469727, z_min=-1.459478735923767, z_max=1.81288480758667)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75829229f814cc4b25d938cd06f2309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:36567/index.html?ui=P_0x76a4efb16aa0_3&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splatter.plot_mesh(attribute=similarity, rgb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets clean up the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import pymeshfix\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def clean_repair_mesh(mesh_path: str) -> trimesh.Trimesh:\n",
    "    \"\"\"\n",
    "    Load a mesh, extract the largest component, repair it conservatively,\n",
    "    and map vertex properties (color, normals) back to the repaired mesh.\n",
    "    \"\"\"\n",
    "    # Load mesh\n",
    "    # mesh = trimesh.load(mesh_path, process=False)\n",
    "    mesh = pv.read(mesh_path)\n",
    "    print(f\"[INFO] Loaded mesh with {mesh.n_points} vertices and {mesh.n_cells} faces\")\n",
    "\n",
    "    # # Get largest connected component\n",
    "    # components = mesh.split(only_watertight=False)\n",
    "    # largest = max(components, key=lambda c: len(c.vertices))\n",
    "    # print(f\"[INFO] Largest component: {len(largest.vertices)} vertices, {len(largest.faces)} faces\")\n",
    "\n",
    "    # Conservatively repair mesh (no component joining or small-part removal)\n",
    "    fixer = pymeshfix.MeshFix(mesh)\n",
    "    fixer.repair(verbose=True, joincomp=False, remove_smallest_components=False)\n",
    "\n",
    "    # # Reconstruct mesh\n",
    "    # repaired_vertices = fixer.v\n",
    "    # repaired_faces = fixer.f\n",
    "    # repaired_mesh = trimesh.Trimesh(vertices=repaired_vertices, faces=repaired_faces, process=False)\n",
    "\n",
    "    # # Map vertex properties using nearest neighbors\n",
    "    # tree = cKDTree(largest.vertices)\n",
    "    # _, indices = tree.query(repaired_vertices)\n",
    "\n",
    "    # if hasattr(largest, 'vertex_colors') and len(largest.vertex_colors) > 0:\n",
    "    #     repaired_mesh.visual.vertex_colors = largest.vertex_colors[indices]\n",
    "\n",
    "    # if hasattr(largest, 'vertex_normals') and len(largest.vertex_normals) > 0:\n",
    "    #     repaired_mesh.vertex_normals = largest.vertex_normals[indices]\n",
    "\n",
    "    # print(\"[INFO] Repaired mesh constructed and attributes mapped.\")\n",
    "    return fixer #, largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import pymeshfix\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def repair_mesh(mesh_path: str, verbose=True) -> pv.PolyData:\n",
    "    \"\"\"\n",
    "    Repair a PyVista mesh using pymeshfix while retaining vertex properties.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (pv.PolyData): Input PyVista mesh to repair.\n",
    "        verbose (bool): Whether to print pymeshfix verbose output.\n",
    "    \n",
    "    Returns:\n",
    "        pv.PolyData: Repaired mesh with vertex properties mapped from original.\n",
    "    \"\"\"\n",
    "    \n",
    "    mesh = pv.read(mesh_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Input mesh: {mesh.n_points} vertices, {mesh.n_faces} faces\")\n",
    "\n",
    "    # Extract vertices and faces from PyVista mesh\n",
    "    vertices = mesh.points\n",
    "    # PyVista faces format: [N, v0, v1, v2, N, v0, v1, v2,...]\n",
    "    faces = mesh.faces.reshape((-1, 4))[:, 1:4]\n",
    "\n",
    "    # Repair mesh with pymeshfix\n",
    "    mf = pymeshfix.MeshFix(vertices, faces)\n",
    "    mf.repair(verbose=verbose, joincomp=False, remove_smallest_components=False)\n",
    "\n",
    "    repaired_vertices = mf.v\n",
    "    repaired_faces = mf.f\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Repaired mesh: {len(repaired_vertices)} vertices, {len(repaired_faces)} faces\")\n",
    "\n",
    "    # Build PyVista faces array format: [3, i0, i1, i2, 3, ...]\n",
    "    flat_faces = np.hstack(\n",
    "        np.column_stack((np.full(len(repaired_faces), 3, dtype=np.int64), repaired_faces))\n",
    "    )\n",
    "\n",
    "    repaired_mesh = pv.PolyData(repaired_vertices, flat_faces)\n",
    "\n",
    "    # Map original vertex properties via nearest neighbor\n",
    "    tree = cKDTree(vertices)\n",
    "    distances, indices = tree.query(repaired_vertices, k=1)\n",
    "\n",
    "    for name in mesh.point_data.keys():\n",
    "        original_array = mesh.point_data[name]\n",
    "        if len(original_array) == mesh.n_points:\n",
    "            mapped_array = original_array[indices]\n",
    "            repaired_mesh.point_data[name] = mapped_array\n",
    "            if verbose:\n",
    "                print(f\"[INFO] Mapped vertex property '{name}'\")\n",
    "\n",
    "    return repaired_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix mesh multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest connected component has 164314 points and 302746 cells\n"
     ]
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "import pymeshfix\n",
    "import numpy as np\n",
    "\n",
    "mesh = pv.read(splatter.config['mesh_info']['mesh'])\n",
    "\n",
    "\n",
    "# Compute connectivity (default to cell connectivity)\n",
    "connectivity = mesh.connectivity()\n",
    "region_ids = connectivity.cell_data['RegionId'] # The 'RegionId' array labels each connected component\n",
    "unique_ids, counts = np.unique(region_ids, return_counts=True) # Count the number of cells in each region\n",
    "largest_region_id = unique_ids[np.argmax(counts)] # Find the region id with the max number of cells\n",
    "\n",
    "# Extract cells belonging to largest connected component\n",
    "lcc = connectivity.extract_cells(region_ids == largest_region_id)\n",
    "\n",
    "print(f\"Largest connected component has {lcc.n_points} points and {lcc.n_cells} cells\")\n",
    "\n",
    "mesh = lcc.extract_surface() # Extract surface as PolyData\n",
    "\n",
    "\n",
    "\n",
    "# Extract surface as PolyData\n",
    "\n",
    "# max_hole_size = 1e6 # tune this based on hole size (units = mesh units)\n",
    "# mesh = mesh.fill_holes(max_hole_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mesh(mesh_path: str):\n",
    "    mesh = pv.read(mesh_path)\n",
    "\n",
    "    # Compute connectivity (default to cell connectivity)\n",
    "    connectivity = mesh.connectivity()\n",
    "    region_ids = connectivity.cell_data['RegionId'] # The 'RegionId' array labels each connected component\n",
    "    unique_ids, counts = np.unique(region_ids, return_counts=True) # Count the number of cells in each region\n",
    "    largest_region_id = unique_ids[np.argmax(counts)] # Find the region id with the max number of cells\n",
    "\n",
    "    # Extract cells belonging to largest connected component\n",
    "    lcc = connectivity.extract_cells(region_ids == largest_region_id)\n",
    "\n",
    "    print(f\"Largest connected component has {lcc.n_points} points and {lcc.n_cells} cells\")\n",
    "\n",
    "    mesh = lcc.extract_surface() # Extract surface as PolyData\n",
    "\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphinx_gallery_thumbnail_number = 1\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "from pymeshfix import MeshFix\n",
    "from pymeshfix._meshfix import PyTMesh\n",
    "from pymeshfix.examples import planar_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meshfix = MeshFix(mesh)\n",
    "holes = meshfix.extract_holes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfix.repair(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7781f10246dc497796e55c9812d29ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:45081/index.html?ui=P_0x7e0525f82770_4&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render the mesh and outline the holes\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(mesh, color=True)\n",
    "plotter.add_mesh(holes, color=\"r\", line_width=5)\n",
    "plotter.enable_eye_dome_lighting()  # helps depth perception\n",
    "_ = plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boundary edges: 31312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bb2cbae82b470abf59cc2c02418d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:45855/index.html?ui=P_0x7c3948f0bbb0_3&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = mesh.extract_feature_edges(\n",
    "    boundary_edges=True,\n",
    "    feature_edges=False,\n",
    "    manifold_edges=False,\n",
    "    non_manifold_edges=False,\n",
    ")\n",
    "\n",
    "print(f\"Number of boundary edges: {edges.n_lines}\")\n",
    "edges.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import pymeshfix\n",
    "from scipy.spatial import Delaunay, cKDTree\n",
    "from tqdm import tqdm  # Optional, for progress bars\n",
    "\n",
    "def fill_small_holes_selectively(mesh: pv.PolyData,\n",
    "                                hole_perimeter_threshold=1.0,\n",
    "                                max_edge_length=0.2,\n",
    "                                verbose=True) -> pv.PolyData:\n",
    "    \"\"\"\n",
    "    Fill only holes smaller than hole_perimeter_threshold and remove suspicious triangles.\n",
    "    \n",
    "    Parameters:\n",
    "        mesh (pv.PolyData): Input mesh (must be PolyData)\n",
    "        hole_perimeter_threshold (float): Max perimeter length of holes to fill\n",
    "        max_edge_length (float): Max allowed edge length after repair to keep triangles\n",
    "        verbose (bool): Print progress info\n",
    "    \n",
    "    Returns:\n",
    "        pv.PolyData: Hole-filled and cleaned mesh\n",
    "    \"\"\"\n",
    "    # if verbose:\n",
    "    print(f\"[INFO] Starting with mesh: {mesh.n_points} points, {mesh.n_faces} faces\")\n",
    "\n",
    "    # Step 1: Extract boundary edges (hole loops)\n",
    "    boundary_edges = mesh.extract_feature_edges(\n",
    "        boundary_edges=True,\n",
    "        feature_edges=False,\n",
    "        manifold_edges=False,\n",
    "        non_manifold_edges=False,\n",
    "    )\n",
    "    \n",
    "    if boundary_edges.n_lines == 0:\n",
    "        # if verbose: \n",
    "        print(\"[INFO] No boundary edges (holes) found. Returning original mesh.\")\n",
    "        return mesh.copy()\n",
    "    \n",
    "    # if verbose:\n",
    "    print(f\"[INFO] Found {boundary_edges.n_lines} hole loops\")\n",
    "\n",
    "    # Step 2: Separate boundary edges into connected loops\n",
    "    connectivity = boundary_edges.connectivity()\n",
    "    region_ids = connectivity.point_data['RegionId']\n",
    "\n",
    "    filled_mesh = mesh.copy()\n",
    "\n",
    "    def order_loop_points_fast(pts):\n",
    "        pts = np.asarray(pts)\n",
    "        N = len(pts)\n",
    "        tree = cKDTree(pts)\n",
    "        ordered = [0]\n",
    "        used = set(ordered)\n",
    "\n",
    "        while len(ordered) < N:\n",
    "            last_idx = ordered[-1]\n",
    "            dists, idxs = tree.query(pts[last_idx], k=10)\n",
    "            for i in idxs:\n",
    "                if i not in used:\n",
    "                    ordered.append(i)\n",
    "                    used.add(i)\n",
    "                    break\n",
    "            else:\n",
    "                # No unused neighbor found - break to avoid infinite loop\n",
    "                break\n",
    "        return pts[ordered]\n",
    "\n",
    "    unique_region_ids = np.unique(region_ids)\n",
    "    iterator = tqdm(unique_region_ids, desc=\"Filling holes\")\n",
    "\n",
    "    for region_id in iterator:\n",
    "        hole_loop = connectivity.extract_points(region_ids == region_id, adjacent_cells=True)\n",
    "        loop_pts = hole_loop.points\n",
    "        ordered_pts = order_loop_points_fast(loop_pts)\n",
    "\n",
    "        perimeter = np.sum(np.linalg.norm(np.diff(np.vstack([ordered_pts, ordered_pts[0]]), axis=0), axis=1))\n",
    "        # if verbose:\n",
    "        #     print(f\" - Hole region {region_id} perimeter: {perimeter:.3f}\")\n",
    "\n",
    "        if perimeter < hole_perimeter_threshold:\n",
    "            # if verbose:\n",
    "            #     print(\"   Filling hole...\")\n",
    "            # Project points to best-fit plane\n",
    "            centroid = ordered_pts.mean(axis=0)\n",
    "            pts_centered = ordered_pts - centroid\n",
    "            _, _, vh = np.linalg.svd(pts_centered)\n",
    "            proj_pts = pts_centered @ vh[:2].T\n",
    "\n",
    "            tri = Delaunay(proj_pts)\n",
    "            hole_faces = tri.simplices\n",
    "\n",
    "            # Add new vertices and faces\n",
    "            vertices = filled_mesh.points\n",
    "            faces = filled_mesh.faces.reshape((-1, 4))[:, 1:4]\n",
    "\n",
    "            hole_vertex_indices = np.arange(len(vertices), len(vertices) + len(ordered_pts))\n",
    "            new_vertices = np.vstack([vertices, ordered_pts])\n",
    "            new_faces = hole_faces + len(vertices)\n",
    "\n",
    "            combined_faces = np.vstack([faces, new_faces])\n",
    "\n",
    "            flat_faces = np.hstack(\n",
    "                np.column_stack((np.full(len(combined_faces), 3, dtype=int), combined_faces))\n",
    "            )\n",
    "            filled_mesh = pv.PolyData(new_vertices, flat_faces)\n",
    "        else:\n",
    "            # if verbose:\n",
    "            print(\"   Hole too large, skipping filling.\")\n",
    "\n",
    "    # if verbose:\n",
    "    print(\"[INFO] Running pymeshfix repair...\")\n",
    "\n",
    "    vertices = filled_mesh.points\n",
    "    faces = filled_mesh.faces.reshape((-1, 4))[:, 1:4]\n",
    "\n",
    "    mf = pymeshfix.MeshFix(vertices, faces)\n",
    "    mf.repair(verbose=verbose, joincomp=False, remove_smallest_components=False)\n",
    "\n",
    "    repaired_vertices = mf.v\n",
    "    repaired_faces = mf.f\n",
    "\n",
    "    flat_faces = np.hstack(\n",
    "        np.column_stack((np.full(len(repaired_faces), 3, dtype=int), repaired_faces))\n",
    "    )\n",
    "\n",
    "    repaired_mesh = pv.PolyData(repaired_vertices, flat_faces)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Removing triangles with edges longer than {max_edge_length}...\")\n",
    "\n",
    "    triangles = repaired_mesh.faces.reshape((-1, 4))[:, 1:4]\n",
    "    points = repaired_mesh.points\n",
    "\n",
    "    tri_verts = points[triangles]\n",
    "    edge_vecs = np.roll(tri_verts, -1, axis=1) - tri_verts\n",
    "    edge_lengths = np.linalg.norm(edge_vecs, axis=2)\n",
    "    mask = np.all(edge_lengths <= max_edge_length, axis=1)\n",
    "\n",
    "    filtered_triangles = triangles[mask]\n",
    "\n",
    "    flat_faces = np.hstack(\n",
    "        np.column_stack((np.full(len(filtered_triangles), 3, dtype=int), filtered_triangles))\n",
    "    )\n",
    "\n",
    "    final_mesh = pv.PolyData(points, flat_faces)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Final mesh: {final_mesh.n_points} points, {final_mesh.n_faces} faces\")\n",
    "\n",
    "    return final_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting with mesh: 164314 points, 302746 faces\n",
      "[INFO] Found 31312 hole loops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling holes:   0%|          | 3/1249 [00:00<04:30,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hole too large, skipping filling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling holes:   1%|          | 9/1249 [00:01<04:11,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hole too large, skipping filling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling holes:   1%|▏         | 17/1249 [00:03<04:23,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hole too large, skipping filling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling holes: 100%|██████████| 1249/1249 [04:06<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running pymeshfix repair...\n"
     ]
    }
   ],
   "source": [
    "filled_mesh = fill_small_holes_selectively(mesh, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest connected component has 164314 points and 302746 cells\n"
     ]
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "import pymeshfix\n",
    "import numpy as np\n",
    "\n",
    "mesh = pv.read(splatter.config['mesh_info']['mesh'])\n",
    "\n",
    "\n",
    "# Compute connectivity (default to cell connectivity)\n",
    "connectivity = mesh.connectivity()\n",
    "\n",
    "# The 'RegionId' array labels each connected component\n",
    "region_ids = connectivity.cell_data['RegionId']\n",
    "\n",
    "# Count the number of cells in each region\n",
    "unique_ids, counts = np.unique(region_ids, return_counts=True)\n",
    "\n",
    "# Find the region id with the max number of cells\n",
    "largest_region_id = unique_ids[np.argmax(counts)]\n",
    "\n",
    "# Extract cells belonging to largest connected component\n",
    "lcc = connectivity.extract_cells(region_ids == largest_region_id)\n",
    "\n",
    "print(f\"Largest connected component has {lcc.n_points} points and {lcc.n_cells} cells\")\n",
    "\n",
    "# Extract surface as PolyData\n",
    "mesh = lcc.extract_surface()\n",
    "\n",
    "# orig_mesh.plot_boundaries()\n",
    "\n",
    "meshfix = pymeshfix.MeshFix(mesh)\n",
    "# holes = meshfix.extract_holes()\n",
    "\n",
    "# mfix = pymeshfix.PyTMesh(False)  # False removes extra verbose output\n",
    "# mfix.load_file(splatter.config['mesh_info']['mesh'])\n",
    "\n",
    "\n",
    "# # Fills all the holes having at at most 'nbe' boundary edges. If\n",
    "# # 'refine' is true, adds inner vertices to reproduce the sampling\n",
    "# # density of the surroundings. Returns number of holes patched.  If\n",
    "# # 'nbe' is 0 (default), all the holes are patched.\n",
    "# meshfix.fill_small_boundaries(nbe=100, refine=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfix.repair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277972c18c854664827e90e6786813b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:40585/index.html?ui=P_0x700365f90160_7&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render the mesh and outline the holes\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(orig_mesh, color=True)\n",
    "plotter.add_mesh(holes, color=\"r\", line_width=5)\n",
    "plotter.enable_eye_dome_lighting()  # helps depth perception\n",
    "_ = plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1eec6d126b4eb7a88c9373ea4f756f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:40585/index.html?ui=P_0x700368e76ef0_3&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "# mesh = pv.read(splatter.config['mesh_info']['mesh'])\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(filled_mesh) #, scalars='RGB', rgb=True)\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded mesh with 277426 vertices and 494857 faces\n",
      "INFO- Loaded 277426 vertices and 494857 faces.\n",
      "Patching holes...\n",
      "Patched 2522 holes\n",
      "Fixing degeneracies and intersections\n",
      "\n",
      "100% done done \n",
      "INFO- ********* ITERATION 0 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 43012 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 4658 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 1963 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 1 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 1393 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 2265 intersecting triangles have been selected.\n",
      "\n",
      "14 % done   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 % done   \n",
      "INFO- 762 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 2 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 1145 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 575 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 443 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 3 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 289 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 308 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 455 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 4 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 272 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 534 intersecting triangles have been selected.\n",
      "\n",
      "98 % done   \n",
      "INFO- 324 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 5 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 298 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 743 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 288 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 6 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 302 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 242 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 222 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 7 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 220 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 244 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 207 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 8 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 86 intersecting triangles have been selected.\n",
      "\n",
      "98 % done   \n",
      "INFO- 23 intersecting triangles have been selected.\n",
      "\n",
      "96 % done   \n",
      "INFO- 91 intersecting triangles have been selected.\n",
      "INFO- ********* ITERATION 9 *********\n",
      "INFO- Removing degeneracies...\n",
      "INFO- Removing self-intersections...\n",
      "\n",
      "99 % done   \n",
      "INFO- 74 intersecting triangles have been selected.\n",
      "\n",
      "99 % done   \n",
      "INFO- 143 intersecting triangles have been selected.\n",
      "\n",
      "97 % done   \n",
      "INFO- 54 intersecting triangles have been selected.\n"
     ]
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "# mesh = pv.read(splatter.config['mesh_info']['mesh'])\n",
    "\n",
    "mesh = clean_repair_mesh(splatter.config['mesh_info']['mesh'])\n",
    "# mesh.fill_holes()\n",
    "\n",
    "# mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def clean_mesh(mesh, min_cluster_size=1000):\n",
    "\n",
    "    # Method 1: Connected component analysis\n",
    "    triangle_clusters, cluster_n_triangles, cluster_area = (\n",
    "        mesh.cluster_connected_triangles())\n",
    "    triangle_clusters = np.asarray(triangle_clusters)\n",
    "    cluster_n_triangles = np.asarray(cluster_n_triangles)\n",
    "    cluster_area = np.asarray(cluster_area)\n",
    "\n",
    "    print(f\"Found {len(cluster_n_triangles)} connected components\")\n",
    "    print(f\"Component sizes: {cluster_n_triangles}\")\n",
    "\n",
    "    # Remove small components\n",
    "    large_cluster_ids = np.where(cluster_n_triangles > min_cluster_size)[0]\n",
    "\n",
    "    # Create mask for triangles to keep\n",
    "    triangles_to_keep = np.isin(triangle_clusters, large_cluster_ids)\n",
    "\n",
    "    # Remove small components\n",
    "    mesh.remove_triangles_by_mask(~triangles_to_keep)\n",
    "    mesh.remove_unreferenced_vertices()\n",
    "    \n",
    "    # Additional cleanup\n",
    "    mesh.remove_degenerate_triangles()\n",
    "    mesh.remove_duplicated_triangles()\n",
    "    mesh.remove_duplicated_vertices()\n",
    "    mesh.remove_non_manifold_edges()\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 connected components\n",
      "Component sizes: [302746   5573   6016   5463  11909  18670   1676   1780   2339   2017\n",
      "   2191   4042   9860   3720   3050   4876   2490   5448  10420   2969\n",
      "   2528   2207   2877   2631   2537   2267   2761   2940   7396   2348\n",
      "   2376   2689   1839   5457   6696   2601   3542   1938   3534   2402\n",
      "   4030   1825   2396   2108   2142   1952   3033   2882   1703   1965]\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "mesh_path = splatter.config['mesh_info']['mesh']\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "\n",
    "mesh = clean_mesh(mesh, min_cluster_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson repair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
