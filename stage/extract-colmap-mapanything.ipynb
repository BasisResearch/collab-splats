{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MapAnything Preprocessing for Nerfstudio\n",
    "\n",
    "This notebook demonstrates how to use **MapAnything** for image-only structure-from-motion preprocessing, with conversion to COLMAP format for nerfstudio compatibility.\n",
    "\n",
    "## What is MapAnything?\n",
    "\n",
    "MapAnything is a foundation model from Meta Research that:\n",
    "- Performs feedforward inference for 3D reconstruction from images\n",
    "- Estimates camera poses and depth maps directly\n",
    "- Outputs dense 3D point clouds with confidence scores\n",
    "- Supports metric scale estimation\n",
    "- Exports directly to COLMAP format\n",
    "\n",
    "Repository: https://github.com/facebookresearch/map-anything\n",
    "\n",
    "## Workflow\n",
    "\n",
    "### Section 1: Image-Only Inference with MapAnything\n",
    "1. **Load MapAnything Model**: Initialize from HuggingFace pretrained weights\n",
    "2. **Process Images**: Load and preprocess input images\n",
    "3. **Run Inference**: Get camera poses, depth maps, confidence, and 3D points\n",
    "4. **Export to COLMAP**: Convert outputs to COLMAP format with voxel downsampling\n",
    "\n",
    "### Section 2: Visualization\n",
    "5. **Visualize Point Cloud**: Display the reconstructed sparse point cloud\n",
    "6. **Compare with VGGT**: Benchmark against VGGT-X/InfiniteVGGT (optional)\n",
    "\n",
    "### Section 3: Loop Closure Extension\n",
    "7. **Setup Loop Closure**: Configure VGGT-Long with MapAnythingAdapter\n",
    "8. **Run Loop Detection**: Detect and close loops for long sequences\n",
    "9. **Global Optimization**: Refine poses with Sim(3) optimization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install MapAnything\n",
    "pip install git+https://github.com/facebookresearch/map-anything.git\n",
    "\n",
    "# For loop closure (optional)\n",
    "git clone https://github.com/DengKaiCQ/VGGT-Long.git\n",
    "cd VGGT-Long && pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: MapAnything Image-Only Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/conda/envs/nerfstudio/bin/python\n",
      "Python version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "\n",
      "‚úì Running in nerfstudio environment\n",
      "‚úì MapAnything imported successfully\n",
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Verify conda environment\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "if 'nerfstudio' not in sys.executable:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Not running in nerfstudio conda environment!\")\n",
    "    print(\"Please activate with: conda activate nerfstudio\")\n",
    "else:\n",
    "    print(\"\\n‚úì Running in nerfstudio environment\")\n",
    "\n",
    "# Import MapAnything\n",
    "try:\n",
    "    from mapanything.models import MapAnything\n",
    "    from mapanything.utils.image import load_images\n",
    "    print(\"‚úì MapAnything imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå MapAnything import failed: {e}\")\n",
    "    print(\"Please install: pip install git+https://github.com/facebookresearch/map-anything.git\")\n",
    "    raise\n",
    "\n",
    "# Import nerfstudio utilities\n",
    "from nerfstudio.process_data import colmap_utils\n",
    "from nerfstudio.utils.rich_utils import CONSOLE\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths for input images, output directory, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: /workspace/bicycle/images_4\n",
      "Output directory: /workspace/bicycle/environment/bicycle_mapanything\n",
      "COLMAP output: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/0\n",
      "Model: facebook/map-anything\n",
      "\n",
      "Found 194 images\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Dataset Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Input: Image directory (update with your path)\n",
    "image_dir = Path(\"/workspace/bicycle/images_4\")\n",
    "\n",
    "# Output: Will create structure similar to VGGT-X\n",
    "output_base = Path(\"/workspace/bicycle/environment/bicycle_mapanything\")\n",
    "preproc_dir = output_base / \"preproc\"\n",
    "colmap_dir = preproc_dir / \"colmap\"\n",
    "colmap_output_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "image_output_dir = colmap_dir / \"images\"\n",
    "\n",
    "# Create directories\n",
    "colmap_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "image_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# MapAnything Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Model selection\n",
    "model_name = \"facebook/map-anything\"  # Standard model (CC-BY-NC 4.0)\n",
    "# model_name = \"facebook/map-anything-apache\"  # Apache 2.0 licensed alternative\n",
    "\n",
    "# Inference parameters\n",
    "memory_efficient_inference = True  # Use memory-efficient mode\n",
    "minibatch_size = 1                 # Process one image at a time\n",
    "use_amp = True                     # Use automatic mixed precision\n",
    "amp_dtype = \"bf16\"                 # bfloat16 for Ampere+ GPUs, use \"fp16\" for older GPUs\n",
    "apply_mask = True                  # Apply geometric validity mask\n",
    "mask_edges = True                  # Mask image edges for better quality\n",
    "\n",
    "# COLMAP export parameters\n",
    "voxel_fraction = 0.01              # Voxel size as fraction of scene extent (0.01 = 1%)\n",
    "# voxel_size = None                # Explicit voxel size in meters (overrides voxel_fraction)\n",
    "export_glb = False                 # Export dense mesh as GLB (optional)\n",
    "\n",
    "print(f\"Input images: {image_dir}\")\n",
    "print(f\"Output directory: {output_base}\")\n",
    "print(f\"COLMAP output: {colmap_output_dir}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "\n",
    "# Check input images\n",
    "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]\n",
    "image_paths = []\n",
    "for ext in image_extensions:\n",
    "    image_paths.extend(sorted(list(image_dir.glob(ext))))\n",
    "\n",
    "print(f\"\\nFound {len(image_paths)} images\")\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(f\"No images found in {image_dir}\")\n",
    "\n",
    "image_names = [os.path.basename(path) for path in image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-header",
   "metadata": {},
   "source": [
    "## Step 1: Load MapAnything Model\n",
    "\n",
    "Load the pretrained MapAnything model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A40\n",
      "\n",
      "======================================================================\n",
      "LOADING MAPANYTHING MODEL\n",
      "======================================================================\n",
      "Loading pretrained dinov2_vitg14 from torch hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded: facebook/map-anything\n",
      "  Device: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is required for MapAnything inference\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Enable CUDA optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MAPANYTHING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load model from HuggingFace\n",
    "model = MapAnything.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Model loaded: {model_name}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess Images\n",
    "\n",
    "Load images using MapAnything's utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-images",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING IMAGES\n",
      "======================================================================\n",
      "Loading 194 images...\n",
      "‚úì Images loaded: 194 views\n",
      "  First image shape: torch.Size([1, 3, 336, 518])\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING IMAGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert paths to strings\n",
    "image_path_list = [str(p) for p in image_paths]\n",
    "\n",
    "# Load images using MapAnything's load_images function\n",
    "print(f\"Loading {len(image_path_list)} images...\")\n",
    "views = load_images(image_path_list)\n",
    "\n",
    "print(f\"‚úì Images loaded: {len(views)} views\")\n",
    "print(f\"  First image shape: {views[0]['img'].shape if 'img' in views[0] else 'N/A'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-inference-header",
   "metadata": {},
   "source": [
    "## Step 3: Run MapAnything Inference\n",
    "\n",
    "Run feedforward inference to get camera poses, depth maps, and 3D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-inference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING MAPANYTHING INFERENCE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking frustum containment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 326.76it/s]\n",
      "Checking triangle intersections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 81.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INFERENCE COMPLETE\n",
      "======================================================================\n",
      "  Total time: 77.11s\n",
      "  Time per frame: 0.397s\n",
      "  Peak GPU memory: 23.42 GB\n",
      "  Frames processed: 194\n",
      "======================================================================\n",
      "\n",
      "Output keys:\n",
      "  pts3d: torch.Size([1, 336, 518, 3]) (torch.float32)\n",
      "  pts3d_cam: torch.Size([1, 336, 518, 3]) (torch.float32)\n",
      "  ray_directions: torch.Size([1, 336, 518, 3]) (torch.float32)\n",
      "  depth_along_ray: torch.Size([1, 336, 518, 1]) (torch.float32)\n",
      "  cam_trans: torch.Size([1, 3]) (torch.float32)\n",
      "  cam_quats: torch.Size([1, 4]) (torch.float32)\n",
      "  metric_scaling_factor: torch.Size([1, 1]) (torch.float32)\n",
      "  conf: torch.Size([1, 336, 518]) (torch.float32)\n",
      "  non_ambiguous_mask: torch.Size([1, 336, 518]) (torch.bool)\n",
      "  non_ambiguous_mask_logits: torch.Size([1, 336, 518]) (torch.float32)\n",
      "  img_no_norm: torch.Size([1, 336, 518, 3]) (torch.float32)\n",
      "  depth_z: torch.Size([1, 336, 518, 1]) (torch.float32)\n",
      "  intrinsics: torch.Size([1, 3, 3]) (torch.float32)\n",
      "  camera_poses: torch.Size([1, 4, 4]) (torch.float32)\n",
      "  mask: torch.Size([1, 336, 518, 1]) (torch.bool)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING MAPANYTHING INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Track inference time and memory\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.infer(\n",
    "        views,\n",
    "        memory_efficient_inference=memory_efficient_inference,\n",
    "        minibatch_size=minibatch_size,\n",
    "        use_amp=use_amp,\n",
    "        amp_dtype=amp_dtype,\n",
    "        apply_mask=apply_mask,\n",
    "        mask_edges=mask_edges,\n",
    "        apply_confidence_mask=True,\n",
    "        use_multiview_confidence=True,\n",
    "        confidence_percentile=35,\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Total time: {inference_time:.2f}s\")\n",
    "print(f\"  Time per frame: {inference_time / len(image_paths):.3f}s\")\n",
    "print(f\"  Peak GPU memory: {peak_memory_gb:.2f} GB\")\n",
    "print(f\"  Frames processed: {len(image_paths)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Inspect outputs\n",
    "print(\"\\nOutput keys:\")\n",
    "for key in outputs[0].keys():\n",
    "    value = outputs[0][key]\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  {key}: list of {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-colmap-header",
   "metadata": {},
   "source": [
    "## Step 4: Export to COLMAP Format\n",
    "\n",
    "Export MapAnything outputs to COLMAP format with adaptive voxel downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "export-colmap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPORTING TO COLMAP FORMAT\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Exporting to COLMAP format...\n",
      "Total points before downsampling: 19557475\n",
      "Scene extent (IQR-based): 9.167m, full extent: 141.767m\n",
      "Adaptive voxel size: 0.1833m\n",
      "Downsampled from 19557475 to 575435 points\n",
      "Backprojecting points to frames...\n",
      "Built COLMAP reconstruction:\n",
      "  - 194 images\n",
      "  - 575435 3D points\n",
      "  - 11255720 Point2D observations\n",
      "Saved COLMAP reconstruction to: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse\n",
      "Saved point cloud PLY to: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/points.ply\n",
      "Saved 194 processed images to: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/images\n",
      "‚úì Exported to COLMAP format: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap\n",
      "\n",
      "======================================================================\n",
      "COLMAP EXPORT COMPLETE\n",
      "======================================================================\n",
      "Output directory: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/0\n",
      "  - cameras.bin\n",
      "  - images.bin\n",
      "  - points3D.bin\n",
      "  - points.ply\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING TO COLMAP FORMAT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Import export function\n",
    "from mapanything.utils.colmap_export import export_predictions_to_colmap\n",
    "\n",
    "# Get image names for COLMAP output\n",
    "image_names = [os.path.basename(path) for path in image_path_list]\n",
    "\n",
    "# Export to COLMAP format (includes saving processed images)\n",
    "print(\"Exporting to COLMAP format...\")\n",
    "_ = export_predictions_to_colmap(\n",
    "    outputs=outputs,\n",
    "    processed_views=views,\n",
    "    image_names=image_names,\n",
    "    output_dir=colmap_dir,\n",
    "    voxel_fraction=0.02,\n",
    "    voxel_size=None,\n",
    "    data_norm_type=model.encoder.data_norm_type,\n",
    "    save_ply=True,\n",
    "    save_images=True,\n",
    "    skip_point2d=False,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Exported to COLMAP format: {colmap_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLMAP EXPORT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Output directory: {colmap_output_dir}\")\n",
    "print(f\"  - cameras.bin\")\n",
    "print(f\"  - images.bin\")\n",
    "print(f\"  - points3D.bin\")\n",
    "print(f\"  - points.ply\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09cc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Move all files within sparse to sparse/0\n",
    "sparse_dir = Path(\"/workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse\")\n",
    "sparse_0_dir = sparse_dir / \"0\"\n",
    "sparse_0_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for item in sparse_dir.iterdir():\n",
    "    if item.is_file():\n",
    "        shutil.move(str(item), str(sparse_0_dir / item.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert-nerfstudio-header",
   "metadata": {},
   "source": [
    "## Step 5: Convert to Nerfstudio Format\n",
    "\n",
    "Convert the COLMAP reconstruction to nerfstudio's `transforms.json` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convert-nerfstudio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONVERTING TO NERFSTUDIO FORMAT\n",
      "======================================================================\n",
      "Converting COLMAP reconstruction to transforms.json...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: More than one camera is found in /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: More than one camera is found in \u001b[0m\u001b[1;33m/workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/\u001b[0m\u001b[1;33m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{194: Camera(id=194, model='PINHOLE', width=518, height=336, params=array([446.20718384, 446.7557373 , 258.47644043, 168.69714355])), 193: Camera(id=193, model='PINHOLE', width=518, height=336, params=array([443.82510376, 444.42956543, 258.24310303, 168.65664673])), 192: Camera(id=192, model='PINHOLE', width=518, height=336, params=array([439.04541016, 439.10784912, 258.19897461, 168.56225586])), 191: Camera(id=191, model='PINHOLE', width=518, height=336, params=array([463.70431519, 463.10665894, 257.81298828, 168.24563599])), 190: Camera(id=190, model='PINHOLE', width=518, height=336, params=array([472.55310059, 471.3659668 , 258.28079224, 168.24697876])), 189: Camera(id=189, model='PINHOLE', width=518, height=336, params=array([473.02542114, 474.08963013, 258.1192627 , 168.08198547])), 188: Camera(id=188, model='PINHOLE', width=518, height=336, params=array([464.24938965, 466.61468506, 258.05456543, 168.02281189])), 187: Camera(id=187, model='PINHOLE', width=518, height=336, params=array([456.5854187 , 459.79815674, 257.97579956, 168.43307495])), 186: Camera(id=186, model='PINHOLE', width=518, height=336, params=array([460.55929565, 463.18322754, 258.01184082, 168.43017578])), 185: Camera(id=185, model='PINHOLE', width=518, height=336, params=array([445.04806519, 446.11434937, 258.11911011, 168.22679138])), 184: Camera(id=184, model='PINHOLE', width=518, height=336, params=array([451.08724976, 451.60391235, 257.87997437, 168.20472717])), 183: Camera(id=183, model='PINHOLE', width=518, height=336, params=array([445.62265015, 446.31668091, 258.50408936, 168.016922  ])), 182: Camera(id=182, model='PINHOLE', width=518, height=336, params=array([449.03570557, 449.97805786, 258.84899902, 168.44610596])), 181: Camera(id=181, model='PINHOLE', width=518, height=336, params=array([444.10122681, 444.7928772 , 259.11395264, 168.55509949])), 180: Camera(id=180, model='PINHOLE', width=518, height=336, params=array([464.35668945, 463.88250732, 258.73773193, 167.93815613])), 179: Camera(id=179, model='PINHOLE', width=518, height=336, params=array([444.31652832, 444.50891113, 258.39389038, 168.64407349])), 178: Camera(id=178, model='PINHOLE', width=518, height=336, params=array([466.89379883, 466.97824097, 258.41516113, 169.03297424])), 177: Camera(id=177, model='PINHOLE', width=518, height=336, params=array([449.78387451, 450.80834961, 258.05383301, 168.4178772 ])), 176: Camera(id=176, model='PINHOLE', width=518, height=336, params=array([474.03012085, 474.3371582 , 258.85269165, 168.49645996])), 175: Camera(id=175, model='PINHOLE', width=518, height=336, params=array([449.63098145, 450.05474854, 258.11581421, 168.2228241 ])), 174: Camera(id=174, model='PINHOLE', width=518, height=336, params=array([465.46191406, 464.94171143, 259.36013794, 168.25265503])), 173: Camera(id=173, model='PINHOLE', width=518, height=336, params=array([457.48046875, 457.02139282, 257.68936157, 168.37121582])), 172: Camera(id=172, model='PINHOLE', width=518, height=336, params=array([470.87963867, 469.49536133, 259.15310669, 168.19024658])), 171: Camera(id=171, model='PINHOLE', width=518, height=336, params=array([474.00445557, 473.72705078, 258.35769653, 168.16827393])), 170: Camera(id=170, model='PINHOLE', width=518, height=336, params=array([478.940979  , 478.46694946, 258.86871338, 168.11880493])), 169: Camera(id=169, model='PINHOLE', width=518, height=336, params=array([476.61087036, 475.18304443, 258.24627686, 167.99937439])), 168: Camera(id=168, model='PINHOLE', width=518, height=336, params=array([473.55487061, 471.29797363, 258.62091064, 168.29356384])), 167: Camera(id=167, model='PINHOLE', width=518, height=336, params=array([469.29550171, 469.76263428, 258.21966553, 167.89979553])), 166: Camera(id=166, model='PINHOLE', width=518, height=336, params=array([482.06451416, 480.82891846, 258.54837036, 167.83396912])), 165: Camera(id=165, model='PINHOLE', width=518, height=336, params=array([470.2482605 , 472.30230713, 257.73876953, 168.01699829])), 164: Camera(id=164, model='PINHOLE', width=518, height=336, params=array([456.44436646, 455.2649231 , 258.70596313, 167.79304504])), 163: Camera(id=163, model='PINHOLE', width=518, height=336, params=array([467.43548584, 470.45150757, 257.98156738, 168.12889099])), 162: Camera(id=162, model='PINHOLE', width=518, height=336, params=array([464.42675781, 462.77902222, 258.84332275, 167.89978027])), 161: Camera(id=161, model='PINHOLE', width=518, height=336, params=array([461.35870361, 464.66424561, 258.00228882, 168.33148193])), 160: Camera(id=160, model='PINHOLE', width=518, height=336, params=array([477.51092529, 476.23760986, 259.09481812, 167.92630005])), 159: Camera(id=159, model='PINHOLE', width=518, height=336, params=array([465.98776245, 468.37988281, 258.10650635, 168.04286194])), 158: Camera(id=158, model='PINHOLE', width=518, height=336, params=array([470.71817017, 469.3470459 , 258.98321533, 167.68882751])), 157: Camera(id=157, model='PINHOLE', width=518, height=336, params=array([463.52822876, 464.03219604, 258.4899292 , 167.97366333])), 156: Camera(id=156, model='PINHOLE', width=518, height=336, params=array([471.18444824, 469.81060791, 259.77087402, 168.16374207])), 155: Camera(id=155, model='PINHOLE', width=518, height=336, params=array([455.28897095, 455.32928467, 258.60955811, 167.75694275])), 154: Camera(id=154, model='PINHOLE', width=518, height=336, params=array([462.6192627 , 461.94390869, 259.5100708 , 168.02903748])), 153: Camera(id=153, model='PINHOLE', width=518, height=336, params=array([465.83062744, 465.95153809, 258.30038452, 167.83129883])), 152: Camera(id=152, model='PINHOLE', width=518, height=336, params=array([460.80838013, 460.22924805, 259.26293945, 168.5501709 ])), 151: Camera(id=151, model='PINHOLE', width=518, height=336, params=array([474.6854248 , 474.34152222, 258.5909729 , 167.76647949])), 150: Camera(id=150, model='PINHOLE', width=518, height=336, params=array([431.90454102, 430.47747803, 259.52578735, 168.43414307])), 149: Camera(id=149, model='PINHOLE', width=518, height=336, params=array([450.50415039, 449.91592407, 258.8381958 , 167.83428955])), 148: Camera(id=148, model='PINHOLE', width=518, height=336, params=array([457.6998291 , 457.24261475, 258.66293335, 167.94555664])), 147: Camera(id=147, model='PINHOLE', width=518, height=336, params=array([444.16342163, 444.60766602, 258.60992432, 168.05284119])), 146: Camera(id=146, model='PINHOLE', width=518, height=336, params=array([460.84527588, 460.48443604, 258.95095825, 167.59347534])), 145: Camera(id=145, model='PINHOLE', width=518, height=336, params=array([450.31491089, 451.07061768, 258.45220947, 168.02790833])), 144: Camera(id=144, model='PINHOLE', width=518, height=336, params=array([467.2321167 , 466.81182861, 259.4949646 , 167.27738953])), 143: Camera(id=143, model='PINHOLE', width=518, height=336, params=array([447.45281982, 448.19259644, 258.6105957 , 167.74491882])), 142: Camera(id=142, model='PINHOLE', width=518, height=336, params=array([473.38143921, 471.68066406, 258.33380127, 167.84005737])), 141: Camera(id=141, model='PINHOLE', width=518, height=336, params=array([450.83026123, 451.8286438 , 258.76837158, 168.21409607])), 140: Camera(id=140, model='PINHOLE', width=518, height=336, params=array([466.88702393, 465.18887329, 257.87115479, 168.36006165])), 139: Camera(id=139, model='PINHOLE', width=518, height=336, params=array([445.78500366, 446.50158691, 258.74765015, 168.48532104])), 138: Camera(id=138, model='PINHOLE', width=518, height=336, params=array([469.22357178, 468.61987305, 258.33929443, 168.19226074])), 137: Camera(id=137, model='PINHOLE', width=518, height=336, params=array([457.50057983, 458.10437012, 258.46862793, 168.54995728])), 136: Camera(id=136, model='PINHOLE', width=518, height=336, params=array([468.75531006, 469.38562012, 258.55438232, 168.45300293])), 135: Camera(id=135, model='PINHOLE', width=518, height=336, params=array([453.34875488, 453.94857788, 258.30935669, 168.30047607])), 134: Camera(id=134, model='PINHOLE', width=518, height=336, params=array([462.93359375, 463.43328857, 259.14562988, 168.49412537])), 133: Camera(id=133, model='PINHOLE', width=518, height=336, params=array([451.70037842, 451.30709839, 258.17874146, 168.24420166])), 132: Camera(id=132, model='PINHOLE', width=518, height=336, params=array([462.55493164, 462.95413208, 259.42645264, 168.1628418 ])), 131: Camera(id=131, model='PINHOLE', width=518, height=336, params=array([461.48690796, 460.45852661, 257.98812866, 168.31848145])), 130: Camera(id=130, model='PINHOLE', width=518, height=336, params=array([477.85134888, 476.88189697, 258.91607666, 168.32499695])), 129: Camera(id=129, model='PINHOLE', width=518, height=336, params=array([478.91073608, 478.74169922, 258.18789673, 167.94102478])), 128: Camera(id=128, model='PINHOLE', width=518, height=336, params=array([472.2958374 , 469.51950073, 258.5774231 , 167.94610596])), 59: Camera(id=59, model='PINHOLE', width=518, height=336, params=array([470.85037231, 470.99417114, 258.28366089, 168.07299805])), 58: Camera(id=58, model='PINHOLE', width=518, height=336, params=array([471.168396  , 470.13961792, 259.57846069, 168.24024963])), 57: Camera(id=57, model='PINHOLE', width=518, height=336, params=array([463.32998657, 463.38375854, 258.62957764, 168.05767822])), 56: Camera(id=56, model='PINHOLE', width=518, height=336, params=array([477.98431396, 475.34835815, 258.47988892, 168.26327515])), 55: Camera(id=55, model='PINHOLE', width=518, height=336, params=array([470.28106689, 469.65969849, 258.94589233, 167.6325531 ])), 54: Camera(id=54, model='PINHOLE', width=518, height=336, params=array([473.99911499, 471.99676514, 258.49050903, 168.74954224])), 53: Camera(id=53, model='PINHOLE', width=518, height=336, params=array([471.92349243, 473.5796814 , 258.20172119, 168.29693604])), 52: Camera(id=52, model='PINHOLE', width=518, height=336, params=array([461.87283325, 460.17990112, 258.17181396, 169.59065247])), 51: Camera(id=51, model='PINHOLE', width=518, height=336, params=array([464.02416992, 465.36831665, 258.5645752 , 168.45677185])), 50: Camera(id=50, model='PINHOLE', width=518, height=336, params=array([467.71936035, 465.9956665 , 258.0234375 , 169.77322388])), 49: Camera(id=49, model='PINHOLE', width=518, height=336, params=array([462.99172974, 464.60317993, 258.54098511, 168.06492615])), 48: Camera(id=48, model='PINHOLE', width=518, height=336, params=array([463.56524658, 461.97476196, 257.72311401, 169.52700806])), 47: Camera(id=47, model='PINHOLE', width=518, height=336, params=array([466.759552  , 466.96530151, 258.46704102, 167.87069702])), 46: Camera(id=46, model='PINHOLE', width=518, height=336, params=array([462.41775513, 459.96798706, 258.40982056, 168.87944031])), 45: Camera(id=45, model='PINHOLE', width=518, height=336, params=array([469.10894775, 469.85351562, 258.46060181, 167.93037415])), 44: Camera(id=44, model='PINHOLE', width=518, height=336, params=array([467.68051147, 465.10479736, 258.06613159, 169.05484009])), 43: Camera(id=43, model='PINHOLE', width=518, height=336, params=array([459.51629639, 460.20141602, 258.45147705, 168.13479614])), 42: Camera(id=42, model='PINHOLE', width=518, height=336, params=array([469.35385132, 468.1791687 , 258.07089233, 168.39419556])), 41: Camera(id=41, model='PINHOLE', width=518, height=336, params=array([470.24996948, 470.53912354, 258.27539062, 168.04197693])), 40: Camera(id=40, model='PINHOLE', width=518, height=336, params=array([480.00601196, 476.46664429, 258.44076538, 168.28890991])), 39: Camera(id=39, model='PINHOLE', width=518, height=336, params=array([473.20578003, 473.41223145, 258.27713013, 167.91679382])), 38: Camera(id=38, model='PINHOLE', width=518, height=336, params=array([464.11169434, 460.44784546, 258.40353394, 168.55551147])), 37: Camera(id=37, model='PINHOLE', width=518, height=336, params=array([480.49050903, 480.31820679, 258.07620239, 167.98800659])), 36: Camera(id=36, model='PINHOLE', width=518, height=336, params=array([474.58093262, 470.93103027, 258.69049072, 168.5680542 ])), 35: Camera(id=35, model='PINHOLE', width=518, height=336, params=array([487.03118896, 486.69989014, 258.31246948, 168.06886292])), 34: Camera(id=34, model='PINHOLE', width=518, height=336, params=array([480.76553345, 476.97650146, 258.50213623, 168.68190002])), 33: Camera(id=33, model='PINHOLE', width=518, height=336, params=array([487.69555664, 486.15606689, 258.04425049, 167.85188293])), 32: Camera(id=32, model='PINHOLE', width=518, height=336, params=array([474.40496826, 470.89389038, 258.43704224, 168.55343628])), 31: Camera(id=31, model='PINHOLE', width=518, height=336, params=array([479.63061523, 478.85745239, 257.93292236, 167.74607849])), 30: Camera(id=30, model='PINHOLE', width=518, height=336, params=array([466.31652832, 465.0758667 , 258.34396362, 168.84394836])), 13: Camera(id=13, model='PINHOLE', width=518, height=336, params=array([469.681427  , 469.13128662, 259.46115112, 167.59024048])), 12: Camera(id=12, model='PINHOLE', width=518, height=336, params=array([477.69317627, 475.77035522, 258.81439209, 168.00076294])), 11: Camera(id=11, model='PINHOLE', width=518, height=336, params=array([474.69723511, 473.61550903, 259.51416016, 167.96725464])), 10: Camera(id=10, model='PINHOLE', width=518, height=336, params=array([466.0730896 , 464.26873779, 258.75863647, 167.97637939])), 9: Camera(id=9, model='PINHOLE', width=518, height=336, params=array([485.95584106, 483.54171753, 259.53341675, 167.55067444])), 8: Camera(id=8, model='PINHOLE', width=518, height=336, params=array([475.07165527, 473.02328491, 258.19064331, 168.37861633])), 7: Camera(id=7, model='PINHOLE', width=518, height=336, params=array([491.99304199, 490.65744019, 258.78622437, 167.86663818])), 6: Camera(id=6, model='PINHOLE', width=518, height=336, params=array([472.64926147, 471.38522339, 257.61724854, 169.37115479])), 5: Camera(id=5, model='PINHOLE', width=518, height=336, params=array([480.78521729, 481.98953247, 258.58779907, 168.1842041 ])), 4: Camera(id=4, model='PINHOLE', width=518, height=336, params=array([475.5982666 , 474.81970215, 258.1463623 , 168.70196533])), 3: Camera(id=3, model='PINHOLE', width=518, height=336, params=array([471.92623901, 472.13095093, 258.6348877 , 168.13673401])), 2: Camera(id=2, model='PINHOLE', width=518, height=336, params=array([473.68765259, 471.64041138, 258.55148315, 168.58261108])), 1: Camera(id=1, model='PINHOLE', width=518, height=336, params=array([474.20892334, 474.7789917 , 258.62121582, 168.43804932])), 14: Camera(id=14, model='PINHOLE', width=518, height=336, params=array([472.41552734, 471.37631226, 258.99209595, 168.09780884])), 15: Camera(id=15, model='PINHOLE', width=518, height=336, params=array([473.66772461, 475.39459229, 258.91409302, 167.89463806])), 16: Camera(id=16, model='PINHOLE', width=518, height=336, params=array([470.15594482, 468.80856323, 259.02484131, 168.31933594])), 17: Camera(id=17, model='PINHOLE', width=518, height=336, params=array([478.1008606 , 478.40820312, 258.63879395, 167.76600647])), 18: Camera(id=18, model='PINHOLE', width=518, height=336, params=array([469.54055786, 468.07299805, 259.65383911, 169.24128723])), 19: Camera(id=19, model='PINHOLE', width=518, height=336, params=array([466.47808838, 466.75683594, 258.92391968, 167.57754517])), 20: Camera(id=20, model='PINHOLE', width=518, height=336, params=array([477.77355957, 476.07315063, 259.65026855, 168.82904053])), 21: Camera(id=21, model='PINHOLE', width=518, height=336, params=array([472.97903442, 474.14120483, 258.44219971, 168.04232788])), 22: Camera(id=22, model='PINHOLE', width=518, height=336, params=array([465.62527466, 462.07815552, 259.31835938, 168.92660522])), 23: Camera(id=23, model='PINHOLE', width=518, height=336, params=array([480.87475586, 481.73800659, 258.58981323, 168.34457397])), 24: Camera(id=24, model='PINHOLE', width=518, height=336, params=array([457.22283936, 452.52056885, 259.43203735, 168.71868896])), 25: Camera(id=25, model='PINHOLE', width=518, height=336, params=array([476.15188599, 476.9201355 , 258.28152466, 168.22857666])), 26: Camera(id=26, model='PINHOLE', width=518, height=336, params=array([476.56765747, 476.63882446, 259.97167969, 169.67539978])), 27: Camera(id=27, model='PINHOLE', width=518, height=336, params=array([474.03543091, 474.74121094, 258.168396  , 168.33058167])), 28: Camera(id=28, model='PINHOLE', width=518, height=336, params=array([533.27886963, 528.52062988, 258.0067749 , 168.78715515])), 29: Camera(id=29, model='PINHOLE', width=518, height=336, params=array([475.98519897, 476.62445068, 258.11419678, 168.2517395 ])), 60: Camera(id=60, model='PINHOLE', width=518, height=336, params=array([473.6836853 , 472.81283569, 259.2131958 , 167.83309937])), 61: Camera(id=61, model='PINHOLE', width=518, height=336, params=array([471.85452271, 471.22467041, 258.46496582, 168.10395813])), 62: Camera(id=62, model='PINHOLE', width=518, height=336, params=array([476.24594116, 475.4382019 , 259.16442871, 168.07925415])), 63: Camera(id=63, model='PINHOLE', width=518, height=336, params=array([481.08953857, 481.7756958 , 259.03982544, 167.62567139])), 64: Camera(id=64, model='PINHOLE', width=518, height=336, params=array([476.68804932, 476.04122925, 258.42306519, 168.22267151])), 65: Camera(id=65, model='PINHOLE', width=518, height=336, params=array([487.9956665 , 488.94094849, 258.65634155, 167.92442322])), 66: Camera(id=66, model='PINHOLE', width=518, height=336, params=array([481.94503784, 480.39538574, 259.24597168, 168.40504456])), 67: Camera(id=67, model='PINHOLE', width=518, height=336, params=array([484.2215271 , 483.08770752, 258.77084351, 167.68493652])), 68: Camera(id=68, model='PINHOLE', width=518, height=336, params=array([486.33459473, 484.31414795, 259.09854126, 168.37419128])), 69: Camera(id=69, model='PINHOLE', width=518, height=336, params=array([474.76580811, 474.37338257, 258.4017334 , 167.87838745])), 70: Camera(id=70, model='PINHOLE', width=518, height=336, params=array([476.06292725, 473.30706787, 259.20639038, 168.03477478])), 71: Camera(id=71, model='PINHOLE', width=518, height=336, params=array([472.66647339, 472.83831787, 258.52349854, 167.99282837])), 72: Camera(id=72, model='PINHOLE', width=518, height=336, params=array([483.25424194, 481.30206299, 258.71994019, 168.03329468])), 73: Camera(id=73, model='PINHOLE', width=518, height=336, params=array([480.44659424, 480.59890747, 257.97381592, 168.1607666 ])), 74: Camera(id=74, model='PINHOLE', width=518, height=336, params=array([480.9465332 , 479.10031128, 259.1210022 , 168.05038452])), 75: Camera(id=75, model='PINHOLE', width=518, height=336, params=array([483.26739502, 483.8263855 , 258.00094604, 168.36993408])), 76: Camera(id=76, model='PINHOLE', width=518, height=336, params=array([468.94207764, 466.71673584, 258.99710083, 168.11618042])), 77: Camera(id=77, model='PINHOLE', width=518, height=336, params=array([478.06900024, 479.00054932, 257.9552002 , 168.49916077])), 78: Camera(id=78, model='PINHOLE', width=518, height=336, params=array([476.35839844, 475.04891968, 258.64422607, 168.04428101])), 79: Camera(id=79, model='PINHOLE', width=518, height=336, params=array([480.22250366, 480.63900757, 258.27911377, 168.39952087])), 80: Camera(id=80, model='PINHOLE', width=518, height=336, params=array([483.51651001, 483.95684814, 258.23965454, 168.58837891])), 81: Camera(id=81, model='PINHOLE', width=518, height=336, params=array([477.61636353, 477.89215088, 258.02642822, 168.23954773])), 82: Camera(id=82, model='PINHOLE', width=518, height=336, params=array([482.98425293, 481.61965942, 258.12405396, 168.36245728])), 83: Camera(id=83, model='PINHOLE', width=518, height=336, params=array([477.1390686 , 477.59106445, 257.83920288, 168.05410767])), 84: Camera(id=84, model='PINHOLE', width=518, height=336, params=array([482.09301758, 480.51159668, 258.00241089, 168.23573303])), 85: Camera(id=85, model='PINHOLE', width=518, height=336, params=array([480.90023804, 478.88977051, 258.20584106, 168.00949097])), 86: Camera(id=86, model='PINHOLE', width=518, height=336, params=array([482.4029541 , 478.41479492, 258.21774292, 168.1947937 ])), 87: Camera(id=87, model='PINHOLE', width=518, height=336, params=array([480.92614746, 478.41491699, 258.24093628, 168.16661072])), 88: Camera(id=88, model='PINHOLE', width=518, height=336, params=array([477.60440063, 473.70941162, 258.68563843, 168.48358154])), 89: Camera(id=89, model='PINHOLE', width=518, height=336, params=array([480.39126587, 479.80044556, 258.20791626, 168.23194885])), 90: Camera(id=90, model='PINHOLE', width=518, height=336, params=array([472.17745972, 469.17788696, 258.69424438, 168.31436157])), 91: Camera(id=91, model='PINHOLE', width=518, height=336, params=array([468.95901489, 469.04174805, 257.82672119, 168.00082397])), 92: Camera(id=92, model='PINHOLE', width=518, height=336, params=array([479.4630127 , 477.90328979, 258.05117798, 168.81217957])), 93: Camera(id=93, model='PINHOLE', width=518, height=336, params=array([468.17141724, 468.49417114, 257.95349121, 168.17419434])), 94: Camera(id=94, model='PINHOLE', width=518, height=336, params=array([476.12417603, 474.22494507, 258.31259155, 169.11428833])), 95: Camera(id=95, model='PINHOLE', width=518, height=336, params=array([464.23666382, 464.39181519, 258.27770996, 168.1709137 ])), 96: Camera(id=96, model='PINHOLE', width=518, height=336, params=array([475.4050293 , 474.14239502, 258.12814331, 169.13320923])), 97: Camera(id=97, model='PINHOLE', width=518, height=336, params=array([458.92111206, 459.93005371, 258.57516479, 168.11398315])), 98: Camera(id=98, model='PINHOLE', width=518, height=336, params=array([470.05892944, 469.87747192, 259.04354858, 168.85054016])), 99: Camera(id=99, model='PINHOLE', width=518, height=336, params=array([455.67279053, 456.43447876, 258.41137695, 168.12005615])), 100: Camera(id=100, model='PINHOLE', width=518, height=336, params=array([461.74035645, 459.41958618, 258.36453247, 169.12475586])), 101: Camera(id=101, model='PINHOLE', width=518, height=336, params=array([459.70663452, 461.31674194, 258.55606079, 168.5826416 ])), 102: Camera(id=102, model='PINHOLE', width=518, height=336, params=array([467.14117432, 468.16958618, 258.28744507, 168.24266052])), 103: Camera(id=103, model='PINHOLE', width=518, height=336, params=array([460.53540039, 461.36203003, 258.49267578, 168.15916443])), 104: Camera(id=104, model='PINHOLE', width=518, height=336, params=array([474.14047241, 475.3710022 , 258.43136597, 167.7578125 ])), 105: Camera(id=105, model='PINHOLE', width=518, height=336, params=array([456.32400513, 457.0637207 , 258.46621704, 167.7713623 ])), 106: Camera(id=106, model='PINHOLE', width=518, height=336, params=array([475.51593018, 474.12252808, 258.58242798, 167.28269958])), 107: Camera(id=107, model='PINHOLE', width=518, height=336, params=array([448.6524353 , 449.1986084 , 258.43664551, 167.91441345])), 108: Camera(id=108, model='PINHOLE', width=518, height=336, params=array([470.98242188, 469.57424927, 259.20889282, 167.65525818])), 109: Camera(id=109, model='PINHOLE', width=518, height=336, params=array([460.23220825, 459.83380127, 258.50921631, 168.17823792])), 110: Camera(id=110, model='PINHOLE', width=518, height=336, params=array([469.5123291 , 468.39804077, 259.43920898, 167.58554077])), 111: Camera(id=111, model='PINHOLE', width=518, height=336, params=array([466.57623291, 466.53106689, 258.53619385, 168.03147888])), 112: Camera(id=112, model='PINHOLE', width=518, height=336, params=array([466.40158081, 466.04031372, 259.50549316, 167.85270691])), 113: Camera(id=113, model='PINHOLE', width=518, height=336, params=array([475.01641846, 475.1574707 , 258.35910034, 167.94577026])), 114: Camera(id=114, model='PINHOLE', width=518, height=336, params=array([477.80737305, 476.96185303, 259.69372559, 167.74485779])), 115: Camera(id=115, model='PINHOLE', width=518, height=336, params=array([469.60171509, 468.92184448, 258.48687744, 168.02229309])), 116: Camera(id=116, model='PINHOLE', width=518, height=336, params=array([480.93051147, 480.30465698, 259.57275391, 167.84584045])), 117: Camera(id=117, model='PINHOLE', width=518, height=336, params=array([470.8302002 , 470.64828491, 258.27597046, 168.1905365 ])), 118: Camera(id=118, model='PINHOLE', width=518, height=336, params=array([476.27566528, 475.28533936, 258.98266602, 167.54637146])), 119: Camera(id=119, model='PINHOLE', width=518, height=336, params=array([469.77658081, 471.74710083, 258.1578064 , 168.17782593])), 120: Camera(id=120, model='PINHOLE', width=518, height=336, params=array([482.95666504, 482.79104614, 258.68621826, 167.93164062])), 121: Camera(id=121, model='PINHOLE', width=518, height=336, params=array([474.63970947, 477.56521606, 258.12362671, 168.4954834 ])), 122: Camera(id=122, model='PINHOLE', width=518, height=336, params=array([477.20794678, 477.81842041, 258.99316406, 167.65707397])), 123: Camera(id=123, model='PINHOLE', width=518, height=336, params=array([472.31552124, 474.11572266, 258.10043335, 168.10075378])), 124: Camera(id=124, model='PINHOLE', width=518, height=336, params=array([480.75714111, 481.12683105, 258.51016235, 167.98339844])), 125: Camera(id=125, model='PINHOLE', width=518, height=336, params=array([471.61141968, 471.2331543 , 258.00686646, 167.93612671])), 126: Camera(id=126, model='PINHOLE', width=518, height=336, params=array([483.45178223, 480.21566772, 258.07977295, 167.92936707])), 127: Camera(id=127, model='PINHOLE', width=518, height=336, params=array([471.79879761, 470.15563965, 258.04171753, 167.81190491]))}\n",
      "‚úì Created: /workspace/bicycle/environment/bicycle_mapanything/preproc/transforms.json\n",
      "\n",
      "Creating point cloud PLY file...\n",
      "Using MapAnything's exported point cloud: /workspace/bicycle/environment/bicycle_mapanything/preproc/colmap/sparse/0/points.ply\n",
      "‚úì Created: /workspace/bicycle/environment/bicycle_mapanything/preproc/sparse_pc.ply\n",
      "\n",
      "======================================================================\n",
      "CONVERSION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Output files:\n",
      "  üìÅ /workspace/bicycle/environment/bicycle_mapanything/preproc/\n",
      "    üìÑ transforms.json\n",
      "    üìÑ sparse_pc.ply\n",
      "    üìÅ colmap/sparse/\n",
      "      üìÑ cameras.bin\n",
      "      üìÑ images.bin\n",
      "      üìÑ points3D.bin\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERTING TO NERFSTUDIO FORMAT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# MapAnything exports to colmap/sparse/ not colmap/sparse/0/\n",
    "mapanything_colmap_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "\n",
    "# Convert COLMAP to transforms.json\n",
    "print(f\"Converting COLMAP reconstruction to transforms.json...\")\n",
    "colmap_utils.colmap_to_json(\n",
    "    recon_dir=mapanything_colmap_dir,\n",
    "    output_dir=preproc_dir,\n",
    ")\n",
    "\n",
    "transforms_path = preproc_dir / \"transforms.json\"\n",
    "print(f\"‚úì Created: {transforms_path}\")\n",
    "\n",
    "# Load transforms to get applied_transform\n",
    "with open(transforms_path) as f:\n",
    "    transforms = json.load(f)\n",
    "\n",
    "applied_transform = torch.tensor(transforms[\"applied_transform\"])\n",
    "\n",
    "# Create point cloud PLY file\n",
    "ply_filename = \"sparse_pc.ply\"\n",
    "print(f\"\\nCreating point cloud PLY file...\")\n",
    "\n",
    "# Check if MapAnything already exported points.ply\n",
    "mapanything_ply = mapanything_colmap_dir / \"points.ply\"\n",
    "if mapanything_ply.exists():\n",
    "    print(f\"Using MapAnything's exported point cloud: {mapanything_ply}\")\n",
    "    import shutil\n",
    "    shutil.copy(mapanything_ply, preproc_dir / ply_filename)\n",
    "else:\n",
    "    print(f\"Creating point cloud from COLMAP reconstruction...\")\n",
    "    colmap_utils.create_ply_from_colmap(\n",
    "        filename=ply_filename,\n",
    "        recon_dir=mapanything_colmap_dir,\n",
    "        output_dir=preproc_dir,\n",
    "        applied_transform=applied_transform,\n",
    "    )\n",
    "\n",
    "ply_path = preproc_dir / ply_filename\n",
    "print(f\"‚úì Created: {ply_path}\")\n",
    "\n",
    "# Update transforms.json with PLY path\n",
    "transforms[\"ply_file_path\"] = ply_filename\n",
    "with open(transforms_path, 'w') as f:\n",
    "    json.dump(transforms, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERSION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  üìÅ {preproc_dir}/\")\n",
    "print(f\"    üìÑ transforms.json\")\n",
    "print(f\"    üìÑ {ply_filename}\")\n",
    "print(f\"    üìÅ colmap/sparse/\")\n",
    "print(f\"      üìÑ cameras.bin\")\n",
    "print(f\"      üìÑ images.bin\")\n",
    "print(f\"      üìÑ points3D.bin\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rescale-header",
   "metadata": {},
   "source": [
    "## Step 8: Rescale Reconstruction to Original Dimensions\n",
    "\n",
    "MapAnything processes images at a fixed resolution (typically 336x518). We need to rescale the COLMAP reconstruction back to the original image dimensions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rescale-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of timm.models.densenet failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/densenet.py\", line 47, in <module>\n",
      "    class DenseLayer(nn.Module):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/densenet.py\", line 84, in DenseLayer\n",
      "    def forward(self, x):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1094, in _overload_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot currently overload the same method name in two different classes with the same name in the same module\n",
      "]\n",
      "[autoreload of timm.models.dpn failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/dpn.py\", line 52, in <module>\n",
      "    class CatBnAct(nn.Module):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/dpn.py\", line 58, in CatBnAct\n",
      "    def forward(self, x):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1094, in _overload_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot currently overload the same method name in two different classes with the same name in the same module\n",
      "]\n",
      "[autoreload of timm.models.selecsls failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/selecsls.py\", line 56, in <module>\n",
      "    class SequentialList(nn.Sequential):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/timm/models/selecsls.py\", line 62, in SequentialList\n",
      "    def forward(self, x):\n",
      "  File \"/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1094, in _overload_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot currently overload the same method name in two different classes with the same name in the same module\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESCALING RECONSTRUCTION TO ORIGINAL DIMENSIONS\n",
      "======================================================================\n",
      "Original reconstruction:\n",
      "  Cameras: 194\n",
      "  Images: 194\n",
      "  Points3D: 575435\n",
      "\n",
      "Image dimensions:\n",
      "  Model resolution: 518x336\n",
      "  Original resolution: 1236x821 (sample)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Rescaling reconstruction from WxH (518x336) to original dimensions</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mRescaling reconstruction from WxH \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m518x336\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m to original dimensions\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - Original image sizes <span style=\"font-weight: bold\">(</span>WxH<span style=\"font-weight: bold\">)</span>: 1236x821\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  - Original image sizes \u001b[1m(\u001b[0mWxH\u001b[1m)\u001b[0m: 1236x821\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">‚úì Rescaled reconstruction to original dimensions</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m‚úì Rescaled reconstruction to original dimensions\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Wrote rescaled reconstruction to: /workspace/bicycle/environment/bicycle/preproc/colmap/sparse/0\n",
      "\n",
      "Regenerating transforms.json with rescaled cameras...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: More than one camera is found in /workspace/bicycle/environment/bicycle/preproc/colmap/sparse/0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: More than one camera is found in \u001b[0m\u001b[1;33m/workspace/bicycle/environment/bicycle/preproc/colmap/sparse/\u001b[0m\u001b[1;33m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{127: Camera(id=127, model='PINHOLE', width=1236, height=821, params=array([1125.75929313, 1148.80291712,  615.71344183,  410.0403986 ])), 126: Camera(id=126, model='PINHOLE', width=1236, height=821, params=array([483.45178223, 480.21566772, 258.07977295, 167.92936707])), 125: Camera(id=125, model='PINHOLE', width=1236, height=821, params=array([471.61141968, 471.2331543 , 258.00686646, 167.93612671])), 124: Camera(id=124, model='PINHOLE', width=1236, height=821, params=array([480.75714111, 481.12683105, 258.51016235, 167.98339844])), 123: Camera(id=123, model='PINHOLE', width=1236, height=821, params=array([472.31552124, 474.11572266, 258.10043335, 168.10075378])), 122: Camera(id=122, model='PINHOLE', width=1236, height=821, params=array([477.20794678, 477.81842041, 258.99316406, 167.65707397])), 121: Camera(id=121, model='PINHOLE', width=1236, height=821, params=array([474.63970947, 477.56521606, 258.12362671, 168.4954834 ])), 120: Camera(id=120, model='PINHOLE', width=1236, height=821, params=array([482.95666504, 482.79104614, 258.68621826, 167.93164062])), 119: Camera(id=119, model='PINHOLE', width=1236, height=821, params=array([469.77658081, 471.74710083, 258.1578064 , 168.17782593])), 118: Camera(id=118, model='PINHOLE', width=1236, height=821, params=array([476.27566528, 475.28533936, 258.98266602, 167.54637146])), 117: Camera(id=117, model='PINHOLE', width=1236, height=821, params=array([470.8302002 , 470.64828491, 258.27597046, 168.1905365 ])), 116: Camera(id=116, model='PINHOLE', width=1236, height=821, params=array([480.93051147, 480.30465698, 259.57275391, 167.84584045])), 115: Camera(id=115, model='PINHOLE', width=1236, height=821, params=array([469.60171509, 468.92184448, 258.48687744, 168.02229309])), 114: Camera(id=114, model='PINHOLE', width=1236, height=821, params=array([477.80737305, 476.96185303, 259.69372559, 167.74485779])), 113: Camera(id=113, model='PINHOLE', width=1236, height=821, params=array([475.01641846, 475.1574707 , 258.35910034, 167.94577026])), 112: Camera(id=112, model='PINHOLE', width=1236, height=821, params=array([466.40158081, 466.04031372, 259.50549316, 167.85270691])), 111: Camera(id=111, model='PINHOLE', width=1236, height=821, params=array([466.57623291, 466.53106689, 258.53619385, 168.03147888])), 110: Camera(id=110, model='PINHOLE', width=1236, height=821, params=array([469.5123291 , 468.39804077, 259.43920898, 167.58554077])), 109: Camera(id=109, model='PINHOLE', width=1236, height=821, params=array([460.23220825, 459.83380127, 258.50921631, 168.17823792])), 108: Camera(id=108, model='PINHOLE', width=1236, height=821, params=array([470.98242188, 469.57424927, 259.20889282, 167.65525818])), 107: Camera(id=107, model='PINHOLE', width=1236, height=821, params=array([448.6524353 , 449.1986084 , 258.43664551, 167.91441345])), 106: Camera(id=106, model='PINHOLE', width=1236, height=821, params=array([475.51593018, 474.12252808, 258.58242798, 167.28269958])), 105: Camera(id=105, model='PINHOLE', width=1236, height=821, params=array([456.32400513, 457.0637207 , 258.46621704, 167.7713623 ])), 104: Camera(id=104, model='PINHOLE', width=1236, height=821, params=array([474.14047241, 475.3710022 , 258.43136597, 167.7578125 ])), 103: Camera(id=103, model='PINHOLE', width=1236, height=821, params=array([460.53540039, 461.36203003, 258.49267578, 168.15916443])), 102: Camera(id=102, model='PINHOLE', width=1236, height=821, params=array([467.14117432, 468.16958618, 258.28744507, 168.24266052])), 101: Camera(id=101, model='PINHOLE', width=1236, height=821, params=array([459.70663452, 461.31674194, 258.55606079, 168.5826416 ])), 100: Camera(id=100, model='PINHOLE', width=1236, height=821, params=array([461.74035645, 459.41958618, 258.36453247, 169.12475586])), 99: Camera(id=99, model='PINHOLE', width=1236, height=821, params=array([455.67279053, 456.43447876, 258.41137695, 168.12005615])), 98: Camera(id=98, model='PINHOLE', width=1236, height=821, params=array([470.05892944, 469.87747192, 259.04354858, 168.85054016])), 97: Camera(id=97, model='PINHOLE', width=1236, height=821, params=array([458.92111206, 459.93005371, 258.57516479, 168.11398315])), 96: Camera(id=96, model='PINHOLE', width=1236, height=821, params=array([475.4050293 , 474.14239502, 258.12814331, 169.13320923])), 95: Camera(id=95, model='PINHOLE', width=1236, height=821, params=array([464.23666382, 464.39181519, 258.27770996, 168.1709137 ])), 94: Camera(id=94, model='PINHOLE', width=1236, height=821, params=array([476.12417603, 474.22494507, 258.31259155, 169.11428833])), 93: Camera(id=93, model='PINHOLE', width=1236, height=821, params=array([468.17141724, 468.49417114, 257.95349121, 168.17419434])), 92: Camera(id=92, model='PINHOLE', width=1236, height=821, params=array([479.4630127 , 477.90328979, 258.05117798, 168.81217957])), 91: Camera(id=91, model='PINHOLE', width=1236, height=821, params=array([468.95901489, 469.04174805, 257.82672119, 168.00082397])), 90: Camera(id=90, model='PINHOLE', width=1236, height=821, params=array([472.17745972, 469.17788696, 258.69424438, 168.31436157])), 89: Camera(id=89, model='PINHOLE', width=1236, height=821, params=array([480.39126587, 479.80044556, 258.20791626, 168.23194885])), 88: Camera(id=88, model='PINHOLE', width=1236, height=821, params=array([477.60440063, 473.70941162, 258.68563843, 168.48358154])), 87: Camera(id=87, model='PINHOLE', width=1236, height=821, params=array([480.92614746, 478.41491699, 258.24093628, 168.16661072])), 86: Camera(id=86, model='PINHOLE', width=1236, height=821, params=array([482.4029541 , 478.41479492, 258.21774292, 168.1947937 ])), 85: Camera(id=85, model='PINHOLE', width=1236, height=821, params=array([480.90023804, 478.88977051, 258.20584106, 168.00949097])), 84: Camera(id=84, model='PINHOLE', width=1236, height=821, params=array([482.09301758, 480.51159668, 258.00241089, 168.23573303])), 83: Camera(id=83, model='PINHOLE', width=1236, height=821, params=array([477.1390686 , 477.59106445, 257.83920288, 168.05410767])), 82: Camera(id=82, model='PINHOLE', width=1236, height=821, params=array([482.98425293, 481.61965942, 258.12405396, 168.36245728])), 81: Camera(id=81, model='PINHOLE', width=1236, height=821, params=array([477.61636353, 477.89215088, 258.02642822, 168.23954773])), 80: Camera(id=80, model='PINHOLE', width=1236, height=821, params=array([483.51651001, 483.95684814, 258.23965454, 168.58837891])), 79: Camera(id=79, model='PINHOLE', width=1236, height=821, params=array([480.22250366, 480.63900757, 258.27911377, 168.39952087])), 78: Camera(id=78, model='PINHOLE', width=1236, height=821, params=array([476.35839844, 475.04891968, 258.64422607, 168.04428101])), 77: Camera(id=77, model='PINHOLE', width=1236, height=821, params=array([478.06900024, 479.00054932, 257.9552002 , 168.49916077])), 76: Camera(id=76, model='PINHOLE', width=1236, height=821, params=array([468.94207764, 466.71673584, 258.99710083, 168.11618042])), 75: Camera(id=75, model='PINHOLE', width=1236, height=821, params=array([483.26739502, 483.8263855 , 258.00094604, 168.36993408])), 74: Camera(id=74, model='PINHOLE', width=1236, height=821, params=array([480.9465332 , 479.10031128, 259.1210022 , 168.05038452])), 73: Camera(id=73, model='PINHOLE', width=1236, height=821, params=array([480.44659424, 480.59890747, 257.97381592, 168.1607666 ])), 72: Camera(id=72, model='PINHOLE', width=1236, height=821, params=array([483.25424194, 481.30206299, 258.71994019, 168.03329468])), 71: Camera(id=71, model='PINHOLE', width=1236, height=821, params=array([472.66647339, 472.83831787, 258.52349854, 167.99282837])), 70: Camera(id=70, model='PINHOLE', width=1236, height=821, params=array([476.06292725, 473.30706787, 259.20639038, 168.03477478])), 69: Camera(id=69, model='PINHOLE', width=1236, height=821, params=array([474.76580811, 474.37338257, 258.4017334 , 167.87838745])), 68: Camera(id=68, model='PINHOLE', width=1236, height=821, params=array([486.33459473, 484.31414795, 259.09854126, 168.37419128])), 67: Camera(id=67, model='PINHOLE', width=1236, height=821, params=array([484.2215271 , 483.08770752, 258.77084351, 167.68493652])), 66: Camera(id=66, model='PINHOLE', width=1236, height=821, params=array([481.94503784, 480.39538574, 259.24597168, 168.40504456])), 65: Camera(id=65, model='PINHOLE', width=1236, height=821, params=array([487.9956665 , 488.94094849, 258.65634155, 167.92442322])), 64: Camera(id=64, model='PINHOLE', width=1236, height=821, params=array([476.68804932, 476.04122925, 258.42306519, 168.22267151])), 63: Camera(id=63, model='PINHOLE', width=1236, height=821, params=array([481.08953857, 481.7756958 , 259.03982544, 167.62567139])), 62: Camera(id=62, model='PINHOLE', width=1236, height=821, params=array([476.24594116, 475.4382019 , 259.16442871, 168.07925415])), 61: Camera(id=61, model='PINHOLE', width=1236, height=821, params=array([471.85452271, 471.22467041, 258.46496582, 168.10395813])), 136: Camera(id=136, model='PINHOLE', width=1236, height=821, params=array([468.75531006, 469.38562012, 258.55438232, 168.45300293])), 9: Camera(id=9, model='PINHOLE', width=1236, height=821, params=array([485.95584106, 483.54171753, 259.53341675, 167.55067444])), 137: Camera(id=137, model='PINHOLE', width=1236, height=821, params=array([457.50057983, 458.10437012, 258.46862793, 168.54995728])), 10: Camera(id=10, model='PINHOLE', width=1236, height=821, params=array([466.0730896 , 464.26873779, 258.75863647, 167.97637939])), 138: Camera(id=138, model='PINHOLE', width=1236, height=821, params=array([469.22357178, 468.61987305, 258.33929443, 168.19226074])), 11: Camera(id=11, model='PINHOLE', width=1236, height=821, params=array([474.69723511, 473.61550903, 259.51416016, 167.96725464])), 139: Camera(id=139, model='PINHOLE', width=1236, height=821, params=array([445.78500366, 446.50158691, 258.74765015, 168.48532104])), 12: Camera(id=12, model='PINHOLE', width=1236, height=821, params=array([477.69317627, 475.77035522, 258.81439209, 168.00076294])), 140: Camera(id=140, model='PINHOLE', width=1236, height=821, params=array([466.88702393, 465.18887329, 257.87115479, 168.36006165])), 13: Camera(id=13, model='PINHOLE', width=1236, height=821, params=array([469.681427  , 469.13128662, 259.46115112, 167.59024048])), 141: Camera(id=141, model='PINHOLE', width=1236, height=821, params=array([450.83026123, 451.8286438 , 258.76837158, 168.21409607])), 14: Camera(id=14, model='PINHOLE', width=1236, height=821, params=array([472.41552734, 471.37631226, 258.99209595, 168.09780884])), 142: Camera(id=142, model='PINHOLE', width=1236, height=821, params=array([473.38143921, 471.68066406, 258.33380127, 167.84005737])), 15: Camera(id=15, model='PINHOLE', width=1236, height=821, params=array([473.66772461, 475.39459229, 258.91409302, 167.89463806])), 143: Camera(id=143, model='PINHOLE', width=1236, height=821, params=array([447.45281982, 448.19259644, 258.6105957 , 167.74491882])), 16: Camera(id=16, model='PINHOLE', width=1236, height=821, params=array([470.15594482, 468.80856323, 259.02484131, 168.31933594])), 144: Camera(id=144, model='PINHOLE', width=1236, height=821, params=array([467.2321167 , 466.81182861, 259.4949646 , 167.27738953])), 17: Camera(id=17, model='PINHOLE', width=1236, height=821, params=array([478.1008606 , 478.40820312, 258.63879395, 167.76600647])), 145: Camera(id=145, model='PINHOLE', width=1236, height=821, params=array([450.31491089, 451.07061768, 258.45220947, 168.02790833])), 18: Camera(id=18, model='PINHOLE', width=1236, height=821, params=array([469.54055786, 468.07299805, 259.65383911, 169.24128723])), 146: Camera(id=146, model='PINHOLE', width=1236, height=821, params=array([460.84527588, 460.48443604, 258.95095825, 167.59347534])), 19: Camera(id=19, model='PINHOLE', width=1236, height=821, params=array([466.47808838, 466.75683594, 258.92391968, 167.57754517])), 147: Camera(id=147, model='PINHOLE', width=1236, height=821, params=array([444.16342163, 444.60766602, 258.60992432, 168.05284119])), 20: Camera(id=20, model='PINHOLE', width=1236, height=821, params=array([477.77355957, 476.07315063, 259.65026855, 168.82904053])), 148: Camera(id=148, model='PINHOLE', width=1236, height=821, params=array([457.6998291 , 457.24261475, 258.66293335, 167.94555664])), 21: Camera(id=21, model='PINHOLE', width=1236, height=821, params=array([472.97903442, 474.14120483, 258.44219971, 168.04232788])), 149: Camera(id=149, model='PINHOLE', width=1236, height=821, params=array([450.50415039, 449.91592407, 258.8381958 , 167.83428955])), 22: Camera(id=22, model='PINHOLE', width=1236, height=821, params=array([465.62527466, 462.07815552, 259.31835938, 168.92660522])), 150: Camera(id=150, model='PINHOLE', width=1236, height=821, params=array([431.90454102, 430.47747803, 259.52578735, 168.43414307])), 23: Camera(id=23, model='PINHOLE', width=1236, height=821, params=array([480.87475586, 481.73800659, 258.58981323, 168.34457397])), 151: Camera(id=151, model='PINHOLE', width=1236, height=821, params=array([474.6854248 , 474.34152222, 258.5909729 , 167.76647949])), 24: Camera(id=24, model='PINHOLE', width=1236, height=821, params=array([457.22283936, 452.52056885, 259.43203735, 168.71868896])), 152: Camera(id=152, model='PINHOLE', width=1236, height=821, params=array([460.80838013, 460.22924805, 259.26293945, 168.5501709 ])), 25: Camera(id=25, model='PINHOLE', width=1236, height=821, params=array([476.15188599, 476.9201355 , 258.28152466, 168.22857666])), 153: Camera(id=153, model='PINHOLE', width=1236, height=821, params=array([465.83062744, 465.95153809, 258.30038452, 167.83129883])), 26: Camera(id=26, model='PINHOLE', width=1236, height=821, params=array([476.56765747, 476.63882446, 259.97167969, 169.67539978])), 154: Camera(id=154, model='PINHOLE', width=1236, height=821, params=array([462.6192627 , 461.94390869, 259.5100708 , 168.02903748])), 27: Camera(id=27, model='PINHOLE', width=1236, height=821, params=array([474.03543091, 474.74121094, 258.168396  , 168.33058167])), 155: Camera(id=155, model='PINHOLE', width=1236, height=821, params=array([455.28897095, 455.32928467, 258.60955811, 167.75694275])), 28: Camera(id=28, model='PINHOLE', width=1236, height=821, params=array([533.27886963, 528.52062988, 258.0067749 , 168.78715515])), 156: Camera(id=156, model='PINHOLE', width=1236, height=821, params=array([471.18444824, 469.81060791, 259.77087402, 168.16374207])), 29: Camera(id=29, model='PINHOLE', width=1236, height=821, params=array([475.98519897, 476.62445068, 258.11419678, 168.2517395 ])), 157: Camera(id=157, model='PINHOLE', width=1236, height=821, params=array([463.52822876, 464.03219604, 258.4899292 , 167.97366333])), 30: Camera(id=30, model='PINHOLE', width=1236, height=821, params=array([466.31652832, 465.0758667 , 258.34396362, 168.84394836])), 158: Camera(id=158, model='PINHOLE', width=1236, height=821, params=array([470.71817017, 469.3470459 , 258.98321533, 167.68882751])), 31: Camera(id=31, model='PINHOLE', width=1236, height=821, params=array([479.63061523, 478.85745239, 257.93292236, 167.74607849])), 159: Camera(id=159, model='PINHOLE', width=1236, height=821, params=array([465.98776245, 468.37988281, 258.10650635, 168.04286194])), 32: Camera(id=32, model='PINHOLE', width=1236, height=821, params=array([474.40496826, 470.89389038, 258.43704224, 168.55343628])), 160: Camera(id=160, model='PINHOLE', width=1236, height=821, params=array([477.51092529, 476.23760986, 259.09481812, 167.92630005])), 33: Camera(id=33, model='PINHOLE', width=1236, height=821, params=array([487.69555664, 486.15606689, 258.04425049, 167.85188293])), 161: Camera(id=161, model='PINHOLE', width=1236, height=821, params=array([461.35870361, 464.66424561, 258.00228882, 168.33148193])), 34: Camera(id=34, model='PINHOLE', width=1236, height=821, params=array([480.76553345, 476.97650146, 258.50213623, 168.68190002])), 162: Camera(id=162, model='PINHOLE', width=1236, height=821, params=array([464.42675781, 462.77902222, 258.84332275, 167.89978027])), 35: Camera(id=35, model='PINHOLE', width=1236, height=821, params=array([487.03118896, 486.69989014, 258.31246948, 168.06886292])), 163: Camera(id=163, model='PINHOLE', width=1236, height=821, params=array([467.43548584, 470.45150757, 257.98156738, 168.12889099])), 36: Camera(id=36, model='PINHOLE', width=1236, height=821, params=array([474.58093262, 470.93103027, 258.69049072, 168.5680542 ])), 164: Camera(id=164, model='PINHOLE', width=1236, height=821, params=array([456.44436646, 455.2649231 , 258.70596313, 167.79304504])), 37: Camera(id=37, model='PINHOLE', width=1236, height=821, params=array([480.49050903, 480.31820679, 258.07620239, 167.98800659])), 165: Camera(id=165, model='PINHOLE', width=1236, height=821, params=array([470.2482605 , 472.30230713, 257.73876953, 168.01699829])), 38: Camera(id=38, model='PINHOLE', width=1236, height=821, params=array([464.11169434, 460.44784546, 258.40353394, 168.55551147])), 182: Camera(id=182, model='PINHOLE', width=1236, height=821, params=array([449.03570557, 449.97805786, 258.84899902, 168.44610596])), 55: Camera(id=55, model='PINHOLE', width=1236, height=821, params=array([470.28106689, 469.65969849, 258.94589233, 167.6325531 ])), 183: Camera(id=183, model='PINHOLE', width=1236, height=821, params=array([445.62265015, 446.31668091, 258.50408936, 168.016922  ])), 56: Camera(id=56, model='PINHOLE', width=1236, height=821, params=array([477.98431396, 475.34835815, 258.47988892, 168.26327515])), 184: Camera(id=184, model='PINHOLE', width=1236, height=821, params=array([451.08724976, 451.60391235, 257.87997437, 168.20472717])), 57: Camera(id=57, model='PINHOLE', width=1236, height=821, params=array([463.32998657, 463.38375854, 258.62957764, 168.05767822])), 185: Camera(id=185, model='PINHOLE', width=1236, height=821, params=array([445.04806519, 446.11434937, 258.11911011, 168.22679138])), 58: Camera(id=58, model='PINHOLE', width=1236, height=821, params=array([471.168396  , 470.13961792, 259.57846069, 168.24024963])), 186: Camera(id=186, model='PINHOLE', width=1236, height=821, params=array([460.55929565, 463.18322754, 258.01184082, 168.43017578])), 59: Camera(id=59, model='PINHOLE', width=1236, height=821, params=array([470.85037231, 470.99417114, 258.28366089, 168.07299805])), 187: Camera(id=187, model='PINHOLE', width=1236, height=821, params=array([456.5854187 , 459.79815674, 257.97579956, 168.43307495])), 60: Camera(id=60, model='PINHOLE', width=1236, height=821, params=array([473.6836853 , 472.81283569, 259.2131958 , 167.83309937])), 188: Camera(id=188, model='PINHOLE', width=1236, height=821, params=array([464.24938965, 466.61468506, 258.05456543, 168.02281189])), 189: Camera(id=189, model='PINHOLE', width=1236, height=821, params=array([473.02542114, 474.08963013, 258.1192627 , 168.08198547])), 190: Camera(id=190, model='PINHOLE', width=1236, height=821, params=array([472.55310059, 471.3659668 , 258.28079224, 168.24697876])), 191: Camera(id=191, model='PINHOLE', width=1236, height=821, params=array([463.70431519, 463.10665894, 257.81298828, 168.24563599])), 192: Camera(id=192, model='PINHOLE', width=1236, height=821, params=array([439.04541016, 439.10784912, 258.19897461, 168.56225586])), 193: Camera(id=193, model='PINHOLE', width=1236, height=821, params=array([443.82510376, 444.42956543, 258.24310303, 168.65664673])), 194: Camera(id=194, model='PINHOLE', width=1236, height=821, params=array([446.20718384, 446.7557373 , 258.47644043, 168.69714355])), 181: Camera(id=181, model='PINHOLE', width=1236, height=821, params=array([444.10122681, 444.7928772 , 259.11395264, 168.55509949])), 54: Camera(id=54, model='PINHOLE', width=1236, height=821, params=array([473.99911499, 471.99676514, 258.49050903, 168.74954224])), 180: Camera(id=180, model='PINHOLE', width=1236, height=821, params=array([464.35668945, 463.88250732, 258.73773193, 167.93815613])), 53: Camera(id=53, model='PINHOLE', width=1236, height=821, params=array([471.92349243, 473.5796814 , 258.20172119, 168.29693604])), 179: Camera(id=179, model='PINHOLE', width=1236, height=821, params=array([444.31652832, 444.50891113, 258.39389038, 168.64407349])), 52: Camera(id=52, model='PINHOLE', width=1236, height=821, params=array([461.87283325, 460.17990112, 258.17181396, 169.59065247])), 178: Camera(id=178, model='PINHOLE', width=1236, height=821, params=array([466.89379883, 466.97824097, 258.41516113, 169.03297424])), 51: Camera(id=51, model='PINHOLE', width=1236, height=821, params=array([464.02416992, 465.36831665, 258.5645752 , 168.45677185])), 177: Camera(id=177, model='PINHOLE', width=1236, height=821, params=array([449.78387451, 450.80834961, 258.05383301, 168.4178772 ])), 50: Camera(id=50, model='PINHOLE', width=1236, height=821, params=array([467.71936035, 465.9956665 , 258.0234375 , 169.77322388])), 176: Camera(id=176, model='PINHOLE', width=1236, height=821, params=array([474.03012085, 474.3371582 , 258.85269165, 168.49645996])), 49: Camera(id=49, model='PINHOLE', width=1236, height=821, params=array([462.99172974, 464.60317993, 258.54098511, 168.06492615])), 175: Camera(id=175, model='PINHOLE', width=1236, height=821, params=array([449.63098145, 450.05474854, 258.11581421, 168.2228241 ])), 48: Camera(id=48, model='PINHOLE', width=1236, height=821, params=array([463.56524658, 461.97476196, 257.72311401, 169.52700806])), 174: Camera(id=174, model='PINHOLE', width=1236, height=821, params=array([465.46191406, 464.94171143, 259.36013794, 168.25265503])), 47: Camera(id=47, model='PINHOLE', width=1236, height=821, params=array([466.759552  , 466.96530151, 258.46704102, 167.87069702])), 173: Camera(id=173, model='PINHOLE', width=1236, height=821, params=array([457.48046875, 457.02139282, 257.68936157, 168.37121582])), 46: Camera(id=46, model='PINHOLE', width=1236, height=821, params=array([462.41775513, 459.96798706, 258.40982056, 168.87944031])), 172: Camera(id=172, model='PINHOLE', width=1236, height=821, params=array([470.87963867, 469.49536133, 259.15310669, 168.19024658])), 45: Camera(id=45, model='PINHOLE', width=1236, height=821, params=array([469.10894775, 469.85351562, 258.46060181, 167.93037415])), 171: Camera(id=171, model='PINHOLE', width=1236, height=821, params=array([474.00445557, 473.72705078, 258.35769653, 168.16827393])), 44: Camera(id=44, model='PINHOLE', width=1236, height=821, params=array([467.68051147, 465.10479736, 258.06613159, 169.05484009])), 170: Camera(id=170, model='PINHOLE', width=1236, height=821, params=array([478.940979  , 478.46694946, 258.86871338, 168.11880493])), 43: Camera(id=43, model='PINHOLE', width=1236, height=821, params=array([459.51629639, 460.20141602, 258.45147705, 168.13479614])), 169: Camera(id=169, model='PINHOLE', width=1236, height=821, params=array([476.61087036, 475.18304443, 258.24627686, 167.99937439])), 42: Camera(id=42, model='PINHOLE', width=1236, height=821, params=array([469.35385132, 468.1791687 , 258.07089233, 168.39419556])), 168: Camera(id=168, model='PINHOLE', width=1236, height=821, params=array([473.55487061, 471.29797363, 258.62091064, 168.29356384])), 41: Camera(id=41, model='PINHOLE', width=1236, height=821, params=array([470.24996948, 470.53912354, 258.27539062, 168.04197693])), 167: Camera(id=167, model='PINHOLE', width=1236, height=821, params=array([469.29550171, 469.76263428, 258.21966553, 167.89979553])), 40: Camera(id=40, model='PINHOLE', width=1236, height=821, params=array([480.00601196, 476.46664429, 258.44076538, 168.28890991])), 166: Camera(id=166, model='PINHOLE', width=1236, height=821, params=array([482.06451416, 480.82891846, 258.54837036, 167.83396912])), 39: Camera(id=39, model='PINHOLE', width=1236, height=821, params=array([473.20578003, 473.41223145, 258.27713013, 167.91679382])), 135: Camera(id=135, model='PINHOLE', width=1236, height=821, params=array([453.34875488, 453.94857788, 258.30935669, 168.30047607])), 8: Camera(id=8, model='PINHOLE', width=1236, height=821, params=array([475.07165527, 473.02328491, 258.19064331, 168.37861633])), 134: Camera(id=134, model='PINHOLE', width=1236, height=821, params=array([462.93359375, 463.43328857, 259.14562988, 168.49412537])), 7: Camera(id=7, model='PINHOLE', width=1236, height=821, params=array([491.99304199, 490.65744019, 258.78622437, 167.86663818])), 133: Camera(id=133, model='PINHOLE', width=1236, height=821, params=array([451.70037842, 451.30709839, 258.17874146, 168.24420166])), 6: Camera(id=6, model='PINHOLE', width=1236, height=821, params=array([472.64926147, 471.38522339, 257.61724854, 169.37115479])), 132: Camera(id=132, model='PINHOLE', width=1236, height=821, params=array([462.55493164, 462.95413208, 259.42645264, 168.1628418 ])), 5: Camera(id=5, model='PINHOLE', width=1236, height=821, params=array([480.78521729, 481.98953247, 258.58779907, 168.1842041 ])), 131: Camera(id=131, model='PINHOLE', width=1236, height=821, params=array([461.48690796, 460.45852661, 257.98812866, 168.31848145])), 4: Camera(id=4, model='PINHOLE', width=1236, height=821, params=array([475.5982666 , 474.81970215, 258.1463623 , 168.70196533])), 130: Camera(id=130, model='PINHOLE', width=1236, height=821, params=array([477.85134888, 476.88189697, 258.91607666, 168.32499695])), 3: Camera(id=3, model='PINHOLE', width=1236, height=821, params=array([471.92623901, 472.13095093, 258.6348877 , 168.13673401])), 129: Camera(id=129, model='PINHOLE', width=1236, height=821, params=array([478.91073608, 478.74169922, 258.18789673, 167.94102478])), 2: Camera(id=2, model='PINHOLE', width=1236, height=821, params=array([473.68765259, 471.64041138, 258.55148315, 168.58261108])), 128: Camera(id=128, model='PINHOLE', width=1236, height=821, params=array([472.2958374 , 469.51950073, 258.5774231 , 167.94610596])), 1: Camera(id=1, model='PINHOLE', width=1236, height=821, params=array([474.20892334, 474.7789917 , 258.62121582, 168.43804932]))}\n",
      "‚úì Updated transforms.json: /workspace/bicycle/environment/bicycle/preproc/transforms.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pycolmap\n",
    "from nerfstudio.process_data.vggt_utils import _rescale_reconstruction_to_original_dimensions\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESCALING RECONSTRUCTION TO ORIGINAL DIMENSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the COLMAP reconstruction\n",
    "map_anything_recon_dir = mapanything_colmap_dir / \"sparse\" / \"0\"\n",
    "reconstruction = pycolmap.Reconstruction(str(map_anything_recon_dir))\n",
    "\n",
    "recon_dir = Path('/workspace/bicycle/environment/bicycle/preproc/colmap/sparse/0')\n",
    "rescaled_dir = Path('/workspace/bicycle/environment/bicycle/preproc')\n",
    "\n",
    "recon_dir.mkdir(parents=True, exist_ok=True)\n",
    "rescaled_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Original reconstruction:\")\n",
    "print(f\"  Cameras: {len(reconstruction.cameras)}\")\n",
    "print(f\"  Images: {len(reconstruction.images)}\")\n",
    "print(f\"  Points3D: {len(reconstruction.points3D)}\")\n",
    "\n",
    "# Get original image sizes\n",
    "original_coords = []\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path)\n",
    "    # Format: [top_left_x, top_left_y, crop_right, crop_bottom, original_width, original_height]\n",
    "    # For MapAnything, images are resized without cropping, so top_left = (0, 0)\n",
    "    # and crop dimensions = model resolution\n",
    "    model_width = views[0]['img'].shape[-1]  # Width from loaded views\n",
    "    model_height = views[0]['img'].shape[-2]  # Height from loaded views\n",
    "    original_coords.append([0, 0, model_width, model_height, img.width, img.height])\n",
    "\n",
    "original_coords = np.array(original_coords)\n",
    "\n",
    "print(f\"\\nImage dimensions:\")\n",
    "print(f\"  Model resolution: {model_width}x{model_height}\")\n",
    "print(f\"  Original resolution: {original_coords[0, -2]}x{original_coords[0, -1]} (sample)\")\n",
    "\n",
    "# Rescale reconstruction\n",
    "reconstruction = _rescale_reconstruction_to_original_dimensions(\n",
    "    reconstruction=reconstruction,\n",
    "    image_paths=image_paths,\n",
    "    original_image_sizes=original_coords,\n",
    "    image_size=(model_width, model_height),\n",
    "    shared_camera=True,  # MapAnything uses per-image cameras\n",
    "    shift_point2d_to_original_res=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Write rescaled reconstruction back\n",
    "reconstruction.write_binary(str(recon_dir))\n",
    "print(f\"\\n‚úì Wrote rescaled reconstruction to: {recon_dir}\")\n",
    "\n",
    "# Regenerate transforms.json with rescaled reconstruction\n",
    "print(f\"\\nRegenerating transforms.json with rescaled cameras...\")\n",
    "colmap_utils.colmap_to_json(\n",
    "    recon_dir=recon_dir,\n",
    "    output_dir=rescaled_dir,\n",
    ")\n",
    "\n",
    "# Update transforms.json with PLY path\n",
    "transforms_path = rescaled_dir / \"transforms.json\"\n",
    "with open(transforms_path) as f:\n",
    "    transforms = json.load(f)\n",
    "\n",
    "transforms[\"ply_file_path\"] = ply_filename\n",
    "with open(transforms_path, 'w') as f:\n",
    "    json.dump(transforms, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Updated transforms.json: {transforms_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sparse Point Cloud\n",
    "\n",
    "Visualize the reconstructed point cloud using PyVista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "visualize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading point cloud from: /workspace/bicycle/environment/bicycle_mapanything/preproc/sparse_pc.ply\n",
      "  Points: 575,435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb636c91766945018179e852498f88bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:35325/index.html?ui=P_0x7127fd4aae30_1&reconnect=auto\" class=\"pyvi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "# Optional: Import visualization utilities if available\n",
    "try:\n",
    "    from collab_splats.utils.visualization import (\n",
    "        CAMERA_KWARGS,\n",
    "        MESH_KWARGS,\n",
    "        VIZ_KWARGS,\n",
    "        visualize_splat,\n",
    "    )\n",
    "    has_viz_utils = True\n",
    "except ImportError:\n",
    "    has_viz_utils = False\n",
    "    print(\"collab_splats visualization utilities not available, using basic PyVista\")\n",
    "\n",
    "print(f\"\\nLoading point cloud from: {ply_path}\")\n",
    "point_cloud = pv.PolyData(str(ply_path))\n",
    "point_cloud.point_data['RGB'] = point_cloud.point_data['RGBA']\n",
    "print(f\"  Points: {point_cloud.n_points:,}\")\n",
    "\n",
    "if has_viz_utils:\n",
    "    # Use collab_splats visualization\n",
    "    pcd_kwargs = MESH_KWARGS.copy()\n",
    "    pcd_kwargs.update({\n",
    "        \"point_size\": 2,\n",
    "        \"render_points_as_spheres\": True,\n",
    "        \"ambient\": 0.3,\n",
    "        \"diffuse\": 0.8,\n",
    "        \"specular\": 0.1,\n",
    "    })\n",
    "    \n",
    "    plotter = visualize_splat(\n",
    "        mesh=point_cloud,\n",
    "        mesh_kwargs=pcd_kwargs,\n",
    "        viz_kwargs=VIZ_KWARGS,\n",
    "    )\n",
    "else:\n",
    "    # Basic PyVista visualization\n",
    "    plotter = pv.Plotter()\n",
    "    plotter.add_mesh(\n",
    "        point_cloud,\n",
    "        point_size=2,\n",
    "        render_points_as_spheres=True,\n",
    "    )\n",
    "    plotter.add_axes()\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perf-summary-header",
   "metadata": {},
   "source": [
    "## Step 7: Performance Summary\n",
    "\n",
    "Display comprehensive performance metrics for MapAnything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "perf-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MAPANYTHING PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Dataset: images_4\n",
      "  Images: 194\n",
      "\n",
      "Inference Performance:\n",
      "  Total time: 94.99s\n",
      "  Per frame: 0.490s\n",
      "  FPS: 2.04\n",
      "  Peak GPU memory: 40.66 GB\n",
      "\n",
      "Reconstruction Quality:\n",
      "  Point cloud points: 575,366\n",
      "  Voxel downsampling: 1.0% of scene extent\n",
      "\n",
      "Output Directory: /workspace/bicycle/environment/bicycle_mapanything/preproc\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MAPANYTHING PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset: {image_dir.name}\")\n",
    "print(f\"  Images: {len(image_paths)}\")\n",
    "\n",
    "print(f\"\\nInference Performance:\")\n",
    "print(f\"  Total time: {inference_time:.2f}s\")\n",
    "print(f\"  Per frame: {inference_time / len(image_paths):.3f}s\")\n",
    "print(f\"  FPS: {len(image_paths) / inference_time:.2f}\")\n",
    "print(f\"  Peak GPU memory: {peak_memory_gb:.2f} GB\")\n",
    "\n",
    "print(f\"\\nReconstruction Quality:\")\n",
    "print(f\"  Point cloud points: {point_cloud.n_points:,}\")\n",
    "print(f\"  Voxel downsampling: {voxel_fraction * 100:.1f}% of scene extent\")\n",
    "\n",
    "print(f\"\\nOutput Directory: {preproc_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-splat-header",
   "metadata": {},
   "source": [
    "## Step 9: Train Gaussian Splatting Model\n",
    "\n",
    "Train a Gaussian Splatting model using the preprocessed MapAnything data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-splat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_splats.wrapper import Splatter\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING GAUSSIAN SPLATTING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create splatter pointing to MapAnything preprocessing output\n",
    "splatter = Splatter(\n",
    "    dataset=image_dir.name,\n",
    "    method=\"rade-gs\",\n",
    "    file_path=image_dir,\n",
    "    input_type=\"images\",\n",
    "    output_path=output_base,\n",
    "    preproc_data_path=preproc_dir,  # Use existing MapAnything preprocessing\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset: {splatter.config['dataset']}\")\n",
    "print(f\"Method: {splatter.config['method']}\")\n",
    "print(f\"Preprocessing: {preproc_dir}\")\n",
    "print(f\"Output: {output_base}\")\n",
    "\n",
    "# Training configuration\n",
    "training_kwargs = {\n",
    "    \"pipeline.model.cull-alpha-thresh\": 0.005,\n",
    "    \"pipeline.model.continue-cull-post-densification\": False,\n",
    "    \"pipeline.model.output-depth-during-training\": True,\n",
    "    \"pipeline.model.rasterize-mode\": \"antialiased\",\n",
    "    \"optimizers.xyz.optimizer\": \"Adam\",\n",
    "    \"optimizers.xyz.scheduler\": \"ExponentialDecayScheduler\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "for key, value in training_kwargs.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nStarting training...\")\n",
    "splatter.train(\n",
    "    kwargs=training_kwargs,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel saved to: {output_base / 'outputs'}\")\n",
    "print(f\"\\nTo visualize the trained model:\")\n",
    "print(f\"  ns-viewer --load-config {output_base / 'outputs' / 'config.yml'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Loop Closure Extension\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loop-closure-overview",
   "metadata": {},
   "source": [
    "## Overview: VGGT-Long Loop Closure\n",
    "\n",
    "**VGGT-Long** extends foundation models like MapAnything with loop closure capabilities for long video sequences. This is particularly useful for:\n",
    "- Long video sequences with revisited locations\n",
    "- Reducing drift in camera pose estimation\n",
    "- Improving global consistency of 3D reconstructions\n",
    "\n",
    "Repository: https://github.com/DengKaiCQ/VGGT-Long\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The VGGT-Long system uses an **adapter pattern** to support multiple foundation models:\n",
    "- `VGGTAdapter`: For VGGT models\n",
    "- `Pi3Adapter`: For Pi3 models\n",
    "- `MapAnythingAdapter`: For MapAnything models\n",
    "\n",
    "### Loop Closure Pipeline\n",
    "\n",
    "1. **Chunked Inference**: Process video in overlapping chunks\n",
    "2. **Loop Detection**: Detect revisited locations using:\n",
    "   - DBoW2-based retrieval\n",
    "   - DNIO v2 loop detector\n",
    "3. **Loop Alignment**: Align detected loops using Sim(3) transformation\n",
    "4. **Global Optimization**: Refine all poses with `Sim3LoopOptimizer`\n",
    "5. **Point Cloud Merging**: Combine aligned point clouds\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- Handles long sequences (>1000 frames)\n",
    "- Reduces pose drift\n",
    "- Improves reconstruction consistency\n",
    "- Memory-efficient chunked processing\n",
    "- Supports metric scale from MapAnything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-loop-closure-header",
   "metadata": {},
   "source": [
    "## Step 9: Setup VGGT-Long with MapAnythingAdapter\n",
    "\n",
    "Configure VGGT-Long to use MapAnything with loop closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-loop-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VGGT-Long components\n",
    "try:\n",
    "    from vggt_long import VGGTLong\n",
    "    from base_models.base_model import MapAnythingAdapter\n",
    "    print(\"‚úì VGGT-Long imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå VGGT-Long import failed: {e}\")\n",
    "    print(\"Please install: git clone https://github.com/DengKaiCQ/VGGT-Long.git && cd VGGT-Long && pip install -e .\")\n",
    "    raise\n",
    "\n",
    "# Configuration for VGGT-Long with MapAnything\n",
    "vggt_long_config = {\n",
    "    # Model configuration\n",
    "    \"Weights\": {\n",
    "        \"model\": \"Mapanything\",\n",
    "        \"model_url\": \"facebook/map-anything\",  # or \"facebook/map-anything-apache\"\n",
    "    },\n",
    "    \n",
    "    # Chunking parameters\n",
    "    \"chunk_size\": 100,        # Frames per chunk\n",
    "    \"overlap\": 10,             # Overlap between chunks\n",
    "    \n",
    "    # Loop closure detection\n",
    "    \"useDBoW\": True,           # Use DBoW2 for loop detection\n",
    "    \"dbow_threshold\": 0.5,     # Similarity threshold\n",
    "    \"dbow_repeat_count\": 3,    # Minimum repeat count\n",
    "    \n",
    "    # Alignment parameters\n",
    "    \"conf_threshold_coef\": 0.1,  # Confidence threshold coefficient\n",
    "    \"overlap_ratio\": 0.3,         # Overlap ratio for alignment\n",
    "    \n",
    "    # Point cloud saving\n",
    "    \"Pointcloud_Save\": {\n",
    "        \"save_unaligned\": True,   # Save per-chunk point clouds\n",
    "        \"save_aligned\": True,      # Save aligned point clouds\n",
    "        \"save_loop\": True,         # Save loop-closed point clouds\n",
    "        \"sample_ratio\": 0.1,       # Downsampling ratio for final point cloud\n",
    "    },\n",
    "    \n",
    "    # Output directory\n",
    "    \"output_dir\": str(output_base / \"loop_closure\"),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VGGT-LONG CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {vggt_long_config['Weights']['model']}\")\n",
    "print(f\"Chunk size: {vggt_long_config['chunk_size']}\")\n",
    "print(f\"Overlap: {vggt_long_config['overlap']}\")\n",
    "print(f\"Loop detection: DBoW2 (threshold={vggt_long_config['dbow_threshold']})\")\n",
    "print(f\"Output: {vggt_long_config['output_dir']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-loop-closure-header",
   "metadata": {},
   "source": [
    "## Step 10: Run Loop Closure Pipeline\n",
    "\n",
    "Execute the full VGGT-Long pipeline with MapAnything.\n",
    "\n",
    "**Note**: This step is computationally intensive and best suited for long video sequences (>200 frames) with loop closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-loop-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "loop_output_dir = Path(vggt_long_config[\"output_dir\"])\n",
    "loop_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING VGGT-LONG LOOP CLOSURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize VGGT-Long\n",
    "vggt_long = VGGTLong(config=vggt_long_config)\n",
    "\n",
    "# Run the full pipeline\n",
    "# This will:\n",
    "# 1. Process images in chunks using MapAnythingAdapter\n",
    "# 2. Detect loops between chunks\n",
    "# 3. Align chunks with loop constraints\n",
    "# 4. Optimize poses globally\n",
    "# 5. Merge point clouds\n",
    "\n",
    "start_time = time.time()\n",
    "results = vggt_long.run(image_paths=image_path_list)\n",
    "end_time = time.time()\n",
    "\n",
    "loop_closure_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOOP CLOSURE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Total time: {loop_closure_time:.2f}s\")\n",
    "print(f\"  Loops detected: {len(results.get('loops', []))}\")\n",
    "print(f\"  Chunks processed: {results.get('num_chunks', 0)}\")\n",
    "print(f\"  Final point cloud: {loop_output_dir / 'merged_point_cloud.ply'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-loop-header",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Loop-Closed Reconstruction\n",
    "\n",
    "Compare the original MapAnything reconstruction with the loop-closed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "# Load both point clouds\n",
    "original_pc = pv.PolyData(str(ply_path))\n",
    "loop_closed_pc = pv.PolyData(str(loop_output_dir / \"merged_point_cloud.ply\"))\n",
    "\n",
    "print(f\"Original point cloud: {original_pc.n_points:,} points\")\n",
    "print(f\"Loop-closed point cloud: {loop_closed_pc.n_points:,} points\")\n",
    "\n",
    "# Visualize side by side\n",
    "plotter = pv.Plotter(shape=(1, 2))\n",
    "\n",
    "# Original reconstruction\n",
    "plotter.subplot(0, 0)\n",
    "plotter.add_text(\"Original MapAnything\", font_size=12)\n",
    "plotter.add_mesh(original_pc, point_size=2, render_points_as_spheres=True)\n",
    "plotter.add_axes()\n",
    "\n",
    "# Loop-closed reconstruction\n",
    "plotter.subplot(0, 1)\n",
    "plotter.add_text(\"With Loop Closure\", font_size=12)\n",
    "plotter.add_mesh(loop_closed_pc, point_size=2, render_points_as_spheres=True)\n",
    "plotter.add_axes()\n",
    "\n",
    "plotter.link_views()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-loop-colmap-header",
   "metadata": {},
   "source": [
    "## Step 12: Export Loop-Closed Results to COLMAP\n",
    "\n",
    "Convert the loop-closed reconstruction to COLMAP format for nerfstudio training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-loop-colmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VGGT-Long pipeline should have already exported to COLMAP format\n",
    "# Check the output directory for COLMAP files\n",
    "\n",
    "loop_colmap_dir = loop_output_dir / \"colmap\" / \"sparse\" / \"0\"\n",
    "\n",
    "if loop_colmap_dir.exists():\n",
    "    print(f\"Loop-closed COLMAP reconstruction found at: {loop_colmap_dir}\")\n",
    "    \n",
    "    # Convert to nerfstudio format\n",
    "    loop_preproc_dir = loop_output_dir / \"preproc\"\n",
    "    loop_preproc_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    colmap_utils.colmap_to_json(\n",
    "        recon_dir=loop_colmap_dir,\n",
    "        output_dir=loop_preproc_dir,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Nerfstudio transforms.json created: {loop_preproc_dir / 'transforms.json'}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  COLMAP output not found. Check VGGT-Long configuration.\")\n",
    "    print(f\"    Expected: {loop_colmap_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Comparison: MapAnything vs VGGT vs InfiniteVGGT\n",
    "\n",
    "| Feature | MapAnything | VGGT | InfiniteVGGT |\n",
    "|---------|-------------|------|---------------|\n",
    "| **Speed** | Fast (feedforward) | Moderate | Fast (streaming) |\n",
    "| **Memory** | Memory-efficient | High | Frame-cached |\n",
    "| **Metric Scale** | ‚úÖ Yes | ‚ùå No | ‚ùå No |\n",
    "| **Long Sequences** | Limited | Limited | ‚úÖ Optimized |\n",
    "| **Point Density** | Dense | Dense | Dense |\n",
    "| **COLMAP Export** | ‚úÖ Built-in | Manual | Manual |\n",
    "| **Loop Closure** | ‚ö†Ô∏è Via VGGT-Long | ‚ö†Ô∏è Via VGGT-Long | ‚ö†Ô∏è Via VGGT-Long |\n",
    "| **Best For** | Metric reconstruction, quick preview | General scenes | Long videos, streaming |\n",
    "\n",
    "### When to Use MapAnything\n",
    "\n",
    "‚úÖ **Use MapAnything when:**\n",
    "- You need metric scale (real-world units)\n",
    "- You want fast feedforward inference\n",
    "- You need quick previews or prototyping\n",
    "- You have short to medium sequences (<500 frames)\n",
    "- You want built-in COLMAP export\n",
    "\n",
    "‚ùå **Don't use MapAnything when:**\n",
    "- You have very long sequences (use InfiniteVGGT)\n",
    "- You need maximum accuracy (use VGGT with BA)\n",
    "- You have limited GPU memory (use InfiniteVGGT with disk caching)\n",
    "\n",
    "### Loop Closure Benefits\n",
    "\n",
    "Adding loop closure (Section 3) is beneficial when:\n",
    "- Your sequence has revisited locations\n",
    "- You observe drift in camera poses\n",
    "- You want globally consistent reconstruction\n",
    "- Your sequence is longer than 200-300 frames\n",
    "\n",
    "The overhead is:\n",
    "- ~1.5-2x inference time\n",
    "- Additional memory for loop detection\n",
    "- More complex pipeline\n",
    "\n",
    "But you gain:\n",
    "- Reduced pose drift\n",
    "- Better global consistency\n",
    "- Improved reconstruction quality\n",
    "- Support for very long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-header",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "- Set `minibatch_size=1` for memory-efficient inference\n",
    "- Use `amp_dtype=\"fp16\"` instead of `\"bf16\"` on older GPUs\n",
    "- Process fewer images at once\n",
    "- Reduce image resolution before inference\n",
    "\n",
    "### Poor Reconstruction Quality\n",
    "- Adjust `voxel_fraction` (try 0.005 - 0.02)\n",
    "- Enable `apply_mask=True` and `mask_edges=True`\n",
    "- Check input image quality and lighting\n",
    "- Ensure sufficient overlap between images\n",
    "\n",
    "### Loop Closure Issues\n",
    "- Adjust `dbow_threshold` (try 0.3 - 0.7)\n",
    "- Increase `chunk_size` and `overlap`\n",
    "- Check that sequences have actual loop closures\n",
    "- Verify GPU memory is sufficient for long sequences\n",
    "\n",
    "### Installation Issues\n",
    "```bash\n",
    "# MapAnything\n",
    "pip install git+https://github.com/facebookresearch/map-anything.git\n",
    "\n",
    "# VGGT-Long (for loop closure)\n",
    "git clone https://github.com/DengKaiCQ/VGGT-Long.git\n",
    "cd VGGT-Long\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### License Considerations\n",
    "- Default model: `facebook/map-anything` (CC-BY-NC 4.0 - non-commercial)\n",
    "- Apache licensed: `facebook/map-anything-apache` (Apache 2.0 - commercial OK)\n",
    "- Choose based on your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
