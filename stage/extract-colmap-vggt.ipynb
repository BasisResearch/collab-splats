{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# VGGT Preprocessing for Nerfstudio\n",
    "\n",
    "This notebook demonstrates how to use VGGT (Visual Geometry Grounded deep Transformer) for structure-from-motion preprocessing with nerfstudio.\n",
    "\n",
    "**VGGT** is now integrated directly into nerfstudio's custom implementation!\n",
    "\n",
    "## What is VGGT?\n",
    "\n",
    "VGGT uses deep learning to estimate camera poses and depth maps directly from images, without traditional feature matching. This can be faster and more robust than COLMAP for certain scenarios.\n",
    "\n",
    "## How VGGT Integration Works\n",
    "\n",
    "The nerfstudio integration (`nerfstudio/process_data/vggt_utils.py`) runs VGGT inference and converts the output to **COLMAP-compatible format**:\n",
    "- `colmap/sparse/0/cameras.bin` - Camera intrinsics\n",
    "- `colmap/sparse/0/images.bin` - Camera poses and 2D keypoints\n",
    "- `colmap/sparse/0/points3D.bin` - 3D point cloud with tracks\n",
    "- `transforms.json` - Nerfstudio format (generated from COLMAP output)\n",
    "\n",
    "This means VGGT output can be used exactly like COLMAP output!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure VGGT is installed:\n",
    "```bash\n",
    "pip install git+https://github.com/facebookresearch/vggt.git\n",
    "```\n",
    "\n",
    "## Usage Modes\n",
    "\n",
    "1. **Using Splatter wrapper** (recommended): `splatter.preprocess(sfm_tool='vggt')`\n",
    "2. **Using ns-process-data CLI**: `ns-process-data video --sfm-tool vggt`\n",
    "3. **Using Python API directly**: `vggt_utils.run_vggt(image_dir, colmap_dir, ...)`\n",
    "4. **Legacy method**: `splatter.preprocess_vggt()` (deprecated, kept for compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "check_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/conda/envs/nerfstudio/bin/python\n",
      "Python version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "\n",
      "‚úì Running in nerfstudio environment\n"
     ]
    }
   ],
   "source": [
    "# Verify conda environment\n",
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're in the nerfstudio environment\n",
    "if 'nerfstudio' not in sys.executable:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Not running in nerfstudio conda environment!\")\n",
    "    print(\"Please activate with: conda activate nerfstudio\")\n",
    "else:\n",
    "    print(\"\\n‚úì Running in nerfstudio environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from collab_splats.wrapper import Splatter, SplatterConfig\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up your input video and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video: /workspace/fieldwork-data/birds/2024-02-06/SplatsSD/C0043.MP4\n",
      "Output path: /workspace/fieldwork-data/birds/2024-02-06/environment/C0043\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "base_dir = Path(\"/workspace/fieldwork-data/\")\n",
    "# session_dir = base_dir / \"rats/2024-07-11/SplatsSD\" / \"C0119.MP4\"\n",
    "session_dir = base_dir / \"birds/2024-02-06/SplatsSD\" / \"C0043.MP4\"\n",
    "\n",
    "# Create splatter configuration\n",
    "splatter_config = SplatterConfig(\n",
    "    file_path=session_dir,\n",
    "    method=\"rade-features\",\n",
    "    frame_proportion=0.01,\n",
    "    min_frames=15,\n",
    ")\n",
    "\n",
    "# Initialize the Splatter class\n",
    "splatter = Splatter(splatter_config)\n",
    "\n",
    "print(f\"Input video: {splatter.config['file_path']}\")\n",
    "print(f\"Output path: {splatter.config['output_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "method1_header",
   "metadata": {},
   "source": [
    "## Method 1: Using Splatter Wrapper (Recommended)\n",
    "\n",
    "The simplest way to use VGGT - just specify `sfm_tool='vggt'` in preprocess()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preprocess_vggt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames to sample:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KNumber of frames in video: \u001b[1;36m2388\u001b[0mes.....\n",
      "\u001b[2KExtracting \u001b[1;36m24\u001b[0m frames in evenly spaced intervals\n",
      "\u001b[2K\u001b[2;36m[16:46:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32müéâ Done converting video to images.\u001b[0m                                                 \u001b]8;id=360715;file:///workspace/nerfstudio/nerfstudio/process_data/process_data_utils.py\u001b\\\u001b[2mprocess_data_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=336181;file:///workspace/nerfstudio/nerfstudio/process_data/process_data_utils.py#227\u001b\\\u001b[2m227\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2K\u001b[32m(     ‚óè)\u001b[0m Converting video to images...\n",
      "\u001b[1A\u001b[2K\u001b[1;32mUsing device: cuda\u001b[0m\n",
      "\u001b[1;33mLoading VGGT model: facebook/VGGT-1B\u001b[0m\n",
      "\u001b[1;32mFound \u001b[0m\u001b[1;32m24\u001b[0m\u001b[1;32m images\u001b[0m\n",
      "\u001b[1;33mRunning VGGT inference\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "  - Images shape after preprocessing: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/nerfstudio/nerfstudio/process_data/vggt_utils.py:481: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/vggt/models/vggt.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mConverting tensors to numpy\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "  - predictions keys: \u001b[1m[\u001b[0m\u001b[32m'pose_enc'\u001b[0m, \u001b[32m'pose_enc_list'\u001b[0m, \u001b[32m'depth'\u001b[0m, \u001b[32m'depth_conf'\u001b[0m, \u001b[32m'world_points'\u001b[0m, \u001b[32m'world_points_conf'\u001b[0m, \n",
      "\u001b[32m'images'\u001b[0m\u001b[1m]\u001b[0m\n",
      "  - pose_enc shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - depth shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - depth_conf shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - world_points shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - world_points_conf shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - images shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - extrinsic shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - intrinsic shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - intrinsic_downsampled shape before squeeze: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[1;32m‚úì VGGT inference complete\u001b[0m\n",
      "  - Camera poses: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - 3D points: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[1;33mCleared GPU cache after VGGT inference\u001b[0m\n",
      "  - GPU memory allocated: \u001b[1;36m0.01\u001b[0m GiB\n",
      "  - GPU memory reserved: \u001b[1;36m0.02\u001b[0m GiB\n",
      "\u001b[1;33mUsing resolution 518x518 for track prediction\u001b[0m\n",
      "  - This uses ~4x less memory than 1024x1024\n",
      "  - Tracks will be scaled to original resolution after prediction\n",
      "\u001b[1;33mLoading \u001b[0m\u001b[1;33m24\u001b[0m\u001b[1;33m images at resolution \u001b[0m\u001b[1;33m518\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "  - Loaded images with shape: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - Resized depth to: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - Computed 3D points: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - Scaled intrinsics by factor: \u001b[1;36m0.2698\u001b[0m\n",
      "\u001b[1;33mRunning feature-based track prediction\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "  - max_query_pts: \u001b[1;36m2048\u001b[0m\n",
      "  - query_frame_num: \u001b[1;36m5\u001b[0m\n",
      "  - fine_tracking: \u001b[3;92mTrue\u001b[0m\n",
      "\u001b[1;33mMemory usage tips:\u001b[0m\n",
      "  - \u001b[33mfine_tracking\u001b[0m=\u001b[3;92mTrue\u001b[0m uses significant memory. Set to \u001b[3;91mFalse\u001b[0m if OOM occurs.\n",
      "  - \u001b[33mmax_query_pts\u001b[0m=\u001b[1;36m2048\u001b[0m is high. Try \u001b[1;36m1024\u001b[0m or \u001b[1;36m512\u001b[0m if OOM occurs.\n",
      "  - \u001b[33mquery_frame_num\u001b[0m=\u001b[1;36m5\u001b[0m is high. Try \u001b[1;36m3\u001b[0m if OOM occurs.\n",
      "  - See: \u001b[4;94mhttps://github.com/facebookresearch/vggt/issues/238\u001b[0m\n",
      "  - images shape for predict_tracks: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - depth_resized shape for predict_tracks: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m\u001b[1m)\u001b[0m\n",
      "  - points_3d shape for predict_tracks: \u001b[1m(\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m518\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For faster inference, consider disabling fine_tracking\n",
      "Predicting tracks for query frame 0\n",
      "\u001b[2;36m[16:46:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mCouldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions \u001b[0m   \u001b]8;id=498156;file:///workspace/nerfstudio/nerfstudio/scripts/process_data.py\u001b\\\u001b[2mprocess_data.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=237606;file:///workspace/nerfstudio/nerfstudio/scripts/process_data.py#564\u001b\\\u001b[2m564\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;31mare incompatible, or if you had errors while compiling torchvision from source. For \u001b[0m      \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;31mfurther information on the compatible versions, check \u001b[0m                                    \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;4;31mhttps://github.com/pytorch/vision#installation\u001b[0m\u001b[1;31m for the compatibility matrix. Please check\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;31myour PyTorch version with torch.__version__ and your torchvision version with \u001b[0m            \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;31mtorchvision.__version__ and verify if they are compatible, and if not please reinstall \u001b[0m   \u001b[2m                   \u001b[0m\n",
      "\u001b[2;36m           \u001b[0m\u001b[1;31mtorchvision so that it matches your PyTorch install.\u001b[0m                                      \u001b[2m                   \u001b[0m\n",
      "\n",
      "‚úì VGGT preprocessing complete!\n",
      "Output directory: /workspace/fieldwork-data/birds/2024-02-06/environment/C0043/preproc\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing with VGGT\n",
    "# This will:\n",
    "# 1. Extract frames from the video\n",
    "# 2. Run VGGT to estimate camera poses and depth\n",
    "# 3. Generate transforms.json for nerfstudio training\n",
    "\n",
    "preproc_kwargs = {\n",
    "    \"refine_vggt_ba\": \"\", # This sets to true?\n",
    "}\n",
    "splatter.preprocess(\n",
    "    sfm_tool='vggt', \n",
    "    overwrite=True,\n",
    "    kwargs=preproc_kwargs\n",
    ") \n",
    "\n",
    "print(\"\\n‚úì VGGT preprocessing complete!\")\n",
    "print(f\"Output directory: {splatter.config['preproc_data_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ibf3urooqsn",
   "metadata": {},
   "source": [
    "## Inspecting VGGT Output\n",
    "\n",
    "After running VGGT, you can inspect the generated COLMAP files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nb2val6v4v",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the COLMAP output from VGGT\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_colmap_output(colmap_dir):\n",
    "    \"\"\"Inspect COLMAP binary files generated by VGGT.\"\"\"\n",
    "    sparse_dir = Path(colmap_dir) / \"sparse\" / \"0\"\n",
    "    \n",
    "    print(f\"Checking COLMAP output in: {sparse_dir}\\n\")\n",
    "    \n",
    "    # Check for required files\n",
    "    cameras_file = sparse_dir / \"cameras.bin\"\n",
    "    images_file = sparse_dir / \"images.bin\"\n",
    "    points3D_file = sparse_dir / \"points3D.bin\"\n",
    "    \n",
    "    if not sparse_dir.exists():\n",
    "        print(\"‚ùå COLMAP sparse directory not found. Run VGGT first!\")\n",
    "        return\n",
    "    \n",
    "    # Read cameras.bin\n",
    "    if cameras_file.exists():\n",
    "        with open(cameras_file, \"rb\") as f:\n",
    "            num_cameras = struct.unpack(\"<Q\", f.read(8))[0]\n",
    "        print(f\"‚úì cameras.bin: {num_cameras} cameras\")\n",
    "    else:\n",
    "        print(\"‚ùå cameras.bin not found\")\n",
    "    \n",
    "    # Read images.bin\n",
    "    if images_file.exists():\n",
    "        with open(images_file, \"rb\") as f:\n",
    "            num_images = struct.unpack(\"<Q\", f.read(8))[0]\n",
    "        print(f\"‚úì images.bin: {num_images} camera poses\")\n",
    "    else:\n",
    "        print(\"‚ùå images.bin not found\")\n",
    "    \n",
    "    # Read points3D.bin\n",
    "    if points3D_file.exists():\n",
    "        with open(points3D_file, \"rb\") as f:\n",
    "            num_points = struct.unpack(\"<Q\", f.read(8))[0]\n",
    "        print(f\"‚úì points3D.bin: {num_points:,} 3D points\")\n",
    "    else:\n",
    "        print(\"‚ùå points3D.bin not found\")\n",
    "    \n",
    "    print(\"\\nThese files are in COLMAP binary format and can be:\")\n",
    "    print(\"  - Visualized with COLMAP GUI: colmap gui\")\n",
    "    print(\"  - Converted to nerfstudio format (transforms.json) automatically\")\n",
    "    print(\"  - Used with any tool that reads COLMAP format\")\n",
    "\n",
    "# Example usage (uncomment if you've run VGGT):\n",
    "inspect_colmap_output(splatter.config['preproc_data_path'] / 'colmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from nerfstudio.process_data.colmap_utils import create_ply_from_colmap\n",
    "\n",
    "data_path = splatter.config[\"preproc_data_path\"]\n",
    "\n",
    "with open(data_path / \"transforms.json\") as f:\n",
    "    transforms = json.load(f)\n",
    "\n",
    "applied_transform = torch.tensor(transforms[\"applied_transform\"])\n",
    "\n",
    "ply_filename = \"sparse_pc.ply\"\n",
    "create_ply_from_colmap(\n",
    "    filename=ply_filename,\n",
    "    recon_dir=data_path / \"colmap\" / \"sparse\" / \"0\",\n",
    "    output_dir=data_path,\n",
    "    applied_transform=applied_transform,\n",
    ")\n",
    "ply_file_path = data_path / ply_filename\n",
    "transforms[\"ply_file_path\"] = ply_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca83020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "from collab_splats.utils.visualization import (\n",
    "    CAMERA_KWARGS,\n",
    "    MESH_KWARGS,\n",
    "    VIZ_KWARGS,\n",
    "    visualize_splat,\n",
    ")\n",
    "\n",
    "old_fn = '/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/archive/sparse_pc.ply'\n",
    "old_fn = ply_file_path\n",
    "splat = pv.PolyData(old_fn)\n",
    "# splat.point_data[\"RGB\"] = np.asarray(pcd.colors)\n",
    "\n",
    "pcd_kwargs = MESH_KWARGS.copy()\n",
    "pcd_kwargs.update(\n",
    "    {\n",
    "        \"point_size\": 2,\n",
    "        \"render_points_as_spheres\": True,\n",
    "        \"ambient\": 0.3,\n",
    "        \"diffuse\": 0.8,\n",
    "        \"specular\": 0.1,\n",
    "    }\n",
    ")\n",
    "\n",
    "plotter = visualize_splat(\n",
    "    mesh=splat,\n",
    "    mesh_kwargs=pcd_kwargs,\n",
    "    viz_kwargs=VIZ_KWARGS,\n",
    ")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_header",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "After preprocessing with VGGT, train your model as usual. The workflow is identical regardless of which SfM tool you used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (same for any SfM tool)\n",
    "feature_kwargs = {\n",
    "    \"pipeline.model.output-depth-during-training\": True,\n",
    "    \"pipeline.model.rasterize-mode\": \"antialiased\",\n",
    "    \"pipeline.model.use-scale-regularization\": True,\n",
    "    \"pipeline.model.random-scale\": 1.0,\n",
    "    # \"pipeline.model.cull-alpha-thresh\": 0.01,\n",
    "    \"pipeline.model.collider-params\": \"near_plane 0.1 far_plane 3.0\",\n",
    "}\n",
    "\n",
    "splatter.extract_features(kwargs=feature_kwargs, overwrite=True)\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli_header",
   "metadata": {},
   "source": [
    "## Method 4: Direct CLI Usage\n",
    "\n",
    "You can also use VGGT directly from the command line with `ns-process-data`:\n",
    "\n",
    "```bash\n",
    "# Process video with VGGT\n",
    "ns-process-data video \\\n",
    "    --data /path/to/video.mp4 \\\n",
    "    --output-dir /path/to/output \\\n",
    "    --sfm-tool vggt\n",
    "\n",
    "# Process images with VGGT\n",
    "ns-process-data images \\\n",
    "    --data /path/to/images/ \\\n",
    "    --output-dir /path/to/output \\\n",
    "    --sfm-tool vggt\n",
    "```\n",
    "\n",
    "### Behind the Scenes\n",
    "\n",
    "When you run `ns-process-data` with `--sfm-tool vggt`, it:\n",
    "1. Extracts/copies images to the output directory\n",
    "2. Calls `nerfstudio/process_data/vggt_utils.run_vggt()` \n",
    "3. VGGT runs inference and generates COLMAP binary files\n",
    "4. `colmap_utils.colmap_to_json()` converts COLMAP ‚Üí `transforms.json`\n",
    "5. You can then train with: `ns-train rade-features --data /path/to/output`\n",
    "\n",
    "This is the same pipeline whether you use Splatter, CLI, or Python API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advantages_header",
   "metadata": {},
   "source": [
    "## When to Use VGGT vs COLMAP\n",
    "\n",
    "**Use VGGT when:**\n",
    "- You have textureless or repetitive scenes (where COLMAP struggles)\n",
    "- You want faster preprocessing\n",
    "- You have good lighting and clear images\n",
    "\n",
    "**Use COLMAP when:**\n",
    "- You need maximum accuracy\n",
    "- You have well-textured scenes\n",
    "- You have challenging camera motions or occlusions\n",
    "\n",
    "**Use hloc when:**\n",
    "- You want modern deep features (SuperPoint + SuperGlue)\n",
    "- You need a balance between speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eg7ppmzbciv",
   "metadata": {},
   "source": [
    "## Key Integration Details\n",
    "\n",
    "### VGGT ‚Üí COLMAP Format Conversion\n",
    "\n",
    "The nerfstudio integration (`nerfstudio/process_data/vggt_utils.py`) performs the following conversions:\n",
    "\n",
    "**1. Camera Poses:**\n",
    "- VGGT outputs: 4√ó4 extrinsic matrices (world-to-camera transform)\n",
    "- Converted to: COLMAP quaternion + translation format\n",
    "- Stored in: `images.bin` (COLMAP binary format)\n",
    "\n",
    "**2. Camera Intrinsics:**\n",
    "- VGGT estimates: Per-image intrinsic matrices (fx, fy, cx, cy)\n",
    "- Stored in: `cameras.bin` with PINHOLE camera model\n",
    "\n",
    "**3. 3D Points:**\n",
    "- VGGT outputs: Dense depth maps for each image\n",
    "- Unprojected to: 3D world coordinates using camera poses\n",
    "- Filtered by: Confidence threshold (default: top 50%)\n",
    "- Stored in: `points3D.bin` with RGB colors and track information\n",
    "\n",
    "**4. Point Tracks:**\n",
    "- VGGT depth ‚Üí 3D points are matched across views by spatial proximity\n",
    "- Tracks link 2D observations (in `images.bin`) to 3D points (in `points3D.bin`)\n",
    "\n",
    "### Implementation Files\n",
    "\n",
    "- **Main integration**: `nerfstudio/process_data/vggt_utils.py` (lines 35-400)\n",
    "- **CLI hook**: `nerfstudio/process_data/colmap_converter_to_nerfstudio_dataset.py` (lines 242-251)\n",
    "- **Splatter wrapper**: `collab_splats/wrapper/splatter.py` (lines 132-236)\n",
    "- **Legacy (deprecated)**: `collab_splats/utils/vggt_utils.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
