{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLATS = {\n",
    "    'rats': {\n",
    "        'base_dir': '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features',\n",
    "        'load_config': \"2025-07-25_074037/config.yml\",\n",
    "    },\n",
    "    'birds': {\n",
    "        'base_dir': '/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features',\n",
    "        'load_config': \"2025-07-25_040743/config.yml\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Taichi] version 1.7.4, llvm 15.0.4, commit b4b956fd, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 09/25/25 16:30:56.763 126422] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory bank loaded from /workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/grouping/memory_bank.pkl with 2569 masks\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collab_splats.utils.grouping import GroupingClassifier, GroupingConfig\n",
    "\n",
    "# Path to the config for a trained model\n",
    "species = 'birds'\n",
    "base_dir = Path(SPLATS[species]['base_dir'])\n",
    "load_config = base_dir / SPLATS[species]['load_config']\n",
    "\n",
    "# saved_model = Path(base_dir) / \"grouping\" / \"checkpoints\" / \"grouping-classifier-v1.ckpt\"\n",
    "\n",
    "# if saved_model.exists():\n",
    "#     print (f\"Loading model from {saved_model}\")\n",
    "#     grouping_classifier = GroupingClassifier.load_from_checkpoint(saved_model)\n",
    "\n",
    "#     grouping_classifier.load_pipeline()\n",
    "# else:\n",
    "grouping_config = GroupingConfig(\n",
    "    segmentation_backend='mobilesamv2', \n",
    "    segmentation_strategy='object', \n",
    "    front_percentage=0.2, \n",
    "    iou_threshold=0.1, \n",
    "    num_patches=32,\n",
    "    identity_dim=8,\n",
    "    # lr=5e-5\n",
    ")\n",
    "\n",
    "grouping_classifier = GroupingClassifier(load_config=load_config, config=grouping_config)\n",
    "\n",
    "# grouping_classifier.identities\n",
    "\n",
    "# # Step 2: Load checkpoint state_dict only\n",
    "# if saved_model.exists():\n",
    "#     checkpoint = torch.load(saved_model)\n",
    "#     state_dict = checkpoint['state_dict']\n",
    "#     grouping_classifier.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # Step 3: Inject runtime pipeline & model\n",
    "# grouping_classifier.load_pipeline()        # loads the NeRF pipeline at runtime\n",
    "# grouping_classifier.load_segmentation()    # loads the segmentation backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_classifier.create_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_classifier.associate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try pytorch lightning datamodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train identity embeddings to lift objects from 2d to 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NeRF pipeline and model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:31:04] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:31:04]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=434123;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=477998;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Train dataset has over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> images, overriding cache_images to cpu. If you still get OOM errors or segfault, please </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">consider seting cache_images to </span><span style=\"color: #008000; text-decoration-color: #008000\">'disk'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mTrain dataset has over \u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m images, overriding cache_images to cpu. If you still get OOM errors or segfault, please \u001b[0m\n",
       "\u001b[1;33mconsider seting cache_images to \u001b[0m\u001b[32m'disk'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:32:19] </span>use color only optimization with sigmoid activation                                         <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">splatfacto.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:32:19]\u001b[0m\u001b[2;36m \u001b[0muse color only optimization with sigmoid activation                                         \u001b]8;id=551906;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\u001b\\\u001b[2msplatfacto.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=354884;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\u001b\\\u001b[2m213\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/2025-07-25_040743/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">step-0000</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">29999.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "\u001b[35m/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/2025-07-25_040743/nerfstudio_models/\u001b[0m\u001b[95mstep-0000\u001b[0m\n",
       "\u001b[95m29999.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouping_classifier.load_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NeRF pipeline and model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:32:45] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:32:45]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=643219;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=35226;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Train dataset has over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> images, overriding cache_images to cpu. If you still get OOM errors or segfault, please </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">consider seting cache_images to </span><span style=\"color: #008000; text-decoration-color: #008000\">'disk'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mTrain dataset has over \u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m images, overriding cache_images to cpu. If you still get OOM errors or segfault, please \u001b[0m\n",
       "\u001b[1;33mconsider seting cache_images to \u001b[0m\u001b[32m'disk'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:33:52] </span>use color only optimization with sigmoid activation                                         <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">splatfacto.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:33:52]\u001b[0m\u001b[2;36m \u001b[0muse color only optimization with sigmoid activation                                         \u001b]8;id=65642;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\u001b\\\u001b[2msplatfacto.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=960258;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\u001b\\\u001b[2m213\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/2025-07-25_040743/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">step-0000</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">29999.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "\u001b[35m/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/2025-07-25_040743/nerfstudio_models/\u001b[0m\u001b[95mstep-0000\u001b[0m\n",
       "\u001b[95m29999.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "logger = WandbLogger(\n",
    "    project=\"collab-splats\", \n",
    "    name=f\"grouping_{species}\",\n",
    "    log_model=False\n",
    ")\n",
    "\n",
    "# grouping_classifier.config.identity_dim = 16\n",
    "\n",
    "# # Use simulated data (10 total mask types)\n",
    "# grouping_classifier.total_masks = 10\n",
    "\n",
    "grouping_classifier.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtbotch\u001b[0m (\u001b[33mfinnlab\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250925_163439-3tzu3j7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finnlab/collab-splats/runs/3tzu3j7u' target=\"_blank\">grouping_birds</a></strong> to <a href='https://wandb.ai/finnlab/collab-splats' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finnlab/collab-splats' target=\"_blank\">https://wandb.ai/finnlab/collab-splats</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finnlab/collab-splats/runs/3tzu3j7u' target=\"_blank\">https://wandb.ai/finnlab/collab-splats/runs/3tzu3j7u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NeRF pipeline and model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:34:41] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:34:41]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=159233;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=320639;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Train dataset has over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> images, overriding cache_images to cpu. If you still get OOM errors or segfault, please </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">consider seting cache_images to </span><span style=\"color: #008000; text-decoration-color: #008000\">'disk'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mTrain dataset has over \u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m images, overriding cache_images to cpu. If you still get OOM errors or segfault, please \u001b[0m\n",
       "\u001b[1;33mconsider seting cache_images to \u001b[0m\u001b[32m'disk'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouping_classifier.lift_segmentation(logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_splats.utils.grouping import GroupingDataModule \n",
    "\n",
    "datamodule = GroupingDataModule(\n",
    "    datamanager=grouping_classifier.pipeline.datamanager,\n",
    "    mask_dir=grouping_classifier.associated_mask_dir,\n",
    "    device=\"cuda\",\n",
    "    train_num_workers=0,\n",
    "    val_num_workers=0,\n",
    "    use_simulated=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[1]['segmentation'].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(x[1]['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_classifier.training_step(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = grouping_classifier(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outs['identities'][..., :3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs['identities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "grouping_classifier.eval()\n",
    "logits = grouping_classifier(camera)\n",
    "\n",
    "labels = logits.argmax(0).detach().cpu().numpy()\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "print (unique_labels)\n",
    "\n",
    "plt.imshow(labels)\n",
    "\n",
    "plt.imshow(data['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "identities = identities.unsqueeze(0)\n",
    "segmentation = data['segmentation'].unsqueeze(0).to(grouping_classifier.model.device)\n",
    "\n",
    "CrossEntropyLoss(reduction=\"none\")(identities, segmentation)\n",
    "\n",
    "# grouping_classifier.loss_fn(identities, data['segmentation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to map onto the mesh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import open3d as o3d\n",
    "from collab_splats.utils.mesh import features2vertex\n",
    "\n",
    "\n",
    "mesh_dir = grouping_classifier.output_dir.parent / 'mesh'\n",
    "\n",
    "mesh_path = mesh_dir / 'mesh.ply'\n",
    "transforms_path = mesh_dir / 'transforms.pkl'\n",
    "\n",
    "with open(transforms_path, 'rb') as f:\n",
    "    transforms = pickle.load(f)\n",
    "\n",
    "mesh = o3d.io.read_triangle_mesh(mesh_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the means to the mesh\n",
    "means = grouping_classifier.model.means.clone()\n",
    "means = means @ transforms[\"mesh_transform\"][:3, :3].T + transforms[\"mesh_transform\"][:3, 3]\n",
    "\n",
    "# Get the classes for each point\n",
    "classes = grouping_classifier.per_gaussian_forward(grouping_classifier.identities)\n",
    "classes = classes.argmax(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map to the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_classes = features2vertex(\n",
    "    mesh_vertices=mesh.vertices,\n",
    "    points=means,\n",
    "    features=classes,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create RGB colors for each unique class\n",
    "unique_classes = torch.unique(mesh_classes)\n",
    "n_classes = len(unique_classes)\n",
    "\n",
    "# Generate distinct colors using HSV colorspace for better visual separation\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = plt.get_cmap('tab10')  # or 'viridis', 'plasma', etc.\n",
    "\n",
    "# Create color mapping\n",
    "class_to_rgb = {}\n",
    "for i, class_id in enumerate(unique_classes):\n",
    "    color = cmap(i / max(1, n_classes - 1))  # Normalize to [0,1]\n",
    "    class_to_rgb[class_id.item()] = torch.tensor(color[:3], dtype=torch.float32)  # RGB only\n",
    "\n",
    "# Map classes to RGB colors\n",
    "rgb_colors = torch.zeros(mesh_classes.shape[0], 3, dtype=torch.float32)\n",
    "for i, class_id in enumerate(mesh_classes.squeeze()):\n",
    "    rgb_colors[i] = class_to_rgb[class_id.item()]\n",
    "\n",
    "rgb_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "mesh = pv.read(mesh_path.as_posix())\n",
    "\n",
    "print(mesh.n_points)\n",
    "print(mesh.n_cells)\n",
    "print(mesh.bounds)\n",
    "\n",
    "image = mesh.plot(\n",
    "    scalars=rgb_colors, \n",
    "    rgb=True,\n",
    "    screenshot=True\n",
    ")\n",
    "\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
