{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Taichi] version 1.7.4, llvm 15.0.4, commit b4b956fd, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/31/25 18:57:23.155 7691] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "# from ns_extension.utils.grouping import GroupingClassifier\n",
    "import collab_splats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the config for a trained model\n",
    "load_config = '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-25_074037/config.yml'\n",
    "# load_config = '/workspace/fieldwork-data/birds/2024-02-06/environment/C0043/rade-features/2025-07-25_040743/config.yml'\n",
    "load_config = Path(load_config)\n",
    "\n",
    "config, pipeline, checkpoint_path, step = eval_setup(load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question is whether to build grouping on top of the existing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collab_splats.utils.features import resize_image\n",
    "from collab_splats.utils.segmentation import Segmentation\n",
    "\n",
    "segmentation = Segmentation(\n",
    "    backend='mobilesamv2',\n",
    "    strategy='object',\n",
    "    device='cuda',\n",
    ")\n",
    "segmentation.strategy = 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start making our functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_splats.utils.grouping import GroupingClassifier, GroupingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NeRF pipeline and model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:57:28] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:57:28]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=585361;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=797708;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:57:49] </span>use color only optimization with sigmoid activation                                         <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">splatfacto.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:57:49]\u001b[0m\u001b[2;36m \u001b[0muse color only optimization with sigmoid activation                                         \u001b]8;id=189508;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\u001b\\\u001b[2msplatfacto.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=753362;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#213\u001b\\\u001b[2m213\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-25_074037/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">step-00002</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">9999.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "\u001b[35m/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-25_074037/nerfstudio_models/\u001b[0m\u001b[95mstep-00002\u001b[0m\n",
       "\u001b[95m9999.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading segmentation module...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/RogerQi_MobileSAMV2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_load_scucess\n"
     ]
    }
   ],
   "source": [
    "# Path to the config for a trained model\n",
    "load_config = '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-25_074037/config.yml'\n",
    "load_config = Path(load_config)\n",
    "\n",
    "grouping_params = GroupingParams(segmentation_backend='mobilesamv2', segmentation_strategy='object', front_percentage=0.2, iou_threshold=0.1, num_patches=32)\n",
    "grouping_classifier = GroupingClassifier(load_config=load_config, params=grouping_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4526898e21974b2aba91e11c7b9491c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/collab-splats/collab_splats/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n",
      "/workspace/collab-splats/collab_splats/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n",
      "\n",
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "0: 1024x576 18 objects, 132.1ms\n",
      "Speed: 3.8ms preprocess, 132.1ms inference, 34.7ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161f50ccf6554cf78e29912e2885aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing masks:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs for unknown reason\n",
      "/workspace/collab-splats/collab_splats/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n",
      "\n",
      "0: 1024x576 21 objects, 20.6ms\n",
      "Speed: 3.2ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0e28f9443846a7b9487e78d4ea7341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing masks:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs for unknown reason\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GroupingClassifier' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrouping_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massociate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/collab-splats/collab_splats/utils/grouping.py:138\u001b[0m, in \u001b[0;36mGroupingClassifier.associate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m composite_mask \u001b[38;5;241m=\u001b[39m create_composite_mask(results)\n\u001b[1;32m    132\u001b[0m mask_gaussians \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_front_gaussians(\n\u001b[1;32m    133\u001b[0m     meta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minfo,\n\u001b[1;32m    134\u001b[0m     composite_mask\u001b[38;5;241m=\u001b[39mcomposite_mask,\n\u001b[1;32m    135\u001b[0m     patch_mask\u001b[38;5;241m=\u001b[39mpatch_mask\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_gaussians\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_memory_bank(labels, mask_gaussians)\n",
      "File \u001b[0;32m/workspace/collab-splats/collab_splats/utils/grouping.py:181\u001b[0m, in \u001b[0;36mGroupingClassifier._assign_labels\u001b[0;34m(self, mask_gaussians)\u001b[0m\n\u001b[1;32m    178\u001b[0m selected \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(overlaps)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# If no label matches above the threshold → assign new label\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overlaps[selected] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39miou_threshold:\n\u001b[1;32m    182\u001b[0m     selected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_masks\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_masks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupingClassifier' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "grouping_classifier.associate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_image = batch[\"image\"]\n",
    "\n",
    "# Forward pass through model to get metadata\n",
    "_ = model.get_outputs(camera=camera)\n",
    "\n",
    "# Segment the image\n",
    "patch_mask = create_patch_mask(camera_image)\n",
    "\n",
    "# Annoyingly have to pass as a numpy array\n",
    "_, results = segmentation.segment(camera_image.detach().cpu().numpy())\n",
    "\n",
    "# Create composite mask\n",
    "composite_mask = create_composite_mask(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_gaussians = classifier.select_front_gaussians(model.info, composite_mask, patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To initialize the classifier, we first need to build the mask association\n",
    "# This involves \n",
    "# 1) segmenting each camera view\n",
    "# 2) associating the masks to the gaussians\n",
    "\n",
    "\n",
    "def build_mask_association(model, cameras):\n",
    "    for camera in tqdm(cameras):\n",
    "        camera = camera.to(model.device)\n",
    "\n",
    "        front_gaussians, _ = get_patch_front_gaussian_of_mask(model, camera)\n",
    "        labels = assign_labels(front_gaussians)\n",
    "        self._update_gaussian_idx_bank(labels, front_gaussians)\n",
    "\n",
    "        if self.num_mask == 0:\n",
    "            self.assigned_gaussians = torch.unique(torch.cat(front_gaussians))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_memory_bank(self, labels: torch.Tensor, mask_gaussians: list[torch.Tensor]):\n",
    "    for label, gaussians in zip(labels, mask_gaussians):\n",
    "        self.maintain_gaussian_idx_bank(label.item(), gaussians)\n",
    "\n",
    "    if self.num_mask == 0:\n",
    "        self.assigned_gaussians = torch.unique(torch.cat(front_gaussians))\n",
    "\n",
    "def assign_labels(mask_gaussians: list[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Assign a label to each set of front-facing Gaussians (i.e., masks),\n",
    "    based on their overlap with previously observed (and labeled) Gaussians.\n",
    "\n",
    "    Args:\n",
    "        mask_gaussians (list of torch.Tensor): List of 1D tensors, each containing \n",
    "        the indices of Gaussians corresponding to one object mask in the current view.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_masks,) containing the assigned label\n",
    "        index for each input mask.\n",
    "    \"\"\"\n",
    "    num_masks = len(mask_gaussians)\n",
    "\n",
    "    # If this is the first view (no masks have been assigned yet), \n",
    "    # assign each mask a unique label (0 to num_masks - 1).\n",
    "    if self.total_masks == 0:\n",
    "        return torch.arange(num_masks, dtype=torch.long)\n",
    "\n",
    "    # Otherwise, initialize an empty label tensor.\n",
    "    labels = torch.zeros(num_masks, dtype=torch.long)\n",
    "\n",
    "    # Loop over each mask's Gaussians in the current view\n",
    "    for i, gaussians in enumerate(mask_gaussians):\n",
    "        overlaps = []\n",
    "        n_gaussians = len(gaussians)\n",
    "\n",
    "        # Compare with each mask's bank of Gaussians seen so far\n",
    "        for bank in self.memory_bank:\n",
    "            union = torch.unique(torch.cat([bank, gaussians]))\n",
    "            intersection = len(bank) + n_gaussians - len(union)\n",
    "\n",
    "            # Compute IOCUR: intersection over (intersection + current size)\n",
    "            io_cur = intersection / (n_gaussians + intersection + 1e-8)\n",
    "            overlaps.append(io_cur)\n",
    "\n",
    "        # Convert to tensor on the correct device for computation\n",
    "        overlaps = torch.tensor(overlaps, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Choose the best-matching label based on maximum IOCUR\n",
    "        selected = torch.argmax(overlaps)\n",
    "\n",
    "        # If no match passes threshold, assign a new label\n",
    "        if overlaps[selected] < self.iou_threshold:\n",
    "            selected = self.total_masks\n",
    "            self.total_masks += 1 # Increment the number of masks to account for new label\n",
    "\n",
    "        labels[i] = selected\n",
    "\n",
    "    return labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
