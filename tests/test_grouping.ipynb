{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "# from ns_extension.utils.grouping import GroupingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:07:01] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:07:01]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=25405;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=834340;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m load_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m load_config \u001b[38;5;241m=\u001b[39m Path(load_config)\n\u001b[0;32m----> 5\u001b[0m config, pipeline, checkpoint_path, step \u001b[38;5;241m=\u001b[39m \u001b[43meval_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/utils/eval_utils.py:106\u001b[0m, in \u001b[0;36meval_setup\u001b[0;34m(config_path, eval_num_rays_per_chunk, test_mode, update_config_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# setup pipeline (which includes the DataManager)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pipeline, Pipeline)\n\u001b[1;32m    108\u001b[0m pipeline\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/configs/base_config.py:53\u001b[0m, in \u001b[0;36mInstantiateConfig.setup\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the instantiated object using the config.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/pipelines/base_pipeline.py:254\u001b[0m, in \u001b[0;36mVanillaPipeline.__init__\u001b[0;34m(self, config, device, test_mode, world_size, local_rank, grad_scaler)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_mode \u001b[38;5;241m=\u001b[39m test_mode\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamanager: DataManager \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# TODO make cleaner\u001b[39;00m\n\u001b[1;32m    258\u001b[0m seed_pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/configs/base_config.py:53\u001b[0m, in \u001b[0;36mInstantiateConfig.setup\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the instantiated object using the config.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/datamanagers/features_datamanager.py:72\u001b[0m, in \u001b[0;36mFeatureSplattingDataManager.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Extract or load cached features for all images\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dict)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Split features into train and eval sets\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/datamanagers/features_datamanager.py:98\u001b[0m, in \u001b[0;36mFeatureSplattingDataManager.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Try loading from cache if enabled\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_cache \u001b[38;5;129;01mand\u001b[39;00m cache_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 98\u001b[0m     cache_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_filenames\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m image_filenames:\n\u001b[1;32m    101\u001b[0m         CONSOLE\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage filenames have changed, cache invalidated...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to the config for a trained model\n",
    "load_config = '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml'\n",
    "load_config = Path(load_config)\n",
    "\n",
    "config, pipeline, checkpoint_path, step = eval_setup(load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question is whether to build grouping on top of the existing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m camera, batch \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241m.\u001b[39mdatamanager\u001b[38;5;241m.\u001b[39mnext_train(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m fn \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mdatamanager\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mimage_filenames[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "camera, batch = pipeline.datamanager.next_train(0)\n",
    "\n",
    "fn = pipeline.datamanager.train_dataset.image_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/RogerQi_MobileSAMV2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_load_scucess\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m Segmentation(\n\u001b[1;32m      8\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobilesamv2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m segmentation\u001b[38;5;241m.\u001b[39mstrategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mfn\u001b[49m)\n\u001b[1;32m     15\u001b[0m H, W \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mheight, image\u001b[38;5;241m.\u001b[39mwidth\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # Prepare image for segmentation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# image = resize_image(image, longest_edge=1024) # Resize image to SAM resolution\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Apply segmentation masks over features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from ns_extension.utils.features import resize_image\n",
    "from ns_extension.utils.segmentation import Segmentation\n",
    "\n",
    "segmentation = Segmentation(\n",
    "    backend='mobilesamv2',\n",
    "    strategy='object',\n",
    "    device='cuda',\n",
    ")\n",
    "segmentation.strategy = 'auto'\n",
    "\n",
    "image = Image.open(fn)\n",
    "H, W = image.height, image.width\n",
    "\n",
    "# # Prepare image for segmentation\n",
    "# image = resize_image(image, longest_edge=1024) # Resize image to SAM resolution\n",
    "\n",
    "# Apply segmentation masks over features\n",
    "image = np.asarray(image) # Convert to numpy array\n",
    "masks, results = segmentation.segment(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start making our functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gaga --> gaussian grouping via multiview association + memory bank\n",
    "\n",
    "Steps:\n",
    "1. Create masks --> for each view within the dataset, create masks\n",
    "    - Original implementation saves them out as images, but we could just save them out as tensors\n",
    "\n",
    "2. Associate masks --> creates the memory bank?\n",
    "    - Front percentage (0.2)\n",
    "    - Overlap threshold (0.1)\n",
    "    - For each camera --> \n",
    "        - If no masks, initialize a memory bank for the first view's masks\n",
    "        - Get gaussian idxs and zcoords (for depth grouping) for the current view\n",
    "        - Find front gaussians:\n",
    "            - Create Spatial patch mask --> divides image into patch grid\n",
    "            - Object masks --> goes through each mask in the image\n",
    "            - Combines the two masks (i.e., find overlap between patch and object mask)\n",
    "            - Find frontmost gaussians within each patch for each object\n",
    "        - Based on this:\n",
    "            - Stores the indices of the front gaussians\n",
    "            - Mask ID = tensor of ALL indices of that mask (i.e., all gaussians in that mask)\n",
    "            - Num masks == number of masks in the memory bank\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ns_extension.utils.utils import project_gaussians\n",
    "\n",
    "class GroupingClassifier(nn.Module):\n",
    "    def __init__(self, num_masks: int, num_gaussians: int):\n",
    "        super(GroupingClassifier, self).__init__()\n",
    "\n",
    "        # eval_setup(load_config)\n",
    "        self.num_masks = num_masks\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.classifier = nn.Conv2d(in_channels=num_masks, out_channels=num_gaussians, kernel_size=1)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Mask initialization ######################\n",
    "    #########################################################\n",
    "\n",
    "    def set_patch_mask(self, image, num_patches: int = 32):\n",
    "        \"\"\"\n",
    "        Provided an image of given dimensions, create an array of patches.\n",
    "        \"\"\"\n",
    "        # Get image dimensions\n",
    "        H, W = image.shape[:2]\n",
    "\n",
    "        # Get patch dimensions\n",
    "        patch_width = math.ceil(W / num_patches)\n",
    "        patch_height = math.ceil(H / num_patches)\n",
    "        \n",
    "        # Create flattened coordinates\n",
    "        total_pixels = H * W\n",
    "        y_coords = torch.arange(H).unsqueeze(1).expand(-1, W).flatten()\n",
    "        x_coords = torch.arange(W).unsqueeze(0).expand(H, -1).flatten()\n",
    "        \n",
    "        # Calculate patch indices for all pixels at once\n",
    "        patch_y_indices = torch.clamp(y_coords // patch_height, 0, num_patches - 1)\n",
    "        patch_x_indices = torch.clamp(x_coords // patch_width, 0, num_patches - 1)\n",
    "        \n",
    "        # Create sparse representation\n",
    "        flatten_patch_mask = torch.zeros((num_patches, num_patches, total_pixels), \n",
    "                                    dtype=torch.bool)\n",
    "        \n",
    "        # Use indexing to set values\n",
    "        pixel_indices = torch.arange(total_pixels)\n",
    "        flatten_patch_mask[patch_y_indices, patch_x_indices, pixel_indices] = True\n",
    "        \n",
    "        return flatten_patch_mask\n",
    "    \n",
    "    def create_composite_mask(self, results, confidence_threshold=0.85):\n",
    "        \"\"\"\n",
    "        Creates a composite mask from the results of the segmentation model.\n",
    "        \n",
    "        Inputs:\n",
    "            results: list of dicts, each containing a mask and a confidence score\n",
    "            confidence_threshold: float, the minimum confidence score for a mask to be included in the composite mask\n",
    "\n",
    "        Outputs:\n",
    "            composite_mask: numpy array, the composite mask\n",
    "        \"\"\"\n",
    "\n",
    "        selected_masks = []\n",
    "        for mask in results:\n",
    "            if mask['predicted_iou'] < confidence_threshold:\n",
    "                continue\n",
    "\n",
    "            selected_masks.append(\n",
    "                (mask['segmentation'], mask['predicted_iou'])\n",
    "            )\n",
    "        \n",
    "        # Store the masks and confidences\n",
    "        masks, confs = zip(*selected_masks)\n",
    "\n",
    "        # Create empty image to store mask ids\n",
    "        H, W = masks[0].shape[:2]\n",
    "        mask_id = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        sorted_idxs = np.argsort(confs)\n",
    "        for i, idx in enumerate(sorted_idxs, start=1):\n",
    "            current_mask = masks[idx - 1]\n",
    "            mask_id[current_mask == 1] = i\n",
    "\n",
    "        # Find mask indices after having calculated overlap based on ranked confidence\n",
    "        mask_indices = np.unique(mask_id)\n",
    "        mask_indices = np.setdiff1d(mask_indices, [0]) # remove 0 item\n",
    "\n",
    "        composite_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        for i, idx in enumerate(mask_indices, start=1):\n",
    "            mask = (mask_id == idx)\n",
    "            if mask.sum() > 0 and (mask.sum() / masks[idx-1].sum()) > 0.1:\n",
    "                composite_mask[mask] = i\n",
    "\n",
    "        return composite_mask\n",
    "\n",
    "    def mask_id_to_binary_mask(self, composite_mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert an image with integer mask IDs to a binary mask array.\n",
    "\n",
    "        Args:\n",
    "            mask_id (np.ndarray): An (H, W) array where each unique positive integer \n",
    "                                represents a separate object mask.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A (N, H, W) boolean array where N is the number of masks and each \n",
    "                        slice contains a binary mask.\n",
    "        \"\"\"\n",
    "        unique_ids = np.unique(composite_mask)\n",
    "        unique_ids = unique_ids[unique_ids > 0]  # Ignore background (assumed to be 0)\n",
    "\n",
    "        binary_masks = (composite_mask[None, ...] == unique_ids[:, None, None])\n",
    "        return binary_masks\n",
    "\n",
    "    #########################################################\n",
    "    ############## Gaussian selection #######################\n",
    "    #########################################################\n",
    "\n",
    "    def select_front_gaussians(self, model, camera, composite_mask, front_percentage: float = 0.5):\n",
    "        \"\"\"\n",
    "        JIT-compiled version using torch.compile (PyTorch 2.0+).\n",
    "        Maintains original structure and comments while adding compilation optimization.\n",
    "        Now with separated helper functions for better code organization.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Project gaussians onto 2d image\n",
    "        proj_results = project_gaussians(model, camera)\n",
    "        \n",
    "        # Prepare masks = Decimate the composite mask into individual masks\n",
    "        binary_masks = self.mask_id_to_binary_mask(composite_mask)\n",
    "        flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)  # (N, H*W)\n",
    "\n",
    "        # Pre-extract proj_results for compiled function\n",
    "        proj_flattened = proj_results['proj_flattened']\n",
    "        proj_depths = proj_results['proj_depths']\n",
    "\n",
    "        # Compute the gaussian lookup table\n",
    "        max_gaussian_id = proj_results['gaussian_ids'].max() if len(proj_results['gaussian_ids']) > 0 else 0\n",
    "        valid_gaussian_mask = torch.zeros(max_gaussian_id + 1, dtype=torch.bool, device=proj_results['gaussian_ids'].device)\n",
    "        valid_gaussian_mask[proj_results['gaussian_ids']] = True\n",
    "\n",
    "        front_gaussians = []\n",
    "\n",
    "        for mask in tqdm(flattened_masks, total=len(flattened_masks), desc=\"Processing masks\"):\n",
    "            # Use compiled function for main processing\n",
    "            result = self.process_mask_gaussians(\n",
    "                mask, \n",
    "                proj_results, \n",
    "                valid_gaussian_mask, \n",
    "                front_percentage=front_percentage\n",
    "            )\n",
    "            \n",
    "            front_gaussians.append(result)\n",
    "\n",
    "        return front_gaussians\n",
    "\n",
    "    @torch.compile(mode=\"max-autotune\")\n",
    "    def process_mask_gaussians(self, mask, proj_results: Dict[str, torch.Tensor], valid_gaussian_mask: torch.Tensor, front_percentage: float = 0.5):\n",
    "        \"\"\"\n",
    "        JIT-compiled function for processing a single mask.\n",
    "        Optimized for performance with torch.compile.\n",
    "        \"\"\"\n",
    "        # Find intersection between object mask and patch masks\n",
    "        patch_intersections = mask.unsqueeze(0).unsqueeze(0) & self.patch_mask\n",
    "\n",
    "        # Find non-empty patches\n",
    "        patch_sums = patch_intersections.sum(dim=2)\n",
    "        non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n",
    "\n",
    "        if len(non_empty_patches) == 0:\n",
    "            return torch.tensor([], dtype=torch.long, device=mask.device)\n",
    "        \n",
    "        # Extract all patches at once\n",
    "        mask_gaussians = []\n",
    "        patches_data = patch_intersections[non_empty_patches[:, 0], non_empty_patches[:, 1]]\n",
    "\n",
    "        # Go through each non-empty patch and get the front gaussians\n",
    "        for patch_idx, current_patch in enumerate(patches_data):\n",
    "            # Projected flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "            # Grab gaussians in the current patch\n",
    "            patch_gaussians = current_patch[proj_results['proj_flattened']].nonzero().squeeze(-1)\n",
    "            \n",
    "            if len(patch_gaussians) == 0:\n",
    "                continue\n",
    "\n",
    "            # Filter valid gaussians using pre-computed mask\n",
    "            overlap_mask = valid_gaussian_mask[patch_gaussians]\n",
    "\n",
    "            if not overlap_mask.all():\n",
    "                invalid_count = (~overlap_mask).sum()\n",
    "                print(f\"Found {invalid_count} gaussians not in the IDs\")\n",
    "                print(\"Gaussians not in the IDs: \", patch_gaussians[~overlap_mask])\n",
    "\n",
    "            # Note: Error checking moved outside compiled function for better performance\n",
    "            patch_gaussians = patch_gaussians[overlap_mask]\n",
    "\n",
    "            if len(patch_gaussians) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Grab the depths of the gaussians in the patch\n",
    "            num_front_gaussians = max(int(front_percentage * len(patch_gaussians)), 1)\n",
    "            \n",
    "            if num_front_gaussians < len(patch_gaussians):\n",
    "                # Use partial sorting for better performance\n",
    "                patch_depths = proj_results['proj_depths'][patch_gaussians]\n",
    "                _, front_indices = torch.topk(patch_depths, num_front_gaussians, largest=False)\n",
    "                selected_gaussians = patch_gaussians[front_indices]\n",
    "            else:\n",
    "                selected_gaussians = patch_gaussians\n",
    "            \n",
    "            mask_gaussians.append(selected_gaussians)\n",
    "\n",
    "        if len(mask_gaussians) > 0:\n",
    "            mask_gaussians = torch.cat(mask_gaussians)\n",
    "            return mask_gaussians\n",
    "        else:\n",
    "            return torch.tensor([], dtype=torch.long, device=mask.device)\n",
    "\n",
    "    def associate_masks(self):\n",
    "        pass\n",
    "\n",
    "# def get_n_different_colors(n: int) -> np.ndarray:\n",
    "#     np.random.seed(0)\n",
    "#     return np.random.randint(1, 256, (n, 3), dtype=np.uint8)\n",
    "\n",
    "# def visualize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "#     color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "#     num_masks = np.max(mask)\n",
    "#     random_colors = get_n_different_colors(num_masks)\n",
    "#     for i in range(num_masks):\n",
    "#         color_mask[mask == i+1] = random_colors[i]\n",
    "#     return color_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the grouping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      2\u001b[0m proj_results \u001b[38;5;241m=\u001b[39m project_gaussians(model, camera)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "model = pipeline.model\n",
    "proj_results = project_gaussians(model, camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 540)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['segmentation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n",
      "Processing masks:   0%|          | 0/65 [00:00<?, ?it/s]skipping cudagraphs for unknown reason\n",
      "Processing masks:   8%|▊         | 5/65 [00:02<00:24,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1195031])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  11%|█         | 7/65 [00:02<00:22,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1317422])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  37%|███▋      | 24/65 [00:08<00:14,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([730857])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([169239])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([281163])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([424230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  60%|██████    | 39/65 [00:14<00:09,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1103701, 1178042])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  68%|██████▊   | 44/65 [00:16<00:08,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([897836])\n",
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1089400, 1096504])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  78%|███████▊  | 51/65 [00:18<00:04,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1236157])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([369113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  91%|█████████ | 59/65 [00:20<00:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([ 333050,  718835, 1230981])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  94%|█████████▍| 61/65 [00:21<00:01,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1264189, 1296009])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1309851])\n",
      "Found 4 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([466000, 526218, 720453, 992983])\n",
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([392888, 787331])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([895690])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1048921])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  97%|█████████▋| 63/65 [00:22<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1303073])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([719282])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([813548])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1103224])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1231555])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1118953])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1089382])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1254684])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([115346])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1191680])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([150277])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([36077])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([194914])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1231573])\n",
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([285126, 904047])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([384937])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1254771])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([652547])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([35])\n",
      "Found 3 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([  85541, 1121214, 1165543])\n",
      "Found 5 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([118247, 401735, 439226, 599613, 952390])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1231637])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([992976])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1121497])\n",
      "Found 2 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([ 644910, 1232854])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([913792])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([546112])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([925])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([228335])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([823473])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([477592])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1279455])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1232611])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1258072])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1266847])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1255004])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1302337])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([534900])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1231101])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([861653])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks:  98%|█████████▊| 64/65 [00:25<00:01,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([482238])\n",
      "Found 1 gaussians not in the IDs\n",
      "Gaussians not in the IDs:  tensor([1133756])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing masks: 100%|██████████| 65/65 [00:26<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "composite_mask = create_composite_mask(results)\n",
    "patch_mask = create_patch_mask(image, num_patches=32)\n",
    "front_gaussians = select_front_gaussians(model, camera, composite_mask, patch_mask, front_percentage = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the intersection between a given object mask and the patch masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the current mask, but we loop over these in the final product\n",
    "current_mask = flattened_masks[0]\n",
    "patch_intersections = current_mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "# Find non-empty patches\n",
    "patch_sums = patch_intersections.sum(dim=2)\n",
    "non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the gaussians into pixel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.model\n",
    "proj_results = project_gaussians(model, camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab a non-empty patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current patch, find its associated gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projected flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "projected_flattened = proj_results['proj_flattened'] # (M,)\n",
    "patch_gaussians = current_patch[projected_flattened.cpu()].nonzero().squeeze(-1)\n",
    "\n",
    "# This should pass --> need to check all found patch gaussians are in the valid gaussians\n",
    "assert torch.isin(patch_gaussians.detach().cpu(), proj_results['gaussian_ids'].detach().cpu()).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the depths of the gaussians in the patch\n",
    "patch_gaussian_depths = proj_results['proj_depths'][patch_gaussians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_percentage = 0.2\n",
    "num_patch_gaussians = len(patch_gaussians)\n",
    "num_front_gaussians = max(int(front_percentage * num_patch_gaussians), 1)\n",
    "\n",
    "# Sort the gaussians by depth\n",
    "sorted_gaussian_ids = torch.argsort(patch_gaussian_depths)\n",
    "\n",
    "# gaussians sorted by their depths and select the front based on percentage\n",
    "sorted_gaussians = patch_gaussians[sorted_gaussian_ids]\n",
    "selected_gaussians = sorted_gaussians[:num_front_gaussians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_gaussians(model, image, camera, composite_mask, n_patches: int = 32, front_percentage: float = 0.5):\n",
    "\n",
    "    # Project the gaussians to 2d\n",
    "    proj_results = project_gaussians(model, camera)\n",
    "\n",
    "    # Decimate the composite mask into individual masks\n",
    "    binary_masks = mask_id_to_binary_mask(composite_mask)\n",
    "    flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)\n",
    "\n",
    "    # Create a patch mask --> find the intersection between the composite mask and the patch mask\n",
    "    patch_mask = create_patch_mask(image, n_patches)\n",
    "\n",
    "    gaussians = []\n",
    "\n",
    "    for mask in flattened_masks:\n",
    "\n",
    "        # Find the intersection between the mask and the patch mask\n",
    "        patch_intersections = mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "        # Find the non-empty patches\n",
    "        patch_sums = patch_intersections.sum(dim=2)  # Sum pixels per patch\n",
    "        non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n",
    "\n",
    "        # If there are no non-empty patches, add an empty tensor to the gaussians list\n",
    "        if len(non_empty_patches) == 0:\n",
    "            gaussians.append(torch.tensor([], dtype=torch.long))\n",
    "            continue\n",
    "        \n",
    "        # Find the gaussian ids that are inside the non-empty patches\n",
    "        mask_gaussians = []\n",
    "\n",
    "        for patch_idx in non_empty_patches:\n",
    "            i, j = patch_idx\n",
    "            current_patch = patch_intersections[i, j]\n",
    "\n",
    "            # current_patch\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "proj_results = project_gaussians(model, camera)\n",
    "\n",
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
