{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.3, llvm 15.0.4, commit 5ec301be, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/15/25 20:36:41.723 64208] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "# from ns_extension.utils.grouping import GroupingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:36:46] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:36:46]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=573329;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=816130;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:37:14] </span>use color only optimization with sigmoid activation                                         <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">splatfacto.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#266\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">266</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:37:14]\u001b[0m\u001b[2;36m \u001b[0muse color only optimization with sigmoid activation                                         \u001b]8;id=489066;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\u001b\\\u001b[2msplatfacto.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=447081;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#266\u001b\\\u001b[2m266\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">step-00002</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">9999.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "\u001b[35m/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/nerfstudio_models/\u001b[0m\u001b[95mstep-00002\u001b[0m\n",
       "\u001b[95m9999.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the config for a trained model\n",
    "load_config = '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml'\n",
    "load_config = Path(load_config)\n",
    "\n",
    "config, pipeline, checkpoint_path, step = eval_setup(load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question is whether to build grouping on top of the existing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:37:38] </span>Caching <span style=\"color: #800080; text-decoration-color: #800080\">/</span> undistorting train images                                            <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">full_images_datamanager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py#230\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">230</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:37:38]\u001b[0m\u001b[2;36m \u001b[0mCaching \u001b[35m/\u001b[0m undistorting train images                                            \u001b]8;id=607797;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py\u001b\\\u001b[2mfull_images_datamanager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=294323;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py#230\u001b\\\u001b[2m230\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b2a3b2fa674edfb4c829f932ea2b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera, batch = pipeline.datamanager.next_train(0)\n",
    "\n",
    "fn = pipeline.datamanager.train_dataset.image_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/RogerQi_MobileSAMV2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_load_scucess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from ns_extension.utils.features import resize_image\n",
    "from ns_extension.utils.segmentation import Segmentation\n",
    "\n",
    "segmentation = Segmentation(\n",
    "    backend='mobilesamv2',\n",
    "    strategy='object',\n",
    "    device='cuda',\n",
    ")\n",
    "segmentation.strategy = 'auto'\n",
    "\n",
    "image = Image.open(fn)\n",
    "H, W = image.height, image.width\n",
    "\n",
    "# # Prepare image for segmentation\n",
    "# image = resize_image(image, longest_edge=1024) # Resize image to SAM resolution\n",
    "\n",
    "# Apply segmentation masks over features\n",
    "image = np.asarray(image) # Convert to numpy array\n",
    "masks, results = segmentation.segment(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start making our functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def create_composite_mask(results, confidence_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Creates a composite mask from the results of the segmentation model.\n",
    "    \n",
    "    Inputs:\n",
    "        results: list of dicts, each containing a mask and a confidence score\n",
    "        confidence_threshold: float, the minimum confidence score for a mask to be included in the composite mask\n",
    "\n",
    "    Outputs:\n",
    "        composite_mask: numpy array, the composite mask\n",
    "    \"\"\"\n",
    "\n",
    "    selected_masks = []\n",
    "    for mask in results:\n",
    "        if mask['predicted_iou'] < confidence_threshold:\n",
    "            continue\n",
    "\n",
    "        selected_masks.append(\n",
    "            (mask['segmentation'], mask['predicted_iou'])\n",
    "        )\n",
    "    \n",
    "    # Store the masks and confidences\n",
    "    masks, confs = zip(*selected_masks)\n",
    "\n",
    "    # Create empty image to store mask ids\n",
    "    mask_id = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    sorted_idxs = np.argsort(confs)\n",
    "    for i, idx in enumerate(sorted_idxs, start=1):\n",
    "        current_mask = masks[idx - 1]\n",
    "        mask_id[current_mask == 1] = i\n",
    "\n",
    "    # Find mask indices after having calculated overlap based on ranked confidence\n",
    "    mask_indices = np.unique(mask_id)\n",
    "    mask_indices = np.setdiff1d(mask_indices, [0]) # remove 0 item\n",
    "\n",
    "    composite_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    for i, idx in enumerate(mask_indices, start=1):\n",
    "        mask = (mask_id == idx)\n",
    "        if mask.sum() > 0 and (mask.sum() / masks[idx-1].sum()) > 0.1:\n",
    "            composite_mask[mask] = i\n",
    "\n",
    "    return composite_mask\n",
    "\n",
    "def mask_id_to_binary_mask(composite_mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert an image with integer mask IDs to a binary mask array.\n",
    "\n",
    "    Args:\n",
    "        mask_id (np.ndarray): An (H, W) array where each unique positive integer \n",
    "                            represents a separate object mask.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (N, H, W) boolean array where N is the number of masks and each \n",
    "                    slice contains a binary mask.\n",
    "    \"\"\"\n",
    "    unique_ids = np.unique(composite_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]  # Ignore background (assumed to be 0)\n",
    "\n",
    "    binary_masks = (composite_mask[None, ...] == unique_ids[:, None, None])\n",
    "    return binary_masks\n",
    "\n",
    "def create_patch_mask(image, num_patches):\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    \n",
    "    patch_width = math.ceil(image_width / num_patches)\n",
    "    patch_height = math.ceil(image_height / num_patches)\n",
    "    \n",
    "    # Create flattened coordinates\n",
    "    total_pixels = image_height * image_width\n",
    "    y_coords = torch.arange(image_height).unsqueeze(1).expand(-1, image_width).flatten()\n",
    "    x_coords = torch.arange(image_width).unsqueeze(0).expand(image_height, -1).flatten()\n",
    "    \n",
    "    # Calculate patch indices for all pixels at once\n",
    "    patch_y_indices = torch.clamp(y_coords // patch_height, 0, num_patches - 1)\n",
    "    patch_x_indices = torch.clamp(x_coords // patch_width, 0, num_patches - 1)\n",
    "    \n",
    "    # Create sparse representation\n",
    "    flatten_patch_mask = torch.zeros((num_patches, num_patches, total_pixels), \n",
    "                                   dtype=torch.bool)\n",
    "    \n",
    "    # Use indexing to set values\n",
    "    pixel_indices = torch.arange(total_pixels)\n",
    "    flatten_patch_mask[patch_y_indices, patch_x_indices, pixel_indices] = True\n",
    "    \n",
    "    return flatten_patch_mask\n",
    "\n",
    "def project_gaussians(model, camera):\n",
    "\n",
    "    _ = model.get_outputs(camera)\n",
    "    meta = model.info\n",
    "    W, H = meta[\"width\"], meta[\"height\"]\n",
    "\n",
    "    # gaussians where the radius is greater than 1.0 can be seen in the camera frustum\n",
    "    radii = model.info['radii'].squeeze()\n",
    "    gaussian_ids = torch.where(torch.sum(radii > 1.0, axis=1))[0]\n",
    "\n",
    "    # Convert 2D coords to flat pixel indices\n",
    "    xy_rounded = torch.round(meta['means2d']).squeeze().long()\n",
    "    x = torch.clamp(xy_rounded[:, 0], 0, W)\n",
    "    y = torch.clamp(xy_rounded[:, 1], 0, H)\n",
    "    projected_flattened = x + y * W                      # (M,)\n",
    "\n",
    "    return {\n",
    "        \"proj_flattened\": projected_flattened,                      # (M,)\n",
    "        \"proj_depths\": meta['depths'],                                      # (M,)\n",
    "        \"gaussian_ids\": gaussian_ids,                 # (M,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the grouping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a composite mask\n",
    "composite_mask = create_composite_mask(results, confidence_threshold=0.85)\n",
    "\n",
    "# Decimate the composite mask into individual masks\n",
    "binary_masks = mask_id_to_binary_mask(composite_mask)\n",
    "\n",
    "# Flatten the binary masks\n",
    "flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)\n",
    "\n",
    "# Create a patch mask --> find the intersection between the composite mask and the patch mask\n",
    "patch_mask = create_patch_mask(image, num_patches=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the intersection between a given object mask and the patch masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_intersections = flattened_masks[0].unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "# Find non-empty patches\n",
    "patch_sums = patch_intersections.sum(dim=2)  # Sum pixels per patch\n",
    "non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the gaussians into pixel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RadegsFeaturesModelConfig' object has no attribute 'return_packed_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m----> 2\u001b[0m proj_results \u001b[38;5;241m=\u001b[39m \u001b[43mproject_gaussians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 94\u001b[0m, in \u001b[0;36mproject_gaussians\u001b[0;34m(model, camera)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mproject_gaussians\u001b[39m(model, camera):\n\u001b[0;32m---> 94\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     meta \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m     96\u001b[0m     W, H \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m], meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/models/rade_features_model.py:248\u001b[0m, in \u001b[0;36mRadegsFeaturesModel.get_outputs\u001b[0;34m(self, camera)\u001b[0m\n\u001b[1;32m    244\u001b[0m     sh_degree_to_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Modified rasterization function from https://github.com/brian-xu/gsplat-rade/blob/main/gsplat/rendering.py\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Enables returning depth and normal maps for computing of loss\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m return_packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_packed_info\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Rendered contains the following:\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# - rgb: [N, 3]\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# - alphas: [N, 1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# - expected_normals: [N, 1]\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# - meta (set to self.info)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m render, alpha, expected_depths, median_depths, expected_normals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\n\u001b[1;32m    258\u001b[0m     means\u001b[38;5;241m=\u001b[39mmeans_crop,\n\u001b[1;32m    259\u001b[0m     quats\u001b[38;5;241m=\u001b[39mquats_crop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     packed\u001b[38;5;241m=\u001b[39mreturn_packed,\n\u001b[1;32m    269\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RadegsFeaturesModelConfig' object has no attribute 'return_packed_info'"
     ]
    }
   ],
   "source": [
    "model = pipeline.model\n",
    "proj_results = project_gaussians(model, camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab a non-empty patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current patch, find its associated gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([518400])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current patch is a flattened image, where there's a set of pixels in that patch we are looking for\n",
    "patch_gaussians = current_patch[projected_flattened.cpu()].nonzero().squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'proj_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mproj_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'proj_results' is not defined"
     ]
    }
   ],
   "source": [
    "proj_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected_flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "patch_gaussians = current_patch[projected_flattened.cpu()].nonzero().squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_gaussians(model, camera):\n",
    "\n",
    "    _ = model.get_outputs(camera)\n",
    "    meta = model.info\n",
    "    W, H = meta[\"width\"], meta[\"height\"]\n",
    "\n",
    "    # gaussians where the radius is greater than 1.0 can be seen in the camera frustum\n",
    "    radii = model.info['radii'].squeeze()\n",
    "    gaussian_ids = torch.where(torch.sum(radii > 1.0, axis=1))[0]\n",
    "\n",
    "    # Convert 2D coords to flat pixel indices\n",
    "    xy_rounded = torch.round(meta['means2d']).squeeze().long()\n",
    "    x = torch.clamp(xy_rounded[:, 0], 0, W)\n",
    "    y = torch.clamp(xy_rounded[:, 1], 0, H)\n",
    "    projected_flattened = x + y * W                      # (M,)\n",
    "\n",
    "    reverse_mapping = {gid.item(): i for i, gid in enumerate(gaussian_ids)}\n",
    "\n",
    "    return {\n",
    "        \"proj_flattened\": projected_flattened,                      # (M,)\n",
    "        \"proj_depths\": meta['depths'],                                      # (M,)\n",
    "        \"gaussian_ids\": gaussian_ids,                 # (M,)\n",
    "        \"gaussian_id_reverse_mapping\": reverse_mapping,      # dict\n",
    "    }\n",
    "\n",
    "def get_mask_gaussians(model, image, camera, composite_mask, n_patches: int = 32, front_percentage: float = 0.5):\n",
    "\n",
    "    # Project the gaussians to 2d\n",
    "    proj_results = project_gaussians(model, camera)\n",
    "\n",
    "    # Decimate the composite mask into individual masks\n",
    "    binary_masks = mask_id_to_binary_mask(composite_mask)\n",
    "    flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)\n",
    "\n",
    "    # Create a patch mask --> find the intersection between the composite mask and the patch mask\n",
    "    patch_mask = create_patch_mask(image, n_patches)\n",
    "\n",
    "    gaussians = []\n",
    "\n",
    "    for mask in flattened_masks:\n",
    "\n",
    "        # Find the intersection between the mask and the patch mask\n",
    "        patch_intersections = mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "        # Find the non-empty patches\n",
    "        patch_sums = patch_intersections.sum(dim=2)  # Sum pixels per patch\n",
    "        non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n",
    "\n",
    "        # If there are no non-empty patches, add an empty tensor to the gaussians list\n",
    "        if len(non_empty_patches) == 0:\n",
    "            gaussians.append(torch.tensor([], dtype=torch.long))\n",
    "            continue\n",
    "        \n",
    "        # Find the gaussian ids that are inside the non-empty patches\n",
    "        mask_gaussians = []\n",
    "\n",
    "        for patch_idx in non_empty_patches:\n",
    "            i, j = patch_idx\n",
    "            current_patch = patch_intersections[i, j]\n",
    "\n",
    "            # current_patch\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "proj_results = project_gaussians(model, camera)\n",
    "\n",
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projected_flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "patch_gaussians = current_patch[projected_flattened.cpu()].nonzero().squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_tensor_str.py:373\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "proj_results['gaussian_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 112617],\n",
       "        [ 119493],\n",
       "        [ 128966],\n",
       "        [ 131631],\n",
       "        [ 150154],\n",
       "        [ 166959],\n",
       "        [ 191581],\n",
       "        [ 206662],\n",
       "        [ 227541],\n",
       "        [ 228650],\n",
       "        [ 258188],\n",
       "        [ 281990],\n",
       "        [ 332490],\n",
       "        [ 356528],\n",
       "        [ 387293],\n",
       "        [ 389121],\n",
       "        [ 396813],\n",
       "        [ 434758],\n",
       "        [ 537677],\n",
       "        [ 639315],\n",
       "        [ 666595],\n",
       "        [ 780970],\n",
       "        [ 917891],\n",
       "        [ 919133],\n",
       "        [1046450],\n",
       "        [1047148],\n",
       "        [1086774],\n",
       "        [1087433],\n",
       "        [1229077],\n",
       "        [1230453],\n",
       "        [1237335]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_patch[projected_flattened.cpu()].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 517438, 517439, 517440], device='cuda:0')"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_flattened.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([181014])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_results['gaussian_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([130238])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_results['proj_flattened'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_gaussian_ids = torch.where(patch_gaussians)[0]\n",
    "patch_gaussian_depths = proj_results['proj_depths'].squeeze()[patch_gaussian_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2D coords to flat pixel indices\n",
    "xy_rounded = torch.round(meta['means2d']).squeeze().long()\n",
    "x = torch.clamp(xy_rounded[:, 0], 0, W)\n",
    "y = torch.clamp(xy_rounded[:, 1], 0, H)\n",
    "projected_flattened = x + y * W                      # (M,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([      8,      13,      14,  ..., 1339596, 1339602, 1339611], device='cuda:0')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the IDs of the gaussians within the current camera frustum\n",
    "gaussian_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
