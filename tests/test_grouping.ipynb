{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.3, llvm 15.0.4, commit 5ec301be, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/21/25 14:13:50.135 13956] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "# from ns_extension.utils.grouping import GroupingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:14:54] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:14:54]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=829588;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=930260;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:19:21] </span>use color only optimization with sigmoid activation                                         <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">splatfacto.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#266\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">266</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:19:21]\u001b[0m\u001b[2;36m \u001b[0muse color only optimization with sigmoid activation                                         \u001b]8;id=337416;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py\u001b\\\u001b[2msplatfacto.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=119761;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/models/splatfacto.py#266\u001b\\\u001b[2m266\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/nerfstudio_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">step-00002</span>\n",
       "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">9999.ckpt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "\u001b[35m/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/nerfstudio_models/\u001b[0m\u001b[95mstep-00002\u001b[0m\n",
       "\u001b[95m9999.ckpt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the config for a trained model\n",
    "load_config = '/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-features/2025-07-11_171420/config.yml'\n",
    "load_config = Path(load_config)\n",
    "\n",
    "config, pipeline, checkpoint_path, step = eval_setup(load_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question is whether to build grouping on top of the existing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:35:00] </span>Caching <span style=\"color: #800080; text-decoration-color: #800080\">/</span> undistorting train images                                            <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">full_images_datamanager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py#230\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">230</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:35:00]\u001b[0m\u001b[2;36m \u001b[0mCaching \u001b[35m/\u001b[0m undistorting train images                                            \u001b]8;id=644880;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py\u001b\\\u001b[2mfull_images_datamanager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=601863;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/datamanagers/full_images_datamanager.py#230\u001b\\\u001b[2m230\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2a95ec5ba4e4a895ba56397e99f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera, batch = pipeline.datamanager.next_train(0)\n",
    "\n",
    "fn = pipeline.datamanager.train_dataset.image_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/RogerQi_MobileSAMV2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_load_scucess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from ns_extension.utils.features import resize_image\n",
    "from ns_extension.utils.segmentation import Segmentation\n",
    "\n",
    "segmentation = Segmentation(\n",
    "    backend='mobilesamv2',\n",
    "    strategy='object',\n",
    "    device='cuda',\n",
    ")\n",
    "segmentation.strategy = 'auto'\n",
    "\n",
    "image = Image.open(fn)\n",
    "H, W = image.height, image.width\n",
    "\n",
    "# # Prepare image for segmentation\n",
    "# image = resize_image(image, longest_edge=1024) # Resize image to SAM resolution\n",
    "\n",
    "# Apply segmentation masks over features\n",
    "image = np.asarray(image) # Convert to numpy array\n",
    "masks, results = segmentation.segment(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start making our functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 129 (460887998.py, line 132)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 132\u001b[0;36m\u001b[0m\n\u001b[0;31m    result = self.process_mask_gaussians(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 129\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gaga --> gaussian grouping via multiview association + memory bank\n",
    "\n",
    "Steps:\n",
    "1. Create masks --> for each view within the dataset, create masks\n",
    "    - Original implementation saves them out as images, but we could just save them out as tensors\n",
    "\n",
    "2. Associate masks --> creates the memory bank?\n",
    "    - Front percentage (0.2)\n",
    "    - Overlap threshold (0.1)\n",
    "    - For each camera --> \n",
    "        - If no masks, initialize a memory bank for the first view's masks\n",
    "        - Get gaussian idxs and zcoords (for depth grouping) for the current view\n",
    "        - Find front gaussians:\n",
    "            - Create Spatial patch mask --> divides image into patch grid\n",
    "            - Object masks --> goes through each mask in the image\n",
    "            - Combines the two masks (i.e., find overlap between patch and object mask)\n",
    "            - Find frontmost gaussians within each patch for each object\n",
    "        - Based on this:\n",
    "            - Stores the indices of the front gaussians\n",
    "            - Mask ID = tensor of ALL indices of that mask (i.e., all gaussians in that mask)\n",
    "            - Num masks == number of masks in the memory bank\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nerfstudio.models.splatfacto import SplatfactoModel\n",
    "from ns_extension.utils.segmentation import Segmentation, create_patch_mask, create_composite_mask, mask_id_to_binary_mask\n",
    "from ns_extension.utils.utils import project_gaussians\n",
    "\n",
    "class GroupingClassifier(nn.Module):\n",
    "    def __init__(self, load_config: str, segmentation_backend: str, segmentation_strategy: str):\n",
    "        super(GroupingClassifier, self).__init__()\n",
    "\n",
    "        self.load_config = load_config\n",
    "        self.segmentation_backend = segmentation_backend\n",
    "        self.segmentation_strategy = segmentation_strategy\n",
    "\n",
    "        # eval_setup(load_config)\n",
    "        # self.num_masks = num_masks\n",
    "        # self.num_gaussians = num_gaussians\n",
    "        # self.classifier = nn.Conv2d(in_channels=num_masks, out_channels=num_gaussians, kernel_size=1)\n",
    "\n",
    "    #########################################################\n",
    "    ############## Mask initialization ######################\n",
    "    #########################################################\n",
    "\n",
    "    def associate(self):\n",
    "        \"\"\"\n",
    "        Creates a memory bank that associates gaussians within masks across fields-of-view.\n",
    "        \"\"\"\n",
    "\n",
    "        _, pipeline, _, _ = eval_setup(self.load_config)\n",
    "\n",
    "        assert isinstance(pipeline.model, SplatfactoModel)\n",
    "\n",
    "        model: SplatfactoModel = pipeline.model\n",
    "\n",
    "        # Load the segmentation model\n",
    "        segmentation = Segmentation(\n",
    "            backend=self.segmentation_backend,\n",
    "            strategy=self.segmentation_strategy,\n",
    "            device=model.device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cameras: Cameras = pipeline.datamanager.train_dataset.cameras  # type: ignore\n",
    "            # TODO: do eval dataset as well\n",
    "\n",
    "            for image_idx, data in tqdm(\n",
    "                enumerate(pipeline.datamanager.train_dataset),  # type: ignore\n",
    "                desc=\"Processing frames\",\n",
    "                total=len(pipeline.datamanager.train_dataset)\n",
    "            ):\n",
    "                # Grab camera and forward pass through model\n",
    "                camera = cameras[image_idx : image_idx + 1]\n",
    "                image = data[\"image\"]\n",
    "\n",
    "                # Forward pass through model to get metadata\n",
    "                _ = model.get_outputs(camera=camera)\n",
    "\n",
    "                # Segment the image\n",
    "                patch_mask = create_patch_mask(image)\n",
    "                _, results = segmentation.segment(image)\n",
    "\n",
    "                # Create composite mask\n",
    "                composite_mask = create_composite_mask(results)\n",
    "\n",
    "                # Select front gaussians\n",
    "                front_gaussians = self.select_front_gaussians(\n",
    "                    meta=model.info, \n",
    "                    composite_mask=composite_mask, \n",
    "                    patch_mask=patch_mask\n",
    "                )\n",
    "                \n",
    "    #########################################################\n",
    "    ############## Gaussian selection #######################\n",
    "    #########################################################\n",
    "\n",
    "    def select_front_gaussians(self, meta: Dict[str, torch.Tensor], composite_mask: torch.Tensor, patch_mask: torch.Tensor, front_percentage: float = 0.5):\n",
    "        \"\"\"\n",
    "        JIT-compiled version using torch.compile (PyTorch 2.0+).\n",
    "        Maintains original structure and comments while adding compilation optimization.\n",
    "        Now with separated helper functions for better code organization.\n",
    "        \"\"\"\n",
    "\n",
    "        proj_results = project_gaussians(meta)\n",
    "                \n",
    "        # Prepare masks = Decimate the composite mask into individual masks\n",
    "        binary_masks = self.mask_id_to_binary_mask(composite_mask)\n",
    "        flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)  # (N, H*W)\n",
    "\n",
    "        # Compute the gaussian lookup table\n",
    "        max_gaussian_id = proj_results['gaussian_ids'].max() if len(proj_results['gaussian_ids']) > 0 else 0\n",
    "        valid_gaussian_mask = torch.zeros(max_gaussian_id + 1, dtype=torch.bool, device=proj_results['gaussian_ids'].device)\n",
    "        valid_gaussian_mask[proj_results['gaussian_ids']] = True\n",
    "\n",
    "        front_gaussians = []\n",
    "\n",
    "        for mask in tqdm(flattened_masks, total=len(flattened_masks), desc=\"Processing masks\"):\n",
    "\n",
    "            if patch_mask is not None:\n",
    "\n",
    "            # Use compiled function for main processing\n",
    "            result = self.process_mask_gaussians(\n",
    "                proj_results, \n",
    "                mask, \n",
    "                patch_mask,\n",
    "                valid_gaussian_mask, \n",
    "                front_percentage=front_percentage\n",
    "            )\n",
    "            \n",
    "            front_gaussians.append(result)\n",
    "\n",
    "        return front_gaussians\n",
    "\n",
    "    @torch.compile(mode=\"max-autotune\")\n",
    "    def process_mask_gaussians(self,  proj_results: Dict[str, torch.Tensor], mask: torch.Tensor, patch_mask: torch.Tensor, valid_gaussian_mask: torch.Tensor, front_percentage: float = 0.5):\n",
    "        \"\"\"\n",
    "        JIT-compiled function for processing a single mask.\n",
    "        Optimized for performance with torch.compile.\n",
    "        \"\"\"\n",
    "\n",
    "        ### TLB THIS SECTION COULD BE REFACTORED OUT FOR MORE FLEXIBILITY (PATCH VS NO PATCH)\n",
    "        # Find intersection between object mask and patch masks\n",
    "        patch_intersections = mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "        # Find non-empty patches\n",
    "        patch_sums = patch_intersections.sum(dim=2)\n",
    "        non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n",
    "\n",
    "        if len(non_empty_patches) == 0:\n",
    "            return torch.tensor([], dtype=torch.long, device=mask.device)\n",
    "        \n",
    "        # Extract all patches at once\n",
    "        mask_gaussians = []\n",
    "        patches_data = patch_intersections[non_empty_patches[:, 0], non_empty_patches[:, 1]]\n",
    "\n",
    "        # Go through each non-empty patch and get the front gaussians\n",
    "        for patch_idx, current_patch in enumerate(patches_data):\n",
    "            # Projected flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "            # Grab gaussians in the current patch\n",
    "            patch_gaussians = current_patch[proj_results['proj_flattened']].nonzero().squeeze(-1)\n",
    "            \n",
    "            if len(patch_gaussians) == 0:\n",
    "                continue\n",
    "\n",
    "            # Filter valid gaussians using pre-computed mask\n",
    "            overlap_mask = valid_gaussian_mask[patch_gaussians]\n",
    "\n",
    "            if not overlap_mask.all():\n",
    "                invalid_count = (~overlap_mask).sum()\n",
    "                print(f\"Found {invalid_count} gaussians not in the IDs\")\n",
    "                print(\"Gaussians not in the IDs: \", patch_gaussians[~overlap_mask])\n",
    "\n",
    "            # Note: Error checking moved outside compiled function for better performance\n",
    "            patch_gaussians = patch_gaussians[overlap_mask]\n",
    "\n",
    "            if len(patch_gaussians) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Grab the depths of the gaussians in the patch\n",
    "            num_front_gaussians = max(int(front_percentage * len(patch_gaussians)), 1)\n",
    "            \n",
    "            if num_front_gaussians < len(patch_gaussians):\n",
    "                # Use partial sorting for better performance\n",
    "                patch_depths = proj_results['proj_depths'][patch_gaussians]\n",
    "                _, front_indices = torch.topk(patch_depths, num_front_gaussians, largest=False)\n",
    "                selected_gaussians = patch_gaussians[front_indices]\n",
    "            else:\n",
    "                selected_gaussians = patch_gaussians\n",
    "            \n",
    "            mask_gaussians.append(selected_gaussians)\n",
    "\n",
    "        if len(mask_gaussians) > 0:\n",
    "            mask_gaussians = torch.cat(mask_gaussians)\n",
    "            return mask_gaussians\n",
    "        else:\n",
    "            return torch.tensor([], dtype=torch.long, device=mask.device)\n",
    "\n",
    "    def associate_masks(self):\n",
    "        pass\n",
    "\n",
    "# def get_n_different_colors(n: int) -> np.ndarray:\n",
    "#     np.random.seed(0)\n",
    "#     return np.random.randint(1, 256, (n, 3), dtype=np.uint8)\n",
    "\n",
    "# def visualize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "#     color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "#     num_masks = np.max(mask)\n",
    "#     random_colors = get_n_different_colors(num_masks)\n",
    "#     for i in range(num_masks):\n",
    "#         color_mask[mask == i+1] = random_colors[i]\n",
    "#     return color_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To initialize the classifier, we first need to build the mask association\n",
    "# This involves \n",
    "# 1) segmenting each camera view\n",
    "# 2) associating the masks to the gaussians\n",
    "\n",
    "\n",
    "def build_mask_association(model, cameras):\n",
    "    for camera in tqdm(cameras):\n",
    "        camera = camera.to(model.device)\n",
    "\n",
    "        front_gaussians, _ = get_patch_front_gaussian_of_mask(model, camera)\n",
    "        labels = assign_labels(front_gaussians)\n",
    "        self._update_gaussian_idx_bank(labels, front_gaussians)\n",
    "\n",
    "        if self.num_mask == 0:\n",
    "            self.assigned_gaussians = torch.unique(torch.cat(front_gaussians))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the grouping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n",
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.model\n",
    "\n",
    "outputs = model.get_outputs(camera)\n",
    "meta = model.info\n",
    "\n",
    "proj_results = project_gaussians(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_composite_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m composite_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_composite_mask\u001b[49m(results)\n\u001b[1;32m      2\u001b[0m patch_mask \u001b[38;5;241m=\u001b[39m create_patch_mask(image, num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      3\u001b[0m front_gaussians \u001b[38;5;241m=\u001b[39m select_front_gaussians(model, camera, composite_mask, patch_mask, front_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_composite_mask' is not defined"
     ]
    }
   ],
   "source": [
    "composite_mask = create_composite_mask(results)\n",
    "patch_mask = create_patch_mask(image, num_patches=32)\n",
    "front_gaussians = select_front_gaussians(model, camera, composite_mask, patch_mask, front_percentage = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the intersection between a given object mask and the patch masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the current mask, but we loop over these in the final product\n",
    "current_mask = flattened_masks[0]\n",
    "patch_intersections = current_mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "# Find non-empty patches\n",
    "patch_sums = patch_intersections.sum(dim=2)\n",
    "non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the gaussians into pixel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.model\n",
    "proj_results = project_gaussians(model, camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab a non-empty patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current patch, find its associated gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projected flattened are the pixel coordinates of each gaussian --> current patch is the pixels of the mask\n",
    "projected_flattened = proj_results['proj_flattened'] # (M,)\n",
    "patch_gaussians = current_patch[projected_flattened.cpu()].nonzero().squeeze(-1)\n",
    "\n",
    "# This should pass --> need to check all found patch gaussians are in the valid gaussians\n",
    "assert torch.isin(patch_gaussians.detach().cpu(), proj_results['gaussian_ids'].detach().cpu()).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the depths of the gaussians in the patch\n",
    "patch_gaussian_depths = proj_results['proj_depths'][patch_gaussians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_percentage = 0.2\n",
    "num_patch_gaussians = len(patch_gaussians)\n",
    "num_front_gaussians = max(int(front_percentage * num_patch_gaussians), 1)\n",
    "\n",
    "# Sort the gaussians by depth\n",
    "sorted_gaussian_ids = torch.argsorxxt(patch_gaussian_depths)\n",
    "\n",
    "# gaussians sorted by their depths and select the front based on percentage\n",
    "sorted_gaussians = patch_gaussians[sorted_gaussian_ids]\n",
    "selected_gaussians = sorted_gaussians[:num_front_gaussians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_gaussians(model, image, camera, composite_mask, n_patches: int = 32, front_percentage: float = 0.5):\n",
    "\n",
    "    # Project the gaussians to 2d\n",
    "    proj_results = project_gaussians(model, camera)\n",
    "\n",
    "    # Decimate the composite mask into individual masks\n",
    "    binary_masks = mask_id_to_binary_mask(composite_mask)\n",
    "    flattened_masks = torch.tensor(binary_masks).flatten(start_dim=1)\n",
    "\n",
    "    # Create a patch mask --> find the intersection between the composite mask and the patch mask\n",
    "    patch_mask = create_patch_mask(image, n_patches)\n",
    "\n",
    "    gaussians = []\n",
    "\n",
    "    for mask in flattened_masks:\n",
    "\n",
    "        # Find the intersection between the mask and the patch mask\n",
    "        patch_intersections = mask.unsqueeze(0).unsqueeze(0) & patch_mask\n",
    "\n",
    "        # Find the non-empty patches\n",
    "        patch_sums = patch_intersections.sum(dim=2)  # Sum pixels per patch\n",
    "        non_empty_patches = (patch_sums > 0).nonzero(as_tuple=False)\n",
    "\n",
    "        # If there are no non-empty patches, add an empty tensor to the gaussians list\n",
    "        if len(non_empty_patches) == 0:\n",
    "            gaussians.append(torch.tensor([], dtype=torch.long))\n",
    "            continue\n",
    "        \n",
    "        # Find the gaussian ids that are inside the non-empty patches\n",
    "        mask_gaussians = []\n",
    "\n",
    "        for patch_idx in non_empty_patches:\n",
    "            i, j = patch_idx\n",
    "            current_patch = patch_intersections[i, j]\n",
    "\n",
    "            # current_patch\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]\n",
    "\n",
    "            # Find the gaussian ids that are inside the current patch\n",
    "            patch_gaussians = torch.where(current_patch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "proj_results = project_gaussians(model, camera)\n",
    "\n",
    "i, j = non_empty_patches[4]\n",
    "current_patch = patch_intersections[i, j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
