{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.3, llvm 15.0.4, commit 5ec301be, linux, python 3.10.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/10/25 02:34:40.089 94240] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# from rade_gs.models import RadegsModel, RadegsModelConfig\n",
    "from nerfstudio.models.splatfacto import SplatfactoModel, SplatfactoModelConfig\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from nerfstudio.data.scene_box import OrientedBox, SceneBox\n",
    "\n",
    "from ns_extension.datamanagers.features_datamanager import FeatureSplattingDataManagerConfig, FeatureSplattingDataManager\n",
    "from ns_extension.models.rade_gs_model import RadegsModelConfig, RadegsModel\n",
    "from ns_extension.models.rade_features_model import RadegsFeaturesModelConfig, RadegsFeaturesModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find config for our current model\n",
    "load_config = Path('/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/feature-splatting/2025-07-03_022235/config.yml')\n",
    "\n",
    "config = yaml.load(load_config.read_text(), Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.pipeline.datamanager.segmentation_backend = \"mobilesamv2\"\n",
    "config.pipeline.datamanager.segmentation_strategy = \"object\"\n",
    "config.pipeline.datamanager.obj_resolution = 100\n",
    "config.pipeline.datamanager.final_resolution = 64\n",
    "# config.pipeline.datamanager.feature_type = \"CLIP\"\n",
    "config.pipeline.datamanager.enable_cache = True\n",
    "config.pipeline.datamanager.sam_resolution = 1024\n",
    "\n",
    "config.pipeline.datamanager.main_features = \"samclip\"\n",
    "config.pipeline.datamanager.regularization_features = \"dinov2\"\n",
    "\n",
    "# datamanager = FeatureSplattingDataManager(config.pipeline.datamanager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:34:59] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                 <a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:34:59]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m2\u001b[0m                                                 \u001b]8;id=612200;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=156450;file:///opt/conda/envs/nerfstudio/lib/python3.10/site-packages/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cache does not exist, extracting features<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cache does not exist, extracting features\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracting samclip features for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">489</span> images<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Extracting samclip features for \u001b[1;36m489\u001b[0m images\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /workspace/models/hub/facebookresearch_dinov2_main\n",
      "/workspace/models/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/workspace/models/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/workspace/models/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Extracting dinov2 features:   0%|          | 0/489 [00:00<?, ?it/s]/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Extracting dinov2 features: 100%|██████████| 489/489 [00:59<00:00,  8.17it/s]\n",
      "Using cache found in /workspace/models/hub/RogerQi_MobileSAMV2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_load_scucess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting samclip features:   0%|          | 0/489 [00:00<?, ?it/s]\n",
      "0: 1024x576 28 objects, 149.4ms\n",
      "Speed: 3.3ms preprocess, 149.4ms inference, 52.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   0%|          | 1/489 [00:01<15:15,  1.88s/it]\n",
      "0: 1024x576 28 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   0%|          | 2/489 [00:02<08:29,  1.05s/it]\n",
      "0: 1024x576 25 objects, 20.6ms\n",
      "Speed: 2.2ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   1%|          | 3/489 [00:02<06:24,  1.26it/s]\n",
      "0: 1024x576 30 objects, 22.6ms\n",
      "Speed: 1.7ms preprocess, 22.6ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   1%|          | 4/489 [00:03<05:43,  1.41it/s]\n",
      "0: 1024x576 29 objects, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   1%|          | 5/489 [00:03<05:06,  1.58it/s]\n",
      "0: 1024x576 27 objects, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   1%|          | 6/489 [00:04<04:41,  1.71it/s]\n",
      "0: 1024x576 29 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   1%|▏         | 7/489 [00:04<04:28,  1.80it/s]\n",
      "0: 1024x576 36 objects, 20.0ms\n",
      "Speed: 1.5ms preprocess, 20.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   2%|▏         | 8/489 [00:05<04:21,  1.84it/s]\n",
      "0: 1024x576 36 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   2%|▏         | 9/489 [00:05<04:14,  1.89it/s]\n",
      "0: 1024x576 31 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   2%|▏         | 10/489 [00:06<04:08,  1.93it/s]\n",
      "0: 1024x576 33 objects, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   2%|▏         | 11/489 [00:06<04:04,  1.95it/s]\n",
      "0: 1024x576 27 objects, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   2%|▏         | 12/489 [00:07<04:00,  1.98it/s]\n",
      "0: 1024x576 29 objects, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   3%|▎         | 13/489 [00:07<03:59,  1.99it/s]\n",
      "0: 1024x576 33 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   3%|▎         | 14/489 [00:08<03:58,  1.99it/s]\n",
      "0: 1024x576 33 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   3%|▎         | 15/489 [00:08<04:09,  1.90it/s]\n",
      "0: 1024x576 43 objects, 21.4ms\n",
      "Speed: 1.4ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   3%|▎         | 16/489 [00:09<04:21,  1.81it/s]\n",
      "0: 1024x576 28 objects, 21.5ms\n",
      "Speed: 1.4ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   3%|▎         | 17/489 [00:10<04:01,  1.95it/s]\n",
      "0: 1024x576 52 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   4%|▎         | 18/489 [00:10<04:04,  1.93it/s]\n",
      "0: 1024x576 55 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   4%|▍         | 19/489 [00:11<04:09,  1.88it/s]\n",
      "0: 1024x576 31 objects, 20.7ms\n",
      "Speed: 1.8ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   4%|▍         | 20/489 [00:11<04:04,  1.92it/s]\n",
      "0: 1024x576 27 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   4%|▍         | 21/489 [00:12<04:00,  1.95it/s]\n",
      "0: 1024x576 41 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   4%|▍         | 22/489 [00:12<03:59,  1.95it/s]\n",
      "0: 1024x576 34 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   5%|▍         | 23/489 [00:13<03:55,  1.98it/s]\n",
      "0: 1024x576 44 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   5%|▍         | 24/489 [00:13<03:57,  1.96it/s]\n",
      "0: 1024x576 36 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   5%|▌         | 25/489 [00:14<03:52,  1.99it/s]\n",
      "0: 1024x576 41 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   5%|▌         | 26/489 [00:14<03:53,  1.98it/s]\n",
      "0: 1024x576 39 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   6%|▌         | 27/489 [00:15<03:52,  1.99it/s]\n",
      "0: 1024x576 50 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   6%|▌         | 28/489 [00:15<04:04,  1.89it/s]\n",
      "0: 1024x576 68 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   6%|▌         | 29/489 [00:16<04:13,  1.81it/s]\n",
      "0: 1024x576 66 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   6%|▌         | 30/489 [00:16<04:20,  1.76it/s]\n",
      "0: 1024x576 66 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   6%|▋         | 31/489 [00:17<04:23,  1.74it/s]\n",
      "0: 1024x576 53 objects, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   7%|▋         | 32/489 [00:18<04:25,  1.72it/s]\n",
      "0: 1024x576 54 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   7%|▋         | 33/489 [00:18<04:28,  1.70it/s]\n",
      "0: 1024x576 62 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   7%|▋         | 34/489 [00:19<04:29,  1.69it/s]\n",
      "0: 1024x576 57 objects, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   7%|▋         | 35/489 [00:19<04:29,  1.68it/s]\n",
      "0: 1024x576 62 objects, 20.4ms\n",
      "Speed: 1.5ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   7%|▋         | 36/489 [00:20<04:29,  1.68it/s]\n",
      "0: 1024x576 68 objects, 21.1ms\n",
      "Speed: 1.4ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   8%|▊         | 37/489 [00:21<04:29,  1.67it/s]\n",
      "0: 1024x576 44 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   8%|▊         | 38/489 [00:21<04:18,  1.75it/s]\n",
      "0: 1024x576 32 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   8%|▊         | 39/489 [00:22<04:05,  1.84it/s]\n",
      "0: 1024x576 42 objects, 20.5ms\n",
      "Speed: 1.6ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   8%|▊         | 40/489 [00:22<04:00,  1.86it/s]\n",
      "0: 1024x576 36 objects, 20.8ms\n",
      "Speed: 1.3ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   8%|▊         | 41/489 [00:23<03:52,  1.92it/s]\n",
      "0: 1024x576 35 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   9%|▊         | 42/489 [00:23<03:49,  1.95it/s]\n",
      "0: 1024x576 54 objects, 21.1ms\n",
      "Speed: 1.3ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   9%|▉         | 43/489 [00:24<04:01,  1.85it/s]\n",
      "0: 1024x576 57 objects, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   9%|▉         | 44/489 [00:24<04:09,  1.79it/s]\n",
      "0: 1024x576 64 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   9%|▉         | 45/489 [00:25<04:13,  1.75it/s]\n",
      "0: 1024x576 50 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:   9%|▉         | 46/489 [00:25<04:08,  1.78it/s]\n",
      "0: 1024x576 45 objects, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  10%|▉         | 47/489 [00:26<03:58,  1.85it/s]\n",
      "0: 1024x576 51 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  10%|▉         | 48/489 [00:26<03:53,  1.89it/s]\n",
      "0: 1024x576 51 objects, 21.0ms\n",
      "Speed: 1.3ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  10%|█         | 49/489 [00:27<03:57,  1.85it/s]\n",
      "0: 1024x576 65 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  10%|█         | 50/489 [00:28<04:17,  1.70it/s]\n",
      "0: 1024x576 73 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  10%|█         | 51/489 [00:28<04:19,  1.68it/s]\n",
      "0: 1024x576 70 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  11%|█         | 52/489 [00:29<04:20,  1.68it/s]\n",
      "0: 1024x576 86 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  11%|█         | 53/489 [00:30<04:26,  1.64it/s]\n",
      "0: 1024x576 82 objects, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  11%|█         | 54/489 [00:30<04:20,  1.67it/s]\n",
      "0: 1024x576 99 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  11%|█         | 55/489 [00:31<04:41,  1.54it/s]\n",
      "0: 1024x576 83 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  11%|█▏        | 56/489 [00:32<04:38,  1.55it/s]\n",
      "0: 1024x576 88 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  12%|█▏        | 57/489 [00:32<04:35,  1.57it/s]\n",
      "0: 1024x576 71 objects, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  12%|█▏        | 58/489 [00:33<04:24,  1.63it/s]\n",
      "0: 1024x576 67 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  12%|█▏        | 59/489 [00:33<04:22,  1.64it/s]\n",
      "0: 1024x576 53 objects, 20.9ms\n",
      "Speed: 1.3ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  12%|█▏        | 60/489 [00:34<04:19,  1.65it/s]\n",
      "0: 1024x576 65 objects, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  12%|█▏        | 61/489 [00:35<04:19,  1.65it/s]\n",
      "0: 1024x576 62 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  13%|█▎        | 62/489 [00:35<04:17,  1.66it/s]\n",
      "0: 1024x576 57 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  13%|█▎        | 63/489 [00:36<04:17,  1.66it/s]\n",
      "0: 1024x576 61 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  13%|█▎        | 64/489 [00:36<04:15,  1.66it/s]\n",
      "0: 1024x576 49 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  13%|█▎        | 65/489 [00:37<04:06,  1.72it/s]\n",
      "0: 1024x576 35 objects, 21.0ms\n",
      "Speed: 1.5ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  13%|█▎        | 66/489 [00:37<03:50,  1.83it/s]\n",
      "0: 1024x576 35 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  14%|█▎        | 67/489 [00:38<03:43,  1.88it/s]\n",
      "0: 1024x576 12 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  14%|█▍        | 68/489 [00:38<03:37,  1.93it/s]\n",
      "0: 1024x576 15 objects, 21.5ms\n",
      "Speed: 1.4ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  14%|█▍        | 69/489 [00:39<03:23,  2.07it/s]\n",
      "0: 1024x576 33 objects, 21.6ms\n",
      "Speed: 1.3ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  14%|█▍        | 70/489 [00:39<03:26,  2.03it/s]\n",
      "0: 1024x576 40 objects, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  15%|█▍        | 71/489 [00:40<03:28,  2.01it/s]\n",
      "0: 1024x576 43 objects, 21.0ms\n",
      "Speed: 1.5ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  15%|█▍        | 72/489 [00:40<03:29,  1.99it/s]\n",
      "0: 1024x576 44 objects, 21.0ms\n",
      "Speed: 1.3ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  15%|█▍        | 73/489 [00:41<03:35,  1.93it/s]\n",
      "0: 1024x576 46 objects, 21.5ms\n",
      "Speed: 1.4ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  15%|█▌        | 74/489 [00:41<03:46,  1.83it/s]\n",
      "0: 1024x576 35 objects, 21.4ms\n",
      "Speed: 1.4ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  15%|█▌        | 75/489 [00:42<03:41,  1.87it/s]\n",
      "0: 1024x576 59 objects, 21.3ms\n",
      "Speed: 1.4ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  16%|█▌        | 76/489 [00:43<04:02,  1.70it/s]\n",
      "0: 1024x576 70 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  16%|█▌        | 77/489 [00:43<04:17,  1.60it/s]\n",
      "0: 1024x576 59 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  16%|█▌        | 78/489 [00:44<04:24,  1.55it/s]\n",
      "0: 1024x576 58 objects, 20.7ms\n",
      "Speed: 2.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  16%|█▌        | 79/489 [00:45<04:30,  1.52it/s]\n",
      "0: 1024x576 48 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  16%|█▋        | 80/489 [00:45<04:23,  1.55it/s]\n",
      "0: 1024x576 46 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  17%|█▋        | 81/489 [00:46<04:17,  1.59it/s]\n",
      "0: 1024x576 56 objects, 20.9ms\n",
      "Speed: 1.3ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  17%|█▋        | 82/489 [00:47<04:25,  1.53it/s]\n",
      "0: 1024x576 62 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  17%|█▋        | 83/489 [00:47<04:31,  1.50it/s]\n",
      "0: 1024x576 45 objects, 20.5ms\n",
      "Speed: 1.3ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  17%|█▋        | 84/489 [00:48<04:32,  1.48it/s]\n",
      "0: 1024x576 42 objects, 21.1ms\n",
      "Speed: 1.4ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  17%|█▋        | 85/489 [00:49<04:35,  1.47it/s]\n",
      "0: 1024x576 44 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  18%|█▊        | 86/489 [00:49<04:38,  1.45it/s]\n",
      "0: 1024x576 37 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  18%|█▊        | 87/489 [00:50<04:34,  1.46it/s]\n",
      "0: 1024x576 47 objects, 21.7ms\n",
      "Speed: 1.4ms preprocess, 21.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  18%|█▊        | 88/489 [00:51<04:27,  1.50it/s]\n",
      "0: 1024x576 47 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  18%|█▊        | 89/489 [00:51<04:30,  1.48it/s]\n",
      "0: 1024x576 60 objects, 21.1ms\n",
      "Speed: 1.4ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  18%|█▊        | 90/489 [00:52<04:34,  1.45it/s]\n",
      "0: 1024x576 52 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  19%|█▊        | 91/489 [00:53<04:34,  1.45it/s]\n",
      "0: 1024x576 49 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  19%|█▉        | 92/489 [00:54<04:34,  1.44it/s]\n",
      "0: 1024x576 46 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  19%|█▉        | 93/489 [00:54<04:34,  1.44it/s]\n",
      "0: 1024x576 63 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  19%|█▉        | 94/489 [00:55<04:36,  1.43it/s]\n",
      "0: 1024x576 55 objects, 20.3ms\n",
      "Speed: 1.4ms preprocess, 20.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  19%|█▉        | 95/489 [00:56<04:34,  1.43it/s]\n",
      "0: 1024x576 77 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  20%|█▉        | 96/489 [00:56<04:48,  1.36it/s]\n",
      "0: 1024x576 64 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  20%|█▉        | 97/489 [00:57<04:42,  1.39it/s]\n",
      "0: 1024x576 61 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  20%|██        | 98/489 [00:58<04:38,  1.40it/s]\n",
      "0: 1024x576 61 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  20%|██        | 99/489 [00:59<04:36,  1.41it/s]\n",
      "0: 1024x576 78 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  20%|██        | 100/489 [00:59<04:48,  1.35it/s]\n",
      "0: 1024x576 69 objects, 20.8ms\n",
      "Speed: 1.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  21%|██        | 101/489 [01:00<04:57,  1.31it/s]\n",
      "0: 1024x576 46 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  21%|██        | 102/489 [01:01<04:46,  1.35it/s]\n",
      "0: 1024x576 51 objects, 20.9ms\n",
      "Speed: 2.2ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  21%|██        | 103/489 [01:02<04:42,  1.37it/s]\n",
      "0: 1024x576 47 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  21%|██▏       | 104/489 [01:02<04:37,  1.39it/s]\n",
      "0: 1024x576 34 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  21%|██▏       | 105/489 [01:03<04:31,  1.42it/s]\n",
      "0: 1024x576 35 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  22%|██▏       | 106/489 [01:04<04:28,  1.43it/s]\n",
      "0: 1024x576 29 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  22%|██▏       | 107/489 [01:04<04:20,  1.47it/s]\n",
      "0: 1024x576 35 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  22%|██▏       | 108/489 [01:05<04:17,  1.48it/s]\n",
      "0: 1024x576 41 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  22%|██▏       | 109/489 [01:06<04:23,  1.44it/s]\n",
      "0: 1024x576 50 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  22%|██▏       | 110/489 [01:06<04:24,  1.43it/s]\n",
      "0: 1024x576 49 objects, 21.1ms\n",
      "Speed: 1.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  23%|██▎       | 111/489 [01:07<04:23,  1.43it/s]\n",
      "0: 1024x576 44 objects, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  23%|██▎       | 112/489 [01:08<04:23,  1.43it/s]\n",
      "0: 1024x576 35 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  23%|██▎       | 113/489 [01:08<04:18,  1.45it/s]\n",
      "0: 1024x576 42 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  23%|██▎       | 114/489 [01:09<04:23,  1.42it/s]\n",
      "0: 1024x576 40 objects, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  24%|██▎       | 115/489 [01:10<04:20,  1.43it/s]\n",
      "0: 1024x576 42 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  24%|██▎       | 116/489 [01:11<04:20,  1.43it/s]\n",
      "0: 1024x576 38 objects, 20.4ms\n",
      "Speed: 1.5ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  24%|██▍       | 117/489 [01:11<04:20,  1.43it/s]\n",
      "0: 1024x576 39 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  24%|██▍       | 118/489 [01:12<04:19,  1.43it/s]\n",
      "0: 1024x576 39 objects, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  24%|██▍       | 119/489 [01:13<04:17,  1.44it/s]\n",
      "0: 1024x576 25 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  25%|██▍       | 120/489 [01:13<04:06,  1.50it/s]\n",
      "0: 1024x576 18 objects, 21.3ms\n",
      "Speed: 2.6ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  25%|██▍       | 121/489 [01:14<03:47,  1.62it/s]\n",
      "0: 1024x576 18 objects, 20.7ms\n",
      "Speed: 1.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  25%|██▍       | 122/489 [01:14<03:34,  1.71it/s]\n",
      "0: 1024x576 5 objects, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  25%|██▌       | 123/489 [01:15<03:23,  1.80it/s]\n",
      "0: 1024x576 20 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  25%|██▌       | 124/489 [01:15<03:29,  1.74it/s]\n",
      "0: 1024x576 30 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  26%|██▌       | 125/489 [01:16<03:34,  1.69it/s]\n",
      "0: 1024x576 23 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  26%|██▌       | 126/489 [01:17<03:42,  1.63it/s]\n",
      "0: 1024x576 33 objects, 20.4ms\n",
      "Speed: 2.2ms preprocess, 20.4ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  26%|██▌       | 127/489 [01:17<03:45,  1.61it/s]\n",
      "0: 1024x576 36 objects, 21.5ms\n",
      "Speed: 1.6ms preprocess, 21.5ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  26%|██▌       | 128/489 [01:18<03:45,  1.60it/s]\n",
      "0: 1024x576 34 objects, 20.5ms\n",
      "Speed: 1.6ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  26%|██▋       | 129/489 [01:19<03:52,  1.55it/s]\n",
      "0: 1024x576 21 objects, 20.7ms\n",
      "Speed: 1.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  27%|██▋       | 130/489 [01:19<03:38,  1.64it/s]\n",
      "0: 1024x576 31 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  27%|██▋       | 131/489 [01:20<03:39,  1.63it/s]\n",
      "0: 1024x576 32 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  27%|██▋       | 132/489 [01:20<03:48,  1.56it/s]\n",
      "0: 1024x576 41 objects, 21.4ms\n",
      "Speed: 1.6ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  27%|██▋       | 133/489 [01:21<03:51,  1.54it/s]\n",
      "0: 1024x576 33 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  27%|██▋       | 134/489 [01:22<03:48,  1.55it/s]\n",
      "0: 1024x576 37 objects, 21.8ms\n",
      "Speed: 1.6ms preprocess, 21.8ms inference, 2.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  28%|██▊       | 135/489 [01:22<03:53,  1.52it/s]\n",
      "0: 1024x576 28 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  28%|██▊       | 136/489 [01:23<03:45,  1.57it/s]\n",
      "0: 1024x576 18 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  28%|██▊       | 137/489 [01:24<03:29,  1.68it/s]\n",
      "0: 1024x576 24 objects, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  28%|██▊       | 138/489 [01:24<03:29,  1.68it/s]\n",
      "0: 1024x576 24 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  28%|██▊       | 139/489 [01:25<03:29,  1.67it/s]\n",
      "0: 1024x576 29 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  29%|██▊       | 140/489 [01:25<03:29,  1.66it/s]\n",
      "0: 1024x576 43 objects, 21.0ms\n",
      "Speed: 1.6ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  29%|██▉       | 141/489 [01:26<03:39,  1.58it/s]\n",
      "0: 1024x576 44 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  29%|██▉       | 142/489 [01:27<03:45,  1.54it/s]\n",
      "0: 1024x576 40 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  29%|██▉       | 143/489 [01:27<03:48,  1.51it/s]\n",
      "0: 1024x576 41 objects, 20.9ms\n",
      "Speed: 1.5ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  29%|██▉       | 144/489 [01:28<03:53,  1.48it/s]\n",
      "0: 1024x576 46 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  30%|██▉       | 145/489 [01:29<03:55,  1.46it/s]\n",
      "0: 1024x576 48 objects, 20.9ms\n",
      "Speed: 1.5ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  30%|██▉       | 146/489 [01:30<03:56,  1.45it/s]\n",
      "0: 1024x576 40 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  30%|███       | 147/489 [01:30<03:56,  1.45it/s]\n",
      "0: 1024x576 31 objects, 21.5ms\n",
      "Speed: 1.5ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  30%|███       | 148/489 [01:31<03:46,  1.50it/s]\n",
      "0: 1024x576 29 objects, 21.3ms\n",
      "Speed: 1.6ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  30%|███       | 149/489 [01:31<03:39,  1.55it/s]\n",
      "0: 1024x576 29 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  31%|███       | 150/489 [01:32<03:33,  1.58it/s]\n",
      "0: 1024x576 32 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  31%|███       | 151/489 [01:33<03:30,  1.60it/s]\n",
      "0: 1024x576 22 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  31%|███       | 152/489 [01:33<03:21,  1.67it/s]\n",
      "0: 1024x576 15 objects, 20.7ms\n",
      "Speed: 1.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  31%|███▏      | 153/489 [01:34<03:15,  1.72it/s]\n",
      "0: 1024x576 16 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  31%|███▏      | 154/489 [01:34<03:06,  1.79it/s]\n",
      "0: 1024x576 11 objects, 20.4ms\n",
      "Speed: 1.7ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  32%|███▏      | 155/489 [01:35<02:58,  1.87it/s]\n",
      "0: 1024x576 11 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  32%|███▏      | 156/489 [01:35<02:54,  1.90it/s]\n",
      "0: 1024x576 15 objects, 21.1ms\n",
      "Speed: 1.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  32%|███▏      | 157/489 [01:36<02:52,  1.92it/s]\n",
      "0: 1024x576 15 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  32%|███▏      | 158/489 [01:36<02:50,  1.94it/s]\n",
      "0: 1024x576 18 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  33%|███▎      | 159/489 [01:37<02:48,  1.96it/s]\n",
      "0: 1024x576 27 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  33%|███▎      | 160/489 [01:37<02:58,  1.85it/s]\n",
      "0: 1024x576 26 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  33%|███▎      | 161/489 [01:38<03:02,  1.79it/s]\n",
      "0: 1024x576 21 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  33%|███▎      | 162/489 [01:39<03:05,  1.77it/s]\n",
      "0: 1024x576 24 objects, 21.1ms\n",
      "Speed: 1.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  33%|███▎      | 163/489 [01:39<03:09,  1.72it/s]\n",
      "0: 1024x576 27 objects, 19.9ms\n",
      "Speed: 1.7ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  34%|███▎      | 164/489 [01:40<03:10,  1.70it/s]\n",
      "0: 1024x576 30 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  34%|███▎      | 165/489 [01:40<03:11,  1.69it/s]\n",
      "0: 1024x576 11 objects, 20.7ms\n",
      "Speed: 1.5ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  34%|███▍      | 166/489 [01:41<03:04,  1.75it/s]\n",
      "0: 1024x576 11 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  34%|███▍      | 167/489 [01:41<02:56,  1.82it/s]\n",
      "0: 1024x576 14 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  34%|███▍      | 168/489 [01:42<03:08,  1.70it/s]\n",
      "0: 1024x576 18 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  35%|███▍      | 169/489 [01:43<02:59,  1.78it/s]\n",
      "0: 1024x576 17 objects, 20.8ms\n",
      "Speed: 1.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  35%|███▍      | 170/489 [01:43<02:54,  1.83it/s]\n",
      "0: 1024x576 20 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  35%|███▍      | 171/489 [01:44<02:49,  1.88it/s]\n",
      "0: 1024x576 28 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  35%|███▌      | 172/489 [01:44<02:56,  1.79it/s]\n",
      "0: 1024x576 27 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  35%|███▌      | 173/489 [01:45<02:59,  1.76it/s]\n",
      "0: 1024x576 39 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  36%|███▌      | 174/489 [01:45<03:04,  1.71it/s]\n",
      "0: 1024x576 23 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  36%|███▌      | 175/489 [01:46<03:03,  1.71it/s]\n",
      "0: 1024x576 47 objects, 22.2ms\n",
      "Speed: 2.2ms preprocess, 22.2ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  36%|███▌      | 176/489 [01:47<03:15,  1.60it/s]\n",
      "0: 1024x576 54 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  36%|███▌      | 177/489 [01:48<03:39,  1.42it/s]\n",
      "0: 1024x576 43 objects, 21.9ms\n",
      "Speed: 1.8ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  36%|███▋      | 178/489 [01:48<03:28,  1.49it/s]\n",
      "0: 1024x576 39 objects, 22.2ms\n",
      "Speed: 1.8ms preprocess, 22.2ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  37%|███▋      | 179/489 [01:49<03:21,  1.54it/s]\n",
      "0: 1024x576 35 objects, 22.3ms\n",
      "Speed: 1.9ms preprocess, 22.3ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  37%|███▋      | 180/489 [01:49<03:17,  1.56it/s]\n",
      "0: 1024x576 36 objects, 21.0ms\n",
      "Speed: 1.9ms preprocess, 21.0ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  37%|███▋      | 181/489 [01:50<03:29,  1.47it/s]\n",
      "0: 1024x576 20 objects, 22.1ms\n",
      "Speed: 1.8ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  37%|███▋      | 182/489 [01:51<03:19,  1.54it/s]\n",
      "0: 1024x576 30 objects, 22.0ms\n",
      "Speed: 2.8ms preprocess, 22.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  37%|███▋      | 183/489 [01:51<03:18,  1.54it/s]\n",
      "0: 1024x576 21 objects, 21.4ms\n",
      "Speed: 1.9ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  38%|███▊      | 184/489 [01:52<03:19,  1.53it/s]\n",
      "0: 1024x576 27 objects, 21.8ms\n",
      "Speed: 2.1ms preprocess, 21.8ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  38%|███▊      | 185/489 [01:53<03:13,  1.57it/s]\n",
      "0: 1024x576 41 objects, 21.2ms\n",
      "Speed: 2.3ms preprocess, 21.2ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  38%|███▊      | 186/489 [01:53<03:11,  1.59it/s]\n",
      "0: 1024x576 45 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  38%|███▊      | 187/489 [01:54<03:09,  1.60it/s]\n",
      "0: 1024x576 66 objects, 22.2ms\n",
      "Speed: 1.9ms preprocess, 22.2ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  38%|███▊      | 188/489 [01:55<03:26,  1.46it/s]\n",
      "0: 1024x576 63 objects, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  39%|███▊      | 189/489 [01:56<03:52,  1.29it/s]\n",
      "0: 1024x576 76 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  39%|███▉      | 190/489 [01:57<04:01,  1.24it/s]\n",
      "0: 1024x576 78 objects, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  39%|███▉      | 191/489 [01:57<04:00,  1.24it/s]\n",
      "0: 1024x576 87 objects, 21.2ms\n",
      "Speed: 1.8ms preprocess, 21.2ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  39%|███▉      | 192/489 [01:58<04:11,  1.18it/s]\n",
      "0: 1024x576 68 objects, 20.8ms\n",
      "Speed: 2.2ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  39%|███▉      | 193/489 [01:59<04:02,  1.22it/s]\n",
      "0: 1024x576 41 objects, 21.9ms\n",
      "Speed: 1.8ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  40%|███▉      | 194/489 [02:00<03:48,  1.29it/s]\n",
      "0: 1024x576 49 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  40%|███▉      | 195/489 [02:00<03:40,  1.33it/s]\n",
      "0: 1024x576 59 objects, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  40%|████      | 196/489 [02:01<03:36,  1.35it/s]\n",
      "0: 1024x576 45 objects, 21.7ms\n",
      "Speed: 2.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  40%|████      | 197/489 [02:02<03:31,  1.38it/s]\n",
      "0: 1024x576 35 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  40%|████      | 198/489 [02:02<03:24,  1.42it/s]\n",
      "0: 1024x576 20 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  41%|████      | 199/489 [02:03<03:09,  1.53it/s]\n",
      "0: 1024x576 7 objects, 21.1ms\n",
      "Speed: 1.9ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  41%|████      | 200/489 [02:04<02:55,  1.65it/s]\n",
      "0: 1024x576 7 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  41%|████      | 201/489 [02:04<02:45,  1.74it/s]\n",
      "0: 1024x576 7 objects, 20.8ms\n",
      "Speed: 1.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  41%|████▏     | 202/489 [02:05<02:37,  1.82it/s]\n",
      "0: 1024x576 3 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  42%|████▏     | 203/489 [02:05<02:33,  1.87it/s]\n",
      "0: 1024x576 1 object, 20.9ms\n",
      "Speed: 1.8ms preprocess, 20.9ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  42%|████▏     | 204/489 [02:06<02:30,  1.89it/s]\n",
      "0: 1024x576 1 object, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  42%|████▏     | 205/489 [02:06<02:26,  1.94it/s]\n",
      "0: 1024x576 1 object, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  42%|████▏     | 206/489 [02:07<02:23,  1.97it/s]\n",
      "0: 1024x576 3 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  42%|████▏     | 207/489 [02:07<02:23,  1.97it/s]\n",
      "0: 1024x576 2 objects, 20.5ms\n",
      "Speed: 2.2ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  43%|████▎     | 208/489 [02:08<02:22,  1.97it/s]\n",
      "0: 1024x576 1 object, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  43%|████▎     | 209/489 [02:08<02:20,  1.99it/s]\n",
      "Extracting samclip features:  43%|████▎     | 210/489 [02:08<01:54,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No object detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 1024x576 4 objects, 20.7ms\n",
      "Speed: 2.1ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  43%|████▎     | 211/489 [02:09<02:02,  2.27it/s]\n",
      "Extracting samclip features:  43%|████▎     | 212/489 [02:09<01:41,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No object detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 1024x576 2 objects, 20.5ms\n",
      "Speed: 1.8ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  44%|████▎     | 213/489 [02:10<02:01,  2.28it/s]\n",
      "0: 1024x576 2 objects, 20.8ms\n",
      "Speed: 1.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  44%|████▍     | 214/489 [02:10<02:05,  2.19it/s]\n",
      "0: 1024x576 1 object, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  44%|████▍     | 215/489 [02:11<02:09,  2.12it/s]\n",
      "0: 1024x576 2 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  44%|████▍     | 216/489 [02:11<02:10,  2.09it/s]\n",
      "0: 1024x576 5 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  44%|████▍     | 217/489 [02:12<02:12,  2.05it/s]\n",
      "0: 1024x576 8 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  45%|████▍     | 218/489 [02:12<02:12,  2.04it/s]\n",
      "0: 1024x576 11 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  45%|████▍     | 219/489 [02:13<02:12,  2.04it/s]\n",
      "0: 1024x576 7 objects, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  45%|████▍     | 220/489 [02:13<02:13,  2.02it/s]\n",
      "0: 1024x576 14 objects, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  45%|████▌     | 221/489 [02:14<02:14,  1.99it/s]\n",
      "0: 1024x576 9 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  45%|████▌     | 222/489 [02:14<02:12,  2.01it/s]\n",
      "0: 1024x576 12 objects, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  46%|████▌     | 223/489 [02:15<02:12,  2.01it/s]\n",
      "0: 1024x576 15 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  46%|████▌     | 224/489 [02:15<02:12,  2.00it/s]\n",
      "0: 1024x576 20 objects, 20.7ms\n",
      "Speed: 1.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  46%|████▌     | 225/489 [02:16<02:12,  1.99it/s]\n",
      "0: 1024x576 19 objects, 20.9ms\n",
      "Speed: 1.9ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  46%|████▌     | 226/489 [02:16<02:12,  1.99it/s]\n",
      "0: 1024x576 29 objects, 20.9ms\n",
      "Speed: 1.8ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  46%|████▋     | 227/489 [02:17<02:25,  1.80it/s]\n",
      "0: 1024x576 24 objects, 20.5ms\n",
      "Speed: 1.7ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  47%|████▋     | 228/489 [02:17<02:28,  1.76it/s]\n",
      "0: 1024x576 31 objects, 20.5ms\n",
      "Speed: 1.6ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  47%|████▋     | 229/489 [02:18<02:38,  1.64it/s]\n",
      "0: 1024x576 38 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  47%|████▋     | 230/489 [02:19<02:45,  1.57it/s]\n",
      "0: 1024x576 43 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  47%|████▋     | 231/489 [02:19<02:44,  1.57it/s]\n",
      "0: 1024x576 61 objects, 21.9ms\n",
      "Speed: 1.7ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  47%|████▋     | 232/489 [02:20<02:54,  1.47it/s]\n",
      "0: 1024x576 68 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  48%|████▊     | 233/489 [02:21<03:05,  1.38it/s]\n",
      "0: 1024x576 71 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  48%|████▊     | 234/489 [02:22<03:09,  1.34it/s]\n",
      "0: 1024x576 39 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  48%|████▊     | 235/489 [02:22<02:57,  1.43it/s]\n",
      "0: 1024x576 58 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  48%|████▊     | 236/489 [02:23<02:57,  1.42it/s]\n",
      "0: 1024x576 61 objects, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  48%|████▊     | 237/489 [02:24<03:03,  1.37it/s]\n",
      "0: 1024x576 56 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  49%|████▊     | 238/489 [02:25<03:01,  1.38it/s]\n",
      "0: 1024x576 49 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  49%|████▉     | 239/489 [02:25<02:54,  1.43it/s]\n",
      "0: 1024x576 38 objects, 20.7ms\n",
      "Speed: 2.2ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  49%|████▉     | 240/489 [02:26<02:49,  1.47it/s]\n",
      "0: 1024x576 30 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  49%|████▉     | 241/489 [02:26<02:43,  1.52it/s]\n",
      "0: 1024x576 37 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  49%|████▉     | 242/489 [02:27<02:38,  1.56it/s]\n",
      "0: 1024x576 32 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  50%|████▉     | 243/489 [02:28<02:34,  1.59it/s]\n",
      "0: 1024x576 40 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  50%|████▉     | 244/489 [02:28<02:32,  1.61it/s]\n",
      "0: 1024x576 37 objects, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  50%|█████     | 245/489 [02:29<02:29,  1.63it/s]\n",
      "0: 1024x576 30 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  50%|█████     | 246/489 [02:29<02:28,  1.64it/s]\n",
      "0: 1024x576 27 objects, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  51%|█████     | 247/489 [02:30<02:26,  1.65it/s]\n",
      "0: 1024x576 16 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  51%|█████     | 248/489 [02:31<02:17,  1.75it/s]\n",
      "0: 1024x576 12 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  51%|█████     | 249/489 [02:31<02:10,  1.83it/s]\n",
      "0: 1024x576 17 objects, 20.5ms\n",
      "Speed: 1.7ms preprocess, 20.5ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  51%|█████     | 250/489 [02:32<02:09,  1.84it/s]\n",
      "0: 1024x576 25 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  51%|█████▏    | 251/489 [02:32<02:11,  1.81it/s]\n",
      "0: 1024x576 25 objects, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  52%|█████▏    | 252/489 [02:33<02:15,  1.75it/s]\n",
      "0: 1024x576 34 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  52%|█████▏    | 253/489 [02:33<02:17,  1.72it/s]\n",
      "0: 1024x576 37 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  52%|█████▏    | 254/489 [02:34<02:17,  1.71it/s]\n",
      "0: 1024x576 37 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  52%|█████▏    | 255/489 [02:35<02:20,  1.67it/s]\n",
      "0: 1024x576 28 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  52%|█████▏    | 256/489 [02:35<02:17,  1.70it/s]\n",
      "0: 1024x576 20 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  53%|█████▎    | 257/489 [02:36<02:09,  1.79it/s]\n",
      "0: 1024x576 28 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  53%|█████▎    | 258/489 [02:36<02:12,  1.74it/s]\n",
      "0: 1024x576 27 objects, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  53%|█████▎    | 259/489 [02:37<02:13,  1.72it/s]\n",
      "0: 1024x576 33 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  53%|█████▎    | 260/489 [02:37<02:14,  1.70it/s]\n",
      "0: 1024x576 25 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  53%|█████▎    | 261/489 [02:38<02:14,  1.69it/s]\n",
      "0: 1024x576 34 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  54%|█████▎    | 262/489 [02:39<02:18,  1.64it/s]\n",
      "0: 1024x576 49 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  54%|█████▍    | 263/489 [02:39<02:27,  1.53it/s]\n",
      "0: 1024x576 27 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  54%|█████▍    | 264/489 [02:40<02:22,  1.58it/s]\n",
      "0: 1024x576 31 objects, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  54%|█████▍    | 265/489 [02:41<02:20,  1.60it/s]\n",
      "0: 1024x576 34 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  54%|█████▍    | 266/489 [02:41<02:24,  1.55it/s]\n",
      "0: 1024x576 19 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  55%|█████▍    | 267/489 [02:42<02:13,  1.67it/s]\n",
      "0: 1024x576 9 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  55%|█████▍    | 268/489 [02:42<02:04,  1.77it/s]\n",
      "0: 1024x576 13 objects, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  55%|█████▌    | 269/489 [02:43<01:59,  1.84it/s]\n",
      "0: 1024x576 17 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  55%|█████▌    | 270/489 [02:43<01:57,  1.87it/s]\n",
      "0: 1024x576 29 objects, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  55%|█████▌    | 271/489 [02:44<02:01,  1.79it/s]\n",
      "0: 1024x576 50 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  56%|█████▌    | 272/489 [02:45<02:12,  1.64it/s]\n",
      "0: 1024x576 51 objects, 20.3ms\n",
      "Speed: 1.7ms preprocess, 20.3ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  56%|█████▌    | 273/489 [02:45<02:24,  1.50it/s]\n",
      "0: 1024x576 23 objects, 20.6ms\n",
      "Speed: 3.3ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  56%|█████▌    | 274/489 [02:46<02:23,  1.50it/s]\n",
      "0: 1024x576 22 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  56%|█████▌    | 275/489 [02:47<02:13,  1.60it/s]\n",
      "0: 1024x576 23 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  56%|█████▋    | 276/489 [02:47<02:09,  1.64it/s]\n",
      "0: 1024x576 25 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  57%|█████▋    | 277/489 [02:48<02:09,  1.64it/s]\n",
      "0: 1024x576 48 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  57%|█████▋    | 278/489 [02:49<02:14,  1.57it/s]\n",
      "0: 1024x576 47 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  57%|█████▋    | 279/489 [02:49<02:17,  1.52it/s]\n",
      "0: 1024x576 31 objects, 20.8ms\n",
      "Speed: 1.9ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  57%|█████▋    | 280/489 [02:50<02:19,  1.50it/s]\n",
      "0: 1024x576 54 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  57%|█████▋    | 281/489 [02:51<02:21,  1.47it/s]\n",
      "0: 1024x576 48 objects, 21.3ms\n",
      "Speed: 2.1ms preprocess, 21.3ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  58%|█████▊    | 282/489 [02:51<02:21,  1.47it/s]\n",
      "0: 1024x576 36 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  58%|█████▊    | 283/489 [02:52<02:16,  1.51it/s]\n",
      "0: 1024x576 31 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  58%|█████▊    | 284/489 [02:53<02:11,  1.56it/s]\n",
      "0: 1024x576 17 objects, 21.9ms\n",
      "Speed: 1.7ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  58%|█████▊    | 285/489 [02:53<02:01,  1.68it/s]\n",
      "0: 1024x576 12 objects, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  58%|█████▊    | 286/489 [02:54<01:53,  1.79it/s]\n",
      "0: 1024x576 50 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  59%|█████▊    | 287/489 [02:54<02:03,  1.64it/s]\n",
      "0: 1024x576 59 objects, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  59%|█████▉    | 288/489 [02:55<02:09,  1.55it/s]\n",
      "0: 1024x576 22 objects, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  59%|█████▉    | 289/489 [02:56<02:04,  1.61it/s]\n",
      "0: 1024x576 13 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  59%|█████▉    | 290/489 [02:56<01:55,  1.72it/s]\n",
      "0: 1024x576 48 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  60%|█████▉    | 291/489 [02:57<02:02,  1.62it/s]\n",
      "0: 1024x576 55 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  60%|█████▉    | 292/489 [02:57<02:07,  1.54it/s]\n",
      "0: 1024x576 49 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  60%|█████▉    | 293/489 [02:58<02:10,  1.50it/s]\n",
      "0: 1024x576 32 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  60%|██████    | 294/489 [02:59<02:05,  1.56it/s]\n",
      "0: 1024x576 16 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  60%|██████    | 295/489 [02:59<01:55,  1.67it/s]\n",
      "0: 1024x576 12 objects, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  61%|██████    | 296/489 [03:00<01:48,  1.78it/s]\n",
      "0: 1024x576 14 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  61%|██████    | 297/489 [03:00<01:44,  1.84it/s]\n",
      "0: 1024x576 1 object, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  61%|██████    | 298/489 [03:01<01:40,  1.89it/s]\n",
      "Extracting samclip features:  61%|██████    | 299/489 [03:01<01:21,  2.33it/s]\n",
      "0: 1024x576 8 objects, 21.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No object detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  61%|██████▏   | 300/489 [03:01<01:27,  2.17it/s]\n",
      "0: 1024x576 7 objects, 21.6ms\n",
      "Speed: 2.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  62%|██████▏   | 301/489 [03:02<01:28,  2.13it/s]\n",
      "0: 1024x576 6 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  62%|██████▏   | 302/489 [03:02<01:28,  2.10it/s]\n",
      "0: 1024x576 12 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  62%|██████▏   | 303/489 [03:03<01:29,  2.08it/s]\n",
      "0: 1024x576 4 objects, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  62%|██████▏   | 304/489 [03:03<01:30,  2.05it/s]\n",
      "0: 1024x576 25 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  62%|██████▏   | 305/489 [03:04<01:36,  1.92it/s]\n",
      "0: 1024x576 19 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  63%|██████▎   | 306/489 [03:05<01:34,  1.93it/s]\n",
      "0: 1024x576 37 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  63%|██████▎   | 307/489 [03:05<01:39,  1.82it/s]\n",
      "0: 1024x576 33 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  63%|██████▎   | 308/489 [03:06<01:42,  1.77it/s]\n",
      "0: 1024x576 48 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  63%|██████▎   | 309/489 [03:06<01:49,  1.65it/s]\n",
      "0: 1024x576 50 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  63%|██████▎   | 310/489 [03:07<01:52,  1.59it/s]\n",
      "0: 1024x576 61 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  64%|██████▎   | 311/489 [03:08<01:59,  1.49it/s]\n",
      "0: 1024x576 53 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  64%|██████▍   | 312/489 [03:09<02:02,  1.45it/s]\n",
      "0: 1024x576 59 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  64%|██████▍   | 313/489 [03:09<02:00,  1.46it/s]\n",
      "0: 1024x576 66 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  64%|██████▍   | 314/489 [03:10<02:05,  1.40it/s]\n",
      "0: 1024x576 42 objects, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  64%|██████▍   | 315/489 [03:11<02:00,  1.44it/s]\n",
      "0: 1024x576 70 objects, 20.5ms\n",
      "Speed: 1.6ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  65%|██████▍   | 316/489 [03:11<02:01,  1.42it/s]\n",
      "0: 1024x576 48 objects, 21.8ms\n",
      "Speed: 1.6ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  65%|██████▍   | 317/489 [03:12<01:53,  1.51it/s]\n",
      "0: 1024x576 40 objects, 21.3ms\n",
      "Speed: 1.4ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  65%|██████▌   | 318/489 [03:13<01:54,  1.49it/s]\n",
      "0: 1024x576 32 objects, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  65%|██████▌   | 319/489 [03:13<01:46,  1.59it/s]\n",
      "0: 1024x576 30 objects, 22.1ms\n",
      "Speed: 2.0ms preprocess, 22.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  65%|██████▌   | 320/489 [03:14<01:44,  1.62it/s]\n",
      "0: 1024x576 37 objects, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  66%|██████▌   | 321/489 [03:14<01:41,  1.65it/s]\n",
      "0: 1024x576 88 objects, 20.7ms\n",
      "Speed: 1.5ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  66%|██████▌   | 322/489 [03:15<01:53,  1.48it/s]\n",
      "0: 1024x576 84 objects, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  66%|██████▌   | 323/489 [03:16<02:00,  1.38it/s]\n",
      "0: 1024x576 88 objects, 21.8ms\n",
      "Speed: 1.6ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  66%|██████▋   | 324/489 [03:17<02:01,  1.35it/s]\n",
      "0: 1024x576 71 objects, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  66%|██████▋   | 325/489 [03:18<02:01,  1.34it/s]\n",
      "0: 1024x576 74 objects, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  67%|██████▋   | 326/489 [03:18<02:04,  1.31it/s]\n",
      "0: 1024x576 70 objects, 21.9ms\n",
      "Speed: 1.8ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  67%|██████▋   | 327/489 [03:19<02:07,  1.27it/s]\n",
      "0: 1024x576 68 objects, 22.1ms\n",
      "Speed: 1.8ms preprocess, 22.1ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  67%|██████▋   | 328/489 [03:20<02:06,  1.27it/s]\n",
      "0: 1024x576 64 objects, 22.3ms\n",
      "Speed: 2.3ms preprocess, 22.3ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  67%|██████▋   | 329/489 [03:21<02:06,  1.27it/s]\n",
      "0: 1024x576 45 objects, 21.9ms\n",
      "Speed: 2.0ms preprocess, 21.9ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  67%|██████▋   | 330/489 [03:22<02:00,  1.32it/s]\n",
      "0: 1024x576 48 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  68%|██████▊   | 331/489 [03:22<01:56,  1.35it/s]\n",
      "0: 1024x576 46 objects, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  68%|██████▊   | 332/489 [03:23<01:54,  1.37it/s]\n",
      "0: 1024x576 36 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  68%|██████▊   | 333/489 [03:24<01:48,  1.44it/s]\n",
      "0: 1024x576 32 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  68%|██████▊   | 334/489 [03:24<01:42,  1.51it/s]\n",
      "0: 1024x576 13 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  69%|██████▊   | 335/489 [03:25<01:37,  1.57it/s]\n",
      "0: 1024x576 14 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  69%|██████▊   | 336/489 [03:25<01:31,  1.68it/s]\n",
      "0: 1024x576 3 objects, 20.7ms\n",
      "Speed: 1.8ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  69%|██████▉   | 337/489 [03:26<01:25,  1.78it/s]\n",
      "0: 1024x576 3 objects, 21.9ms\n",
      "Speed: 3.0ms preprocess, 21.9ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  69%|██████▉   | 338/489 [03:26<01:24,  1.79it/s]\n",
      "0: 1024x576 7 objects, 31.7ms\n",
      "Speed: 1.8ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  69%|██████▉   | 339/489 [03:27<01:20,  1.87it/s]\n",
      "0: 1024x576 3 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  70%|██████▉   | 340/489 [03:27<01:17,  1.93it/s]\n",
      "0: 1024x576 2 objects, 21.0ms\n",
      "Speed: 1.9ms preprocess, 21.0ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  70%|██████▉   | 341/489 [03:28<01:15,  1.95it/s]\n",
      "0: 1024x576 6 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  70%|██████▉   | 342/489 [03:28<01:15,  1.96it/s]\n",
      "0: 1024x576 8 objects, 20.4ms\n",
      "Speed: 1.8ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  70%|███████   | 343/489 [03:29<01:14,  1.97it/s]\n",
      "0: 1024x576 8 objects, 21.3ms\n",
      "Speed: 1.8ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  70%|███████   | 344/489 [03:29<01:13,  1.98it/s]\n",
      "0: 1024x576 18 objects, 21.2ms\n",
      "Speed: 1.9ms preprocess, 21.2ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  71%|███████   | 345/489 [03:30<01:12,  1.98it/s]\n",
      "0: 1024x576 12 objects, 21.0ms\n",
      "Speed: 2.9ms preprocess, 21.0ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  71%|███████   | 346/489 [03:30<01:11,  2.00it/s]\n",
      "0: 1024x576 19 objects, 20.7ms\n",
      "Speed: 1.8ms preprocess, 20.7ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  71%|███████   | 347/489 [03:31<01:11,  1.99it/s]\n",
      "0: 1024x576 13 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  71%|███████   | 348/489 [03:31<01:09,  2.02it/s]\n",
      "0: 1024x576 19 objects, 20.9ms\n",
      "Speed: 1.8ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  71%|███████▏  | 349/489 [03:32<01:10,  1.99it/s]\n",
      "0: 1024x576 28 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  72%|███████▏  | 350/489 [03:32<01:14,  1.87it/s]\n",
      "0: 1024x576 22 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  72%|███████▏  | 351/489 [03:33<01:12,  1.90it/s]\n",
      "0: 1024x576 17 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  72%|███████▏  | 352/489 [03:33<01:10,  1.94it/s]\n",
      "0: 1024x576 20 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  72%|███████▏  | 353/489 [03:34<01:09,  1.95it/s]\n",
      "0: 1024x576 14 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  72%|███████▏  | 354/489 [03:34<01:07,  1.99it/s]\n",
      "0: 1024x576 20 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  73%|███████▎  | 355/489 [03:35<01:08,  1.97it/s]\n",
      "0: 1024x576 34 objects, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  73%|███████▎  | 356/489 [03:35<01:11,  1.87it/s]\n",
      "0: 1024x576 42 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  73%|███████▎  | 357/489 [03:36<01:16,  1.73it/s]\n",
      "0: 1024x576 41 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  73%|███████▎  | 358/489 [03:37<01:18,  1.67it/s]\n",
      "0: 1024x576 49 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  73%|███████▎  | 359/489 [03:37<01:21,  1.59it/s]\n",
      "0: 1024x576 63 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  74%|███████▎  | 360/489 [03:38<01:28,  1.46it/s]\n",
      "0: 1024x576 60 objects, 20.8ms\n",
      "Speed: 1.9ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  74%|███████▍  | 361/489 [03:39<01:35,  1.34it/s]\n",
      "0: 1024x576 63 objects, 22.1ms\n",
      "Speed: 2.2ms preprocess, 22.1ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  74%|███████▍  | 362/489 [03:40<01:36,  1.31it/s]\n",
      "0: 1024x576 54 objects, 21.9ms\n",
      "Speed: 1.8ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  74%|███████▍  | 363/489 [03:41<01:36,  1.30it/s]\n",
      "0: 1024x576 54 objects, 20.9ms\n",
      "Speed: 1.8ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  74%|███████▍  | 364/489 [03:42<01:37,  1.28it/s]\n",
      "0: 1024x576 47 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  75%|███████▍  | 365/489 [03:42<01:33,  1.32it/s]\n",
      "0: 1024x576 31 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  75%|███████▍  | 366/489 [03:43<01:26,  1.41it/s]\n",
      "0: 1024x576 77 objects, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  75%|███████▌  | 367/489 [03:44<01:31,  1.33it/s]\n",
      "0: 1024x576 60 objects, 20.1ms\n",
      "Speed: 1.7ms preprocess, 20.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  75%|███████▌  | 368/489 [03:45<01:34,  1.28it/s]\n",
      "0: 1024x576 48 objects, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  75%|███████▌  | 369/489 [03:45<01:34,  1.27it/s]\n",
      "0: 1024x576 48 objects, 21.9ms\n",
      "Speed: 1.7ms preprocess, 21.9ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  76%|███████▌  | 370/489 [03:46<01:31,  1.29it/s]\n",
      "0: 1024x576 32 objects, 20.2ms\n",
      "Speed: 1.7ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  76%|███████▌  | 371/489 [03:47<01:26,  1.36it/s]\n",
      "0: 1024x576 25 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  76%|███████▌  | 372/489 [03:47<01:21,  1.44it/s]\n",
      "0: 1024x576 20 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  76%|███████▋  | 373/489 [03:48<01:16,  1.51it/s]\n",
      "0: 1024x576 14 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  76%|███████▋  | 374/489 [03:48<01:11,  1.61it/s]\n",
      "0: 1024x576 11 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  77%|███████▋  | 375/489 [03:49<01:05,  1.75it/s]\n",
      "0: 1024x576 2 objects, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  77%|███████▋  | 376/489 [03:49<01:01,  1.84it/s]\n",
      "0: 1024x576 7 objects, 21.4ms\n",
      "Speed: 1.9ms preprocess, 21.4ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  77%|███████▋  | 377/489 [03:50<01:00,  1.86it/s]\n",
      "0: 1024x576 12 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  77%|███████▋  | 378/489 [03:50<00:58,  1.90it/s]\n",
      "0: 1024x576 13 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  78%|███████▊  | 379/489 [03:51<00:56,  1.94it/s]\n",
      "0: 1024x576 18 objects, 20.3ms\n",
      "Speed: 1.7ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  78%|███████▊  | 380/489 [03:51<00:55,  1.95it/s]\n",
      "0: 1024x576 24 objects, 21.4ms\n",
      "Speed: 1.6ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  78%|███████▊  | 381/489 [03:52<00:58,  1.84it/s]\n",
      "0: 1024x576 20 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  78%|███████▊  | 382/489 [03:53<00:57,  1.85it/s]\n",
      "0: 1024x576 19 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  78%|███████▊  | 383/489 [03:53<00:56,  1.89it/s]\n",
      "0: 1024x576 18 objects, 21.5ms\n",
      "Speed: 2.7ms preprocess, 21.5ms inference, 1.9ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  79%|███████▊  | 384/489 [03:54<00:54,  1.93it/s]\n",
      "0: 1024x576 27 objects, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  79%|███████▊  | 385/489 [03:54<00:58,  1.78it/s]\n",
      "0: 1024x576 54 objects, 21.8ms\n",
      "Speed: 1.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  79%|███████▉  | 386/489 [03:55<01:01,  1.68it/s]\n",
      "0: 1024x576 51 objects, 21.7ms\n",
      "Speed: 1.8ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  79%|███████▉  | 387/489 [03:56<01:02,  1.63it/s]\n",
      "0: 1024x576 37 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  79%|███████▉  | 388/489 [03:56<01:01,  1.65it/s]\n",
      "0: 1024x576 29 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  80%|███████▉  | 389/489 [03:57<01:00,  1.66it/s]\n",
      "0: 1024x576 40 objects, 20.7ms\n",
      "Speed: 1.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  80%|███████▉  | 390/489 [03:57<00:59,  1.65it/s]\n",
      "0: 1024x576 33 objects, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  80%|███████▉  | 391/489 [03:58<01:00,  1.62it/s]\n",
      "0: 1024x576 29 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  80%|████████  | 392/489 [03:59<01:00,  1.61it/s]\n",
      "0: 1024x576 24 objects, 21.0ms\n",
      "Speed: 1.6ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  80%|████████  | 393/489 [03:59<00:58,  1.64it/s]\n",
      "0: 1024x576 26 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  81%|████████  | 394/489 [04:00<00:58,  1.63it/s]\n",
      "0: 1024x576 20 objects, 21.3ms\n",
      "Speed: 1.6ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  81%|████████  | 395/489 [04:00<00:54,  1.72it/s]\n",
      "0: 1024x576 13 objects, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  81%|████████  | 396/489 [04:01<00:51,  1.82it/s]\n",
      "0: 1024x576 9 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  81%|████████  | 397/489 [04:01<00:49,  1.87it/s]\n",
      "0: 1024x576 6 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  81%|████████▏ | 398/489 [04:02<00:47,  1.90it/s]\n",
      "0: 1024x576 3 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  82%|████████▏ | 399/489 [04:02<00:46,  1.93it/s]\n",
      "0: 1024x576 4 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  82%|████████▏ | 400/489 [04:03<00:45,  1.95it/s]\n",
      "0: 1024x576 5 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  82%|████████▏ | 401/489 [04:03<00:44,  1.96it/s]\n",
      "0: 1024x576 8 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  82%|████████▏ | 402/489 [04:04<00:44,  1.97it/s]\n",
      "0: 1024x576 3 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  82%|████████▏ | 403/489 [04:04<00:43,  1.98it/s]\n",
      "0: 1024x576 4 objects, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  83%|████████▎ | 404/489 [04:05<00:42,  1.98it/s]\n",
      "0: 1024x576 2 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  83%|████████▎ | 405/489 [04:05<00:42,  1.98it/s]\n",
      "0: 1024x576 1 object, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  83%|████████▎ | 406/489 [04:06<00:41,  1.98it/s]\n",
      "0: 1024x576 5 objects, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  83%|████████▎ | 407/489 [04:06<00:41,  2.00it/s]\n",
      "0: 1024x576 2 objects, 21.1ms\n",
      "Speed: 1.6ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  83%|████████▎ | 408/489 [04:07<00:40,  2.00it/s]\n",
      "0: 1024x576 2 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  84%|████████▎ | 409/489 [04:07<00:39,  2.01it/s]\n",
      "0: 1024x576 3 objects, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  84%|████████▍ | 410/489 [04:08<00:39,  2.00it/s]\n",
      "0: 1024x576 1 object, 20.8ms\n",
      "Speed: 2.3ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  84%|████████▍ | 411/489 [04:08<00:38,  2.01it/s]\n",
      "0: 1024x576 4 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  84%|████████▍ | 412/489 [04:09<00:38,  2.01it/s]\n",
      "0: 1024x576 5 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  84%|████████▍ | 413/489 [04:09<00:37,  2.01it/s]\n",
      "0: 1024x576 11 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  85%|████████▍ | 414/489 [04:10<00:37,  2.03it/s]\n",
      "0: 1024x576 17 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  85%|████████▍ | 415/489 [04:10<00:36,  2.01it/s]\n",
      "0: 1024x576 20 objects, 21.3ms\n",
      "Speed: 1.6ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  85%|████████▌ | 416/489 [04:11<00:38,  1.90it/s]\n",
      "0: 1024x576 19 objects, 21.0ms\n",
      "Speed: 2.3ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  85%|████████▌ | 417/489 [04:12<00:39,  1.82it/s]\n",
      "0: 1024x576 20 objects, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  85%|████████▌ | 418/489 [04:12<00:39,  1.79it/s]\n",
      "0: 1024x576 21 objects, 21.5ms\n",
      "Speed: 1.6ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  86%|████████▌ | 419/489 [04:13<00:38,  1.81it/s]\n",
      "0: 1024x576 24 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  86%|████████▌ | 420/489 [04:13<00:39,  1.76it/s]\n",
      "0: 1024x576 21 objects, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  86%|████████▌ | 421/489 [04:14<00:39,  1.74it/s]\n",
      "0: 1024x576 17 objects, 21.1ms\n",
      "Speed: 1.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  86%|████████▋ | 422/489 [04:14<00:37,  1.80it/s]\n",
      "0: 1024x576 21 objects, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  87%|████████▋ | 423/489 [04:15<00:37,  1.76it/s]\n",
      "0: 1024x576 16 objects, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  87%|████████▋ | 424/489 [04:15<00:35,  1.83it/s]\n",
      "0: 1024x576 26 objects, 21.5ms\n",
      "Speed: 2.0ms preprocess, 21.5ms inference, 2.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  87%|████████▋ | 425/489 [04:16<00:36,  1.76it/s]\n",
      "0: 1024x576 19 objects, 21.3ms\n",
      "Speed: 1.6ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  87%|████████▋ | 426/489 [04:17<00:35,  1.76it/s]\n",
      "0: 1024x576 18 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  87%|████████▋ | 427/489 [04:17<00:34,  1.81it/s]\n",
      "0: 1024x576 21 objects, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  88%|████████▊ | 428/489 [04:18<00:32,  1.86it/s]\n",
      "0: 1024x576 19 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  88%|████████▊ | 429/489 [04:18<00:32,  1.82it/s]\n",
      "0: 1024x576 20 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  88%|████████▊ | 430/489 [04:19<00:32,  1.84it/s]\n",
      "0: 1024x576 22 objects, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  88%|████████▊ | 431/489 [04:19<00:30,  1.88it/s]\n",
      "0: 1024x576 21 objects, 20.6ms\n",
      "Speed: 2.6ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  88%|████████▊ | 432/489 [04:20<00:30,  1.90it/s]\n",
      "0: 1024x576 18 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  89%|████████▊ | 433/489 [04:20<00:29,  1.91it/s]\n",
      "0: 1024x576 21 objects, 21.6ms\n",
      "Speed: 2.2ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  89%|████████▉ | 434/489 [04:21<00:29,  1.87it/s]\n",
      "0: 1024x576 21 objects, 21.0ms\n",
      "Speed: 1.6ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  89%|████████▉ | 435/489 [04:21<00:29,  1.81it/s]\n",
      "0: 1024x576 20 objects, 21.7ms\n",
      "Speed: 2.6ms preprocess, 21.7ms inference, 1.8ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  89%|████████▉ | 436/489 [04:22<00:28,  1.87it/s]\n",
      "0: 1024x576 20 objects, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  89%|████████▉ | 437/489 [04:22<00:27,  1.88it/s]\n",
      "0: 1024x576 18 objects, 21.3ms\n",
      "Speed: 1.6ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  90%|████████▉ | 438/489 [04:23<00:26,  1.93it/s]\n",
      "0: 1024x576 23 objects, 21.0ms\n",
      "Speed: 1.5ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  90%|████████▉ | 439/489 [04:24<00:27,  1.85it/s]\n",
      "0: 1024x576 20 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  90%|████████▉ | 440/489 [04:24<00:26,  1.88it/s]\n",
      "0: 1024x576 26 objects, 20.8ms\n",
      "Speed: 1.7ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  90%|█████████ | 441/489 [04:25<00:26,  1.82it/s]\n",
      "0: 1024x576 31 objects, 21.4ms\n",
      "Speed: 1.6ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  90%|█████████ | 442/489 [04:25<00:27,  1.71it/s]\n",
      "0: 1024x576 49 objects, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  91%|█████████ | 443/489 [04:26<00:29,  1.57it/s]\n",
      "0: 1024x576 57 objects, 21.3ms\n",
      "Speed: 1.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  91%|█████████ | 444/489 [04:27<00:29,  1.52it/s]\n",
      "0: 1024x576 50 objects, 21.5ms\n",
      "Speed: 1.7ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  91%|█████████ | 445/489 [04:27<00:29,  1.49it/s]\n",
      "0: 1024x576 54 objects, 21.5ms\n",
      "Speed: 1.6ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  91%|█████████ | 446/489 [04:28<00:30,  1.41it/s]\n",
      "0: 1024x576 85 objects, 21.4ms\n",
      "Speed: 1.7ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  91%|█████████▏| 447/489 [04:29<00:32,  1.30it/s]\n",
      "0: 1024x576 50 objects, 21.8ms\n",
      "Speed: 1.5ms preprocess, 21.8ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  92%|█████████▏| 448/489 [04:30<00:30,  1.35it/s]\n",
      "0: 1024x576 45 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  92%|█████████▏| 449/489 [04:31<00:28,  1.39it/s]\n",
      "0: 1024x576 54 objects, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  92%|█████████▏| 450/489 [04:31<00:28,  1.39it/s]\n",
      "0: 1024x576 47 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  92%|█████████▏| 451/489 [04:32<00:27,  1.40it/s]\n",
      "0: 1024x576 67 objects, 20.6ms\n",
      "Speed: 1.3ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  92%|█████████▏| 452/489 [04:33<00:27,  1.34it/s]\n",
      "0: 1024x576 47 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  93%|█████████▎| 453/489 [04:33<00:26,  1.38it/s]\n",
      "0: 1024x576 32 objects, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  93%|█████████▎| 454/489 [04:34<00:24,  1.46it/s]\n",
      "0: 1024x576 26 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  93%|█████████▎| 455/489 [04:35<00:22,  1.52it/s]\n",
      "0: 1024x576 22 objects, 20.5ms\n",
      "Speed: 1.7ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  93%|█████████▎| 456/489 [04:35<00:20,  1.64it/s]\n",
      "0: 1024x576 40 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  93%|█████████▎| 457/489 [04:36<00:20,  1.57it/s]\n",
      "0: 1024x576 13 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  94%|█████████▎| 458/489 [04:36<00:18,  1.70it/s]\n",
      "0: 1024x576 23 objects, 21.6ms\n",
      "Speed: 1.8ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  94%|█████████▍| 459/489 [04:37<00:16,  1.77it/s]\n",
      "0: 1024x576 41 objects, 20.1ms\n",
      "Speed: 1.3ms preprocess, 20.1ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  94%|█████████▍| 460/489 [04:38<00:17,  1.65it/s]\n",
      "0: 1024x576 23 objects, 20.9ms\n",
      "Speed: 1.3ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  94%|█████████▍| 461/489 [04:38<00:16,  1.65it/s]\n",
      "0: 1024x576 71 objects, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  94%|█████████▍| 462/489 [04:39<00:18,  1.48it/s]\n",
      "0: 1024x576 3 objects, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  95%|█████████▍| 463/489 [04:39<00:15,  1.64it/s]\n",
      "0: 1024x576 2 objects, 21.2ms\n",
      "Speed: 1.8ms preprocess, 21.2ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  95%|█████████▍| 464/489 [04:40<00:13,  1.83it/s]\n",
      "0: 1024x576 3 objects, 20.8ms\n",
      "Speed: 1.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  95%|█████████▌| 465/489 [04:40<00:13,  1.84it/s]\n",
      "0: 1024x576 34 objects, 21.4ms\n",
      "Speed: 1.8ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  95%|█████████▌| 466/489 [04:41<00:13,  1.69it/s]\n",
      "0: 1024x576 43 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  96%|█████████▌| 467/489 [04:42<00:13,  1.62it/s]\n",
      "0: 1024x576 16 objects, 20.8ms\n",
      "Speed: 1.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  96%|█████████▌| 468/489 [04:42<00:12,  1.72it/s]\n",
      "0: 1024x576 29 objects, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  96%|█████████▌| 469/489 [04:43<00:11,  1.71it/s]\n",
      "0: 1024x576 29 objects, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  96%|█████████▌| 470/489 [04:43<00:11,  1.70it/s]\n",
      "0: 1024x576 18 objects, 20.5ms\n",
      "Speed: 1.3ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  96%|█████████▋| 471/489 [04:44<00:10,  1.77it/s]\n",
      "0: 1024x576 9 objects, 21.5ms\n",
      "Speed: 1.5ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  97%|█████████▋| 472/489 [04:44<00:09,  1.85it/s]\n",
      "0: 1024x576 23 objects, 21.6ms\n",
      "Speed: 1.4ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  97%|█████████▋| 473/489 [04:45<00:08,  1.80it/s]\n",
      "0: 1024x576 3 objects, 21.6ms\n",
      "Speed: 1.4ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  97%|█████████▋| 474/489 [04:45<00:07,  1.95it/s]\n",
      "0: 1024x576 55 objects, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  97%|█████████▋| 475/489 [04:46<00:08,  1.72it/s]\n",
      "0: 1024x576 68 objects, 20.6ms\n",
      "Speed: 1.4ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  97%|█████████▋| 476/489 [04:47<00:08,  1.54it/s]\n",
      "0: 1024x576 48 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  98%|█████████▊| 477/489 [04:48<00:07,  1.52it/s]\n",
      "0: 1024x576 3 objects, 20.6ms\n",
      "Speed: 1.3ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  98%|█████████▊| 478/489 [04:48<00:06,  1.65it/s]\n",
      "0: 1024x576 20 objects, 21.1ms\n",
      "Speed: 1.4ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  98%|█████████▊| 479/489 [04:49<00:05,  1.74it/s]\n",
      "0: 1024x576 52 objects, 20.5ms\n",
      "Speed: 1.3ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  98%|█████████▊| 480/489 [04:49<00:05,  1.61it/s]\n",
      "0: 1024x576 69 objects, 21.6ms\n",
      "Speed: 1.5ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  98%|█████████▊| 481/489 [04:50<00:05,  1.48it/s]\n",
      "0: 1024x576 4 objects, 21.0ms\n",
      "Speed: 1.4ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  99%|█████████▊| 482/489 [04:51<00:04,  1.64it/s]\n",
      "0: 1024x576 33 objects, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  99%|█████████▉| 483/489 [04:51<00:03,  1.57it/s]\n",
      "0: 1024x576 21 objects, 21.2ms\n",
      "Speed: 1.3ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  99%|█████████▉| 484/489 [04:52<00:02,  1.67it/s]\n",
      "0: 1024x576 4 objects, 21.4ms\n",
      "Speed: 1.3ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  99%|█████████▉| 485/489 [04:52<00:02,  1.76it/s]\n",
      "0: 1024x576 3 objects, 20.9ms\n",
      "Speed: 1.8ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features:  99%|█████████▉| 486/489 [04:53<00:01,  1.84it/s]\n",
      "0: 1024x576 19 objects, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features: 100%|█████████▉| 487/489 [04:53<00:01,  1.86it/s]\n",
      "0: 1024x576 19 objects, 21.3ms\n",
      "Speed: 1.3ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features: 100%|█████████▉| 488/489 [04:54<00:00,  1.91it/s]\n",
      "0: 1024x576 20 objects, 21.3ms\n",
      "Speed: 1.3ms preprocess, 21.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Extracting samclip features: 100%|██████████| 489/489 [04:54<00:00,  1.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saved samclip features to cache at \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/preproc/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">feature-splatting_samclip-features.pt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saved samclip features to cache at \n",
       "\u001b[35m/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/preproc/\u001b[0m\u001b[95mfeature-splatting_samclip-features.pt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datamanager = FeatureSplattingDataManager(config.pipeline.datamanager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([441, 384, 57, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamanager.train_features['dinov2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature splatting pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# pipeline = config.pipeline.setup(device='cuda', test_mode='inference')\n",
    "\n",
    "aabb_scale = datamanager.dataparser.config.scene_scale\n",
    "\n",
    "scene_box = SceneBox(\n",
    "    aabb=torch.tensor(\n",
    "        [[-aabb_scale, -aabb_scale, -aabb_scale], [aabb_scale, aabb_scale, aabb_scale]], dtype=torch.float32\n",
    "    )\n",
    ")\n",
    "\n",
    "train_samples = len(datamanager.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth_filenames': None,\n",
       " 'depth_unit_scale_factor': 0.001,\n",
       " 'mask_color': None,\n",
       " 'points3D_xyz': tensor([[ 0.5881,  0.2354,  0.3443],\n",
       "         [ 0.5153,  0.2859,  0.1964],\n",
       "         [ 0.5296,  0.2778,  0.2392],\n",
       "         ...,\n",
       "         [ 0.4155,  0.4354,  0.4631],\n",
       "         [-0.0926,  0.2761,  0.3994],\n",
       "         [ 0.4062,  0.4324,  0.4728]]),\n",
       " 'points3D_rgb': tensor([[174, 173, 178],\n",
       "         [ 20,  10,   6],\n",
       "         [104,  97,  96],\n",
       "         ...,\n",
       "         [ 77,  81,  59],\n",
       "         [ 52,  54,  45],\n",
       "         [ 41,  50,  32]], dtype=torch.uint8),\n",
       " 'feature_type': 'samclip',\n",
       " 'feature_dims': {'dinov2': torch.Size([384, 57, 32]),\n",
       "  'samclip': torch.Size([768, 113, 64])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamanager.train_dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RadegsFeaturesModel(\n",
       "  (gauss_params): ParameterDict(\n",
       "      (features_dc): Parameter containing: [torch.cuda.FloatTensor of size 50000x3 (cuda:0)]\n",
       "      (features_rest): Parameter containing: [torch.cuda.FloatTensor of size 50000x0x3 (cuda:0)]\n",
       "      (means): Parameter containing: [torch.cuda.FloatTensor of size 50000x3 (cuda:0)]\n",
       "      (opacities): Parameter containing: [torch.cuda.FloatTensor of size 50000x1 (cuda:0)]\n",
       "      (quats): Parameter containing: [torch.cuda.FloatTensor of size 50000x4 (cuda:0)]\n",
       "      (scales): Parameter containing: [torch.cuda.FloatTensor of size 50000x3 (cuda:0)]\n",
       "      (distill_features): Parameter containing: [torch.cuda.FloatTensor of size 50000x13 (cuda:0)]\n",
       "  )\n",
       "  (camera_optimizer): CameraOptimizer()\n",
       "  (psnr): PeakSignalNoiseRatio()\n",
       "  (ssim): SSIM()\n",
       "  (lpips): LearnedPerceptualImagePatchSimilarity(\n",
       "    (net): _NoTrainLpips(\n",
       "      (scaling_layer): ScalingLayer()\n",
       "      (net): Alexnet(\n",
       "        (slice1): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice2): Sequential(\n",
       "          (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice3): Sequential(\n",
       "          (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice4): Sequential(\n",
       "          (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (9): ReLU(inplace=True)\n",
       "        )\n",
       "        (slice5): Sequential(\n",
       "          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (11): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (lin0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin3): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lin4): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (lins): ModuleList(\n",
       "        (0): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3-4): 2 x NetLinLayer(\n",
       "          (model): Sequential(\n",
       "            (0): Dropout(p=0.5, inplace=False)\n",
       "            (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TwoLayerMLP(\n",
       "    (hidden_conv): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (feature_branch_dict): ModuleDict(\n",
       "      (dinov2): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (samclip): Conv2d(64, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = RadegsFeaturesModelConfig(output_depth_during_training=False)\n",
    "config.sh_degree = 0\n",
    "\n",
    "radegs_model = RadegsFeaturesModel(\n",
    "    config, \n",
    "    scene_box=scene_box, \n",
    "    num_train_data=train_samples,\n",
    "    metadata=datamanager.train_dataset.metadata,\n",
    ")\n",
    "radegs_model.populate_modules()\n",
    "\n",
    "radegs_model = radegs_model.to(device)\n",
    "\n",
    "radegs_model.eval()\n",
    "\n",
    "# radegs_model.get_outputs(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "camera = datamanager.train_dataset.cameras[idx:idx+1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ns-extension/ns_extension/utils/camera_utils.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(get_world2view_transform(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RadegsFeaturesModel._render() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mradegs_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ns-extension/ns_extension/models/rade_features_model.py:233\u001b[0m, in \u001b[0;36mRadegsFeaturesModel.get_outputs\u001b[0;34m(self, camera)\u001b[0m\n\u001b[1;32m    221\u001b[0m     sh_degree_to_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Modified rasterization function from https://github.com/brian-xu/gsplat-rade/blob/main/gsplat/rendering.py\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Enables returning depth and normal maps for computing of loss\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# - expected_normals: [N, 1]\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# - meta (set to self.info)\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m render, alpha, expected_depths, median_depths, expected_normals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquats_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscales_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopacities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopacities_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistill_features_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43msh_degree_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msh_degree_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisible_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvoxel_visible_mask\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mstep_pre_backward(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgauss_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    249\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: RadegsFeaturesModel._render() got an unexpected keyword argument 'color'"
     ]
    }
   ],
   "source": [
    "outputs = radegs_model.get_outputs(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.models.splatfacto import num_sh_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth_filenames': None,\n",
       " 'depth_unit_scale_factor': 0.001,\n",
       " 'mask_color': None,\n",
       " 'points3D_xyz': tensor([[ 0.5881,  0.2354,  0.3443],\n",
       "         [ 0.5153,  0.2859,  0.1964],\n",
       "         [ 0.5296,  0.2778,  0.2392],\n",
       "         ...,\n",
       "         [ 0.4155,  0.4354,  0.4631],\n",
       "         [-0.0926,  0.2761,  0.3994],\n",
       "         [ 0.4062,  0.4324,  0.4728]]),\n",
       " 'points3D_rgb': tensor([[174, 173, 178],\n",
       "         [ 20,  10,   6],\n",
       "         [104,  97,  96],\n",
       "         ...,\n",
       "         [ 77,  81,  59],\n",
       "         [ 52,  54,  45],\n",
       "         [ 41,  50,  32]], dtype=torch.uint8),\n",
       " 'feature_type': 'SAMCLIP',\n",
       " 'feature_dims': {'dinov2': torch.Size([384, 57, 32]),\n",
       "  'samclip': torch.Size([768, 113, 64])}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamanager.train_dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 16, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dc_crop = radegs_model.features_dc\n",
    "features_rest_crop = radegs_model.features_rest\n",
    "\n",
    "colors_crop = torch.cat((features_dc_crop[:, None, :], features_rest_crop), dim=1)\n",
    "\n",
    "colors_crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "CUDA available: True\n",
      "CUDA version PyTorch was compiled with: 11.8\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version PyTorch was compiled with: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera device: cpu\n",
      "Parameter device_indicator_param is on cpu\n"
     ]
    }
   ],
   "source": [
    "# Before calling radegs_model.get_outputs(camera)\n",
    "print(f\"Camera device: {camera.device if hasattr(camera, 'device') else 'No device attr'}\")\n",
    "\n",
    "# Check model parameters\n",
    "for name, param in radegs_model.named_parameters():\n",
    "    if param.device.type != 'cuda':\n",
    "        print(f\"Parameter {name} is on {param.device}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the entire model to CUDA\n",
    "radegs_model = radegs_model.cuda()\n",
    "\n",
    "# Or if you want to be explicit about the device\n",
    "device = torch.device('cuda:0')\n",
    "radegs_model = radegs_model.to(device)\n",
    "\n",
    "camera = camera.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/rade_gs/rade_gs/utils/camera_utils.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(getWorld2View2(R, T, trans, scale)).transpose(0, 1).cuda()\n"
     ]
    }
   ],
   "source": [
    "outputs = radegs_model.get_outputs(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerfstudio/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:135: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fs_outputs = pipeline.model.get_outputs(camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Radegs pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find config for our current model\n",
    "rade_gs_load_config = Path('/workspace/fieldwork-data/rats/2024-07-11/environment/C0119/rade-gs/2025-07-02_114948/config.yml')\n",
    "\n",
    "rade_gs_config = yaml.load(rade_gs_load_config.read_text(), Loader=yaml.Loader)\n",
    "\n",
    "# Create the environment\n",
    "rade_gs_pipeline = rade_gs_config.pipeline.setup(device='cuda', test_mode='inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "radegs_outs = rade_gs_pipeline.model.get_outputs(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.model.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera.get_intrinsics_matrices().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "radegs_im = radegs_outs['rgb'].detach().cpu().numpy()\n",
    "plt.imshow(radegs_im)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
